<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes – Production-Grade Container Orchestration</title><link>https://kubernetes.io/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/</link></image><atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Contribution, containers and cricket: the Kubernetes 1.22 release interview</title><link>https://kubernetes.io/blog/2021/12/01/contribution-containers-and-cricket-the-kubernetes-1.22-release-interview/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/12/01/contribution-containers-and-cricket-the-kubernetes-1.22-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>The Kubernetes release train rolls on, and we look ahead to the release of 1.23 next week. &lt;a href="https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog">As is our tradition&lt;/a>, I'm pleased to bring you a look back at the process that brought us the previous version.&lt;/p>
&lt;p>The release team for 1.22 was led by &lt;a href="https://twitter.com/coffeeartgirl">Savitha Raghunathan&lt;/a>, who was, at the time, a Senior Platform Engineer at MathWorks. &lt;a href="https://kubernetespodcast.com/episode/157-kubernetes-1.22/">I spoke to Savitha&lt;/a> on the &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a>, the weekly&lt;super>*&lt;/super> show covering the Kubernetes and Cloud Native ecosystem.&lt;/p>
&lt;p>Our release conversations shine a light on the team that puts together each Kubernetes release. Make sure you &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe, wherever you get your podcasts&lt;/a> so you catch the story of 1.23.&lt;/p>
&lt;p>And in case you're interested in why the show has been on a hiatus the last few weeks, all will be revealed in the next episode!&lt;/p>
&lt;p>&lt;em>This transcript has been lightly edited and condensed for clarity.&lt;/em>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>CRAIG BOX: Welcome to the show, Savitha.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Hey, Craig. Thanks for having me on the show. How are you today?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I'm very well, thank you. I've interviewed a lot of people on the show, and you're actually the first person who's asked that of me.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I'm glad. It's something that I always do. I just want to make sure the other person is good and happy.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: That's very kind of you. Thank you for kicking off on a wonderful foot there. I want to ask first of all — you grew up in Chennai. My association with Chennai is the &lt;a href="https://en.wikipedia.org/wiki/Chennai_Super_Kings">Super Kings cricket team&lt;/a>. Was cricket part of your upbringing?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Yeah. Actually, a lot. My mom loves watching cricket. I have a younger brother, and when we were growing up, we used to play cricket on the terrace. Everyone surrounding me, my best friends — and even now, my partner — loves watching cricket, too. Cricket is a part of my life.&lt;/p>
&lt;p>I stopped watching it a while ago, but I still enjoy a good game.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's probably a bit harder in the US. Everything's in a different time zone. I find, with my cricket team being on the other side of the world, that it's a lot easier when they're playing near me, as opposed to trying to keep up with what they're doing when they're playing at 3:00 in the morning.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: That is actually one of the things that made me lose touch with cricket. I'm going to give you a piece of interesting information. I never supported Chennai Super Kings. I always supported &lt;a href="https://en.wikipedia.org/wiki/Royal_Challengers_Bangalore">Royal Challengers of Bangalore&lt;/a>.&lt;/p>
&lt;p>I once went to the stadium, and it was a match between the Chennai Super Kings and the RCB. I was the only one who was cheering whenever the RCB hit a 6, or when they were scoring. I got the stares of thousands of people looking at me. I'm like, &amp;quot;what are you doing?&amp;quot; My friends are like, &amp;quot;you're going to get us killed! Just stop screaming!&amp;quot;&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I hear you. As a New Zealander in the UK, there are a lot of international cricket matches I've been to where I am one of the few people dressed in the full beige kit. But I have to ask, why an affiliation with a different team?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I'm not sure. When the IPL came out, I really liked Virat Kohli. He was playing for RCB at that time, and I think pretty much that's it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Well, what I know about the Chennai Super Kings is that their coach is New Zealand's finest batsmen and &lt;a href="https://www.youtube.com/watch?v=vSZAaUCAclw">air conditioning salesman&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Stephen_Fleming">Stephen Fleming&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Oh, really?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yeah, he's a dead ringer for the guy who played the &lt;a href="https://s1.reutersmedia.net/resources/r/?m=02&amp;amp;d=20061130&amp;amp;t=2&amp;amp;i=153531&amp;amp;w=&amp;amp;fh=545px&amp;amp;fw=&amp;amp;ll=&amp;amp;pl=&amp;amp;sq=&amp;amp;r=153531">yellow Wiggle&lt;/a> back in the day.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Oh, interesting. I remember the name, but I cannot put the picture and the name together. I stopped watching cricket once I moved to the States. Then, all my focus was on studies and extracurriculars. I have always been an introvert. The campus — it was a new thing for me — they had international festivals.&lt;/p>
&lt;p>And every week, they'd have some kind of new thing going on, so I'd go check them out. I wouldn't participate, but I did go out and check them out. That was a big feat for me around that time because a lot of people — and still, even now, a lot of people — they kind of scare me. I don't know how to make a conversation with everyone.&lt;/p>
&lt;p>I'll just go and say, &amp;quot;hi, how are you? OK, I'm good. I'm just going to move on&amp;quot;. And I'll just go to the next person. And after two hours, I'm out of that place.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Perhaps a pleasant side effect of the last 12 months — a lot fewer gatherings of people.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Could be that, but I'm so excited about KubeCon. But when I think about it, I'm like &amp;quot;oh my God. There's going to be a lot of people. What am I going to do? I'm going to meet all my friends over there&amp;quot;.&lt;/p>
&lt;p>Sometimes I have social anxiety like, what's going to happen?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What's going to happen is you're going to ask them how they are at the beginning, and they're immediately going to be set at ease.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: &lt;em>laughs&lt;/em> I hope so.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's talk a little bit, then, about your transition from India to the US. You did your undergraduate degree in computer science at the SSN College of Engineering. How did you end up at Arizona State?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I always wanted to pursue higher studies when I was in India, and I didn't have the opportunity immediately. Once I graduated from my school there, I went and I worked for a couple of years. My aim was always to get out of there and come here, do my graduate studies.&lt;/p>
&lt;p>Eventually, I want to do a PhD. I have an idea of what I want to do. I always wanted to keep studying. If there's an option that I could just keep studying and not do work or anything of that sort, I'd just pick that other one — I'll just keep studying.&lt;/p>
&lt;p>But unfortunately, you need money and other things to live and sustain in this world. So I'm like, OK, I'll take a break from studies, and I will work for a while.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The road to success is littered with dreams of PhDs. I have a lot of friends who thought that that was the path they were going to take, and they've had a beautiful career and probably aren't going to go back to study. Did you use the &lt;a href="https://en.wikipedia.org/wiki/MATLAB">Matlab&lt;/a> software at all while you were going through your schooling?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: No, unfortunately. That is a question that everyone asks. I have not used Matlab. I haven't used it even now. I don't use it for work. I didn't have any necessity for my school work. I didn't have anything to do with Matlab. I never analysed, or did data processing, or anything, with Matlab. So unfortunately, no.&lt;/p>
&lt;p>Everyone asks me like, you're working at &lt;a href="https://en.wikipedia.org/wiki/MathWorks">MathWorks&lt;/a>. Have you used Matlab? I'm like, no.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Fair enough. Nor have I. But it's been around since the late 1970s, so I imagine there are a lot of people who will have come across it at some point. Do you work with a lot of people who have been working on it that whole time?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Kind of. Not all the time, but I get to meet some folks who work on the product itself. Most of my interactions are with the infrastructure team and platform engineering teams at MathWorks. One other interesting fact is that when I joined the company — MathWorks has an extensive internal curriculum for training and learning, which I really love. They have an &amp;quot;Intro to Matlab&amp;quot; course, and that's on my bucket of things to do.&lt;/p>
&lt;p>It was like 500 years ago. I added it, and I never got to it. I'm like, OK, maybe this year at least I want to get to it and I want to learn something new. My partner used Matlab extensively. He misses it right now at his current employer. And he's like, &amp;quot;you have the entire licence! You have access to the entire suite and you haven't used it?&amp;quot; I'm like, &amp;quot;no!&amp;quot;&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Well, I have bad news for the idea of you doing a PhD, I'm sorry.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Another thing is that none of my family knew about the company MathWorks and Matlab. The only person who knew was my younger brother. He was so proud. He was like, &amp;quot;oh my God&amp;quot;.&lt;/p>
&lt;p>When he was 12 years old, he started getting involved in robotics and all that stuff. That's how he got introduced to Matlab. He goes absolutely bananas for the swag. So all the t-shirts, all the hoodies — any swag that I get from MathWorks goes to him, without saying.&lt;/p>
&lt;p>Over the five, six years, the things that I've got — there was only one sweatshirt that I kept for myself. Everything else I've just given to him. And he cherishes it. He's the only one in my family who knew about Matlab and MathWorks.&lt;/p>
&lt;p>Now, everyone knows, because I'm working there. They were initially like, I don't even know that company name. Is it like Amazon? I'm like, no, we make software that can send people to the moon. And we also make software that can do amazing robotic surgeries and even make a car drive on its own. That's something that I take immense pride in.&lt;/p>
&lt;p>I know I don't directly work on the product, but I'm enabling the people who are creating the product. I'm really, really proud of that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I think Jeff Bezos is working on at least two out of three of those disciplines that you mentioned before, so it's maybe a little bit like Amazon. One thing I've always thought about Matlab is that, because it's called Matlab, it solves that whole problem where &lt;a href="https://www.grammar.com/math_vs._maths">Americans call it math, and the rest of the world call it maths&lt;/a>. Why do Americans think there's only one math?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Definitely. I had trouble — growing up in India, it's always British English. And I had so much trouble when I moved here. So many things changed.&lt;/p>
&lt;p>One of the things is maths. I always got used to writing maths, physics, and everything.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: They don't call it &amp;quot;physic&amp;quot; in the US, do they?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: No, no, they don't. Luckily, they don't. That still stays &amp;quot;physics&amp;quot;. But math — I had trouble. It's maths. Even when you do the full abbreviations like mathematics and you are still calling it math, I'm like, mm.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: They can do the computer science abbreviation thing and call it math-7-S or whatever the number of letters is.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Just like Kubernetes. K-8-s.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Your path to Kubernetes is through MathWorks. They started out as a company making software which was distributed in a physical sense — boxed copies, if you will. I understand now there is a cloud version. Can I assume that that is where the two worlds intersect?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Kind of. I have interaction with the team that supports Matlab on the cloud, but I don't get to work with them on a day-to-day basis. They use Docker containers, and they are building the platform using Kubernetes. So yeah, a little bit of that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: So what exactly is the platform that you are engineering day to day?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Providing Kubernetes as a platform, obviously — that goes without saying — to some of the internal development teams. In the future we might expand it to more teams within the company. That is a focus area right now, so that's what we are doing. In the process, we might even get to work with the people who are deploying Matlab on the cloud, which is exciting.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now, your path to contribution to Kubernetes, you've said before, was through &lt;a href="https://github.com/kubernetes/website/pull/15588">fixing a 404 error on the Kubernetes.io website&lt;/a>. Do you remember what the page was?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I do. I was going to something for work, and I came across this changelog. In Kubernetes there's a nice page — once you got to the release page, there would be a long list of changelogs.&lt;/p>
&lt;p>One of the things that I fixed was, the person who worked on the feature had changed their GitHub handle, and that wasn't reflected on this page. So that was my first. I got curious and clicked on the links. One of the links was the handle, and that went to a 404. And I was like &amp;quot;Yeah, I'll just fix that. They have done all the hard work. They can get the credit that's due&amp;quot;.&lt;/p>
&lt;p>It was easy. It wasn't overwhelming for me to pick it up as my first issue. Before that I logged on around Kubernetes for about six to eight months without doing anything because it was just a lot.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One of the other things that you said about your initial contribution is that you had to learn how to use Git. As a very powerful tool, I find Git is a high barrier to entry for even contributing code to a project. When you want to contribute a blog post or documentation or a fix like you did before, I find it almost impossible to think how a new user would come along and do that. What was your process? Do you think that there's anything we can do to make that barrier lower for new contributors?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Of course. There are more and more tutorials available these days. There is a new contributor workshop. They actually have a &lt;a href="https://www.kubernetes.dev/docs/guide/github-workflow/">GitHub workflow section&lt;/a>, &lt;a href="https://www.kubernetes.dev/docs/guide/pull-requests/">how to do a pull request&lt;/a> and stuff like that. I know a couple of folks from SIG Docs that are working on which Git commands that you need, or how to get to writing something small and getting it committed. But more tutorials or more links to intro to Git would definitely help.&lt;/p>
&lt;p>The thing is also, someone like a documentation writer — they don't actually want to know the entirety of Git. Honestly, it's an ocean. I don't know how to do it. Most of the time, I still ask for help even though I work with Git on a day to day basis. There are several articles and a lot of help is available already within the community. Maybe we could just add a couple more to &lt;a href="https://kubernetes.dev/">kubernetes.dev&lt;/a>. That is an amazing site for all the new contributors and existing contributors who want to build code, who want to write documentation.&lt;/p>
&lt;p>We could just add a tutorial there like, &amp;quot;hey, don't know Git, you are new to Git? You just need to know these main things&amp;quot;.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I find it a shame, to be honest, that people need to use Git for that, by comparison to Wikipedia where you can come along, and even though it might be written in Markdown or something like it, it seems like the barrier is a lot lower. Similar to you, I always have to look up anything more complicated than the five or six Git commands that I use on a day to day basis. Even to do simple things, I basically just go and follow a recipe which I find on the internet.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: This is how I got introduced to one of the amazing mentors in Kubernetes. Everyone knows him by his handle, Dims. It was my second PR to the Kubernetes website, and I made a mistake. I destroyed the Git history. I could not push my reviews and comments — I addressed them. I couldn't push them back.&lt;/p>
&lt;p>My immediate thought was to delete it and recreate, do another pull request. But then I was like, &amp;quot;what happens to others who have already put effort into reviewing them?&amp;quot; I asked for help, and Dims was there.&lt;/p>
&lt;p>I would say I just got lucky he was there. And he was like, &amp;quot;OK, let me walk you through&amp;quot;. We did troubleshooting through Slack messages. I copied and pasted all the errors. Every single command that he said, I copied and pasted. And then he was like, &amp;quot;OK, run this one. Try this one. And do this one&amp;quot;.&lt;/p>
&lt;p>Finally, I got it fixed. So you know what I did? I went and I stored the command history somewhere local for the next time when I run into this problem. Luckily, I haven't. But I find the contributors so helpful. They are busy. They have a lot of things to do, but they take moments to stop and help someone who's new.&lt;/p>
&lt;p>That is also another part of the reason why I stay — I want to contribute more. It's mainly the community. It's the Kubernetes community. I know you asked me about Git, and I just took the conversation to the Kubernetes community. That's how my brain works.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: A lot of people in the community do that and think that's fantastic, obviously, people like Dims who are just floating around on Slack and seem to have endless time. I don't know how they do it.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I really want to know the secret for endless time. If I only had 48 hours in a day. I would sleep for 16 hours, and I would use the rest of the time for doing the things that I want.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: If I had a chance to sleep up to 48 hours a day, I think it'd be a lot more than 16.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Now, one of the areas that you've been contributing to Kubernetes is in the release team. In 1.18, you were a shadow for the docs role. You led that role in 1.19. And you were a release lead shadow for versions 1,20 and 1.21 before finally leading this release, 1.22, which we will talk about soon.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>How did you get involved? And how did you decide which roles to take as you went through that process?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: That is a topic I love to talk about. This was fresh when I started learning about Kubernetes and using Kubernetes at work. And I got so much help from the community, I got interested in contributing back.&lt;/p>
&lt;p>At the first KubeCon that I attended in 2018, in Seattle, they had a speed mentoring session. Now they call it &amp;quot;pod mentoring&amp;quot;. I went to the session, and said, &amp;quot;hey, I want to contribute. I don't know where to start&amp;quot;. And I got a lot of information on how to get started.&lt;/p>
&lt;p>One of the places was SIG Release and the release team. I came back and diligently attended all the SIG Release meetings for four to six months. And in between, I applied to the Kubernetes release team — 1.14 and 1.15. I didn't get through. So I took a little bit of a break, and I focused on doing some documentation work. Then I applied for 1.18.&lt;/p>
&lt;p>Since I was already working on some kinds of — not like full fledged &amp;quot;documentation&amp;quot; documentation, I still don't write. I eventually want to write something really nice and full fledged documentation like other awesome folks.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You'll need a lot more than 48 hours in your day to do that.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: &lt;em>laughing&lt;/em> That's how I applied for the docs role, because I know a little bit about the website. I've done a few pull requests and commits. That's how I got started. I applied for that one role, and I got selected for the 1.18 team. That's how my journey just took off.&lt;/p>
&lt;p>And the next release, I was leading the documentation team. And as everyone knows, the pandemic hit. It was one of the longest releases. I could lean back on the community. I would just wait for the release team meetings.&lt;/p>
&lt;p>It was my way of coping with the pandemic. It took my mind off. It was actually more than a release team, they were people. They were all people first, and we took care of each other. So it felt good.&lt;/p>
&lt;p>And then, I became a release lead shadow for 1.20 and 1.21 because I wanted to know more. I wanted to learn more. I wasn't ready. I still don't feel ready, but I have led 1.22. So if I could do it, anyone could do it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: How much of this work is day job?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I am lucky to be blessed with an awesome team. I do most of my work after work, but there have been times where I have to take meetings and attend to immediate urgent stuff. During the time of exception requests and stuff like that, I take a little bit of time from my work.&lt;/p>
&lt;p>My team has been wonderful: they support me in all possible ways, and the management as well. Other than the meetings, I don't do much of the work during the day job. It just takes my focus and attention away too much, and I end up having to spend a lot of time sitting in front of the computer, which I don't like.&lt;/p>
&lt;p>Before the pandemic I had a good work life balance. I'd just go to work at 7:00, 7:30, and I'd be back by 4 o'clock. I never touched my laptop ever again. I left all work behind when I came home. So right now, I'm still learning how to get through.&lt;/p>
&lt;p>I try to limit the amount of open source work that I do during work time. The release lead shadow and the release lead job — they require a lot of time, effort. So on average, I'd be spending two to three hours post work time on the release activities.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Before the pandemic, everyone was worried that if we let people work from home, they wouldn't work enough. I think the opposite has actually happened, is that now we're worried that if we let people work from home, they will just get on the computer in the morning and you'll have to pry it out of their hands at midnight.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Yeah, I think the productivity has increased at least twofold, I would say, for everyone, once they started working from home.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: But at the expense of work-life balance, though, because as you say, when you're sitting in the same chair in front of, perhaps, the same computer doing your MathWorks work and then your open source work, they kind of can blur into one perhaps?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: That is a challenge. I face it every day. But so many others are also facing it. I implemented a few little tricks to help me. When I used to come back home from work, the first thing I would do is remove my watch. That was an indication that OK, I'm done.&lt;/p>
&lt;p>That's the thing that I still do. I just remove my watch, and I just keep it right where my workstation is. And I just close the door so that I never look back. Even going past the room, I don't get a glimpse of my work office. I start implementing tiny little things like that to avoid burnout.&lt;/p>
&lt;p>I think I'm still facing a little bit of burnout. I don't know if I have fully recovered from it. I constantly feel like I need a vacation. And I could just take a vacation for like a month or two. If it's possible, I will just do it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I do hope that travel opens up for everyone as an opportunity because I know that, for a lot of people, it's not so much they've been working from home but they've been living at work. The idea of taking vacation effectively means, well, I've been stuck in the same place, if I've been under a lockdown. It's hard to justify that. It will be good as things improve worldwide for us to be able to start focusing more on mental health and perhaps getting away from the &amp;quot;everything room,&amp;quot; as I sometimes call it.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I'm totally looking forward to it. I hope that travel opens up and I could go home and I could meet my siblings and my aunt and my parents.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Catch a cricket match?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Yeah. Probably yes, if I have company and if there is anything interesting happening around the time. I don't mind going back to the Chepauk Stadium and catching a match or two.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's turn now to the recently released &lt;a href="https://kubernetes.io/blog/2021/08/04/kubernetes-1-22-release-announcement/">Kubernetes 1.22&lt;/a>. Congratulations on the launch.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Thank you.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Each launch comes with a theme and a mascot or a logo. What is the theme for this release?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: The theme for the release is reaching new peaks. I am fascinated with a lot of space travel and chasing stars, the Milky Way. The best place to do that is over the top of a mountain. So that is the release logo, basically. It's a mountain — Mount Rainier. On top of that, there is a Kubernetes flag, and it's overlooking the Milky Way.&lt;/p>
&lt;p>It's also symbolic that with every release, that we are achieving something new, bigger, and better, and we are making the release awesome. So I just wanted to incorporate that into the team as to say, we are achieving new things with every release. That's the &amp;quot;reaching new peaks&amp;quot; theme.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The last couple of releases have both been incrementally larger — as a result, perhaps, of the fact there are now only three releases per year rather than four. There were also changes to the process, where the work has been driven a lot more by the SIGs than by the release team having to go and ask the SIGs what was going on. What can you say about the size and scope of the 1.22 release?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: The 1.22 release is the largest release to date. We have 56 enhancements if I'm not wrong, and we have a good amount of features that's graduated as stable. You can now say that Kubernetes as a project has become more mature because you see new features coming in. At the same time, you see the features that weren't used getting deprecated — we have like three deprecations in this release.&lt;/p>
&lt;p>Aside from that fact, we also have a big team that's supporting one of the longest releases. This is the first official release cycle after the cadence KEP got approved. Officially, we are at four months, even though 1.19 was six months, and 1.21 was like 3 and 1/2 months, I think, this is the first one after the official KEP approval.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What changes did you make to the process knowing that you had that extra month?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: One of the things the community had asked for is more time for development. We tried to incorporate that in the release schedule. We had about six weeks between the enhancements freeze and the code freeze. That's one.&lt;/p>
&lt;p>It might not be visible to everyone, but one of the things that I wanted to make sure of was the health of the team — since it was a long, long release, we had time to plan out, and not have everyone work during the weekends or during their evenings or time off. That actually helped everyone keep their sanity, and also in making good progress and delivering good results at the end of the release. That's one of the process improvements that I'd call out.&lt;/p>
&lt;p>We got better by making a post during the exception request process. Everyone works around the world. People from the UK start a little earlier than the people in the US East Coast. The West Coast starts three hours later than the East Coast. We used to make a post every Friday evening saying &amp;quot;hey, we actually received this many requests. We have addressed a number of them. We are waiting on a couple, or whatever. All the release team members are done for the day. We will see you around on Monday. Have a good weekend.&amp;quot; Something like that.&lt;/p>
&lt;p>We set the expectations from the community as well. We understand things are really important and urgent, but we are done. This gave everyone their time back. They don't have to worry over the weekend thinking like, hey, what's happening? What's happening in the release? They could spend time with their family, or they could do whatever they want to do, like go on a hike, or just sit and watch TV.&lt;/p>
&lt;p>There have been weekends that I just did that. I just binge-watched a series. That's what I did.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Any recommendations?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I'm a big fan of Marvel, so I have watched the new &lt;a href="https://en.wikipedia.org/wiki/Loki_(TV_series)">Loki&lt;/a>, which I really love. Loki is one of my favourite characters in Marvel. And I also liked &lt;a href="https://en.wikipedia.org/wiki/WandaVision">WandaVision&lt;/a>. That was good, too.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I've not seen Loki yet, but I've heard it described as the best series of Doctor Who in the last few years.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Really?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There must be an element of time-travelling in there if that's how people are describing it.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: You should really go and watch it whenever you have time. It's really amazing. I might go back and watch it again because I might have missed bits and pieces. That always happens in Marvel movies and the episodes; you need to watch them a couple of times to catch, &amp;quot;oh, this is how they relate&amp;quot;.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yes, the mark of good media that you want to immediately go back and watch it again once you've seen it.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Let's look now at some of the new features in Kubernetes 1.22. A couple of things that have graduated to general availability — server-side apply, external credential providers, a couple of new security features — the replacement for pod security policy has been announced, and seccomp is now available by default.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Do you have any favourite features in 1.22 that you'd like to discuss?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I have a lot of them. All my favourite features are related to security. OK, one of them is not security, but a major theme of my favourite KEPs is security. I'll start with the &lt;a href="https://github.com/kubernetes/enhancements/issues/2413">default seccomp&lt;/a>. I think it will help make clusters secure by default, and may assist in preventing more vulnerabilities, which means less headaches for the cluster administrators.&lt;/p>
&lt;p>This is close to my heart because the base of the MathWorks platform is provisioning Kubernetes clusters. Knowing that they are secure by default will definitely provide me with some good sleep. And also, I'm paranoid about security most of the time. I'm super interested in making everything secure. It might get in the way of making the users of the platform angry because it's not usable in any way.&lt;/p>
&lt;p>My next one is &lt;a href="https://github.com/kubernetes/enhancements/issues/2033">rootless Kubelet&lt;/a>. That feature's going to enable the cluster admin, the platform developers to deploy Kubernetes components to run in a user namespace. And I think that is also a great addition.&lt;/p>
&lt;p>Like you mention, the most awaited drop in for the PSP replacement is here. It's &lt;a href="https://github.com/kubernetes/enhancements/issues/2579">pod admission control&lt;/a>. It lets cluster admins apply the pod security standards. And I think it's just not related to the cluster admins. I might have to go back and check on that. Anyone can probably use it — the developers and the admins alike.&lt;/p>
&lt;p>It also supports various modes, which is most welcome. There are times where you don't want to just cut the users off because they are trying to do something which is not securely correct. You just want to warn them, hey, this is what you are doing. This might just cause a security issue later, so you might want to correct it. But you just don't want to cut them off from using the platform, or them trying to attempt to do something — deploy their workload and get their day-to-day job done. That is something that I really like, that it also supports a warning mechanism.&lt;/p>
&lt;p>Another one which is not security is &lt;a href="https://github.com/kubernetes/enhancements/issues/2400">node swap support&lt;/a>. Kubernetes didn't have support for swap before, but it is taken into consideration now. This is an alpha feature. With this, you can take advantage of the swap, which is provisioned on the Linux VMs.&lt;/p>
&lt;p>Some of the workloads — when they are deployed, they might need a lot of swap for the start-up — example, like Node and Java applications, which I just took out of their KEP user stories. So if anyone's interested, they can go and look in the KEP. That's useful. And it also increases the node stability and whatnot. So I think it's going to be beneficial for a lot of folks.&lt;/p>
&lt;p>We know how Java and containers work. I think it has gotten better, but five years ago, it was so hard to get a Java application to fit in a small container. It always needed a lot of memory, swap, and everything to start up and run. I think this will help the users and help the admins and keep the cost low, and it will tie into so many other things as well. I'm excited about that feature.&lt;/p>
&lt;p>Another feature that I want to just call out — I don't use Windows that much, but I just want to give a shout out to the folks who are doing an amazing job bringing all the Kubernetes features to Windows as well, to give a seamless experience.&lt;/p>
&lt;p>One of the things is &lt;a href="https://github.com/kubernetes/enhancements/issues/1981">Windows privileged containers&lt;/a>. I think it went alpha this release. And that is a wonderful addition, if you ask me. It can take advantage of whatever that's happening on the Linux side. And they can also port it over and see, OK, I can now run Windows containers in a privileged mode.&lt;/p>
&lt;p>So whatever they are trying to achieve, they can do it. So that's a noteworthy mention. I need to give a shout out for the folks who work and make things happen in the Windows ecosystem as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One of the things that's great about the release process is the continuity between groups and teams. There's always an emeritus advisor who was a lead from a previous release. One thing that I always ask when I do these interviews is, what is the advice that you give to the next person? When &lt;a href="https://kubernetespodcast.com/episode/146-kubernetes-1.21/">we talked to Nabarun for the 1.21 interview&lt;/a>, he said that his advice to you would be &amp;quot;do, delegate, and defer&amp;quot;. Figure out what you can do, figure out what you can ask other people to do, and figure out what doesn't need to be done. Were you able to take that advice on board?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Yeah, you won't believe it. &lt;a href="https://twitter.com/KubernetesPod/status/1423188323347177474/photo/3">I have it right here stuck to my monitor.&lt;/a>&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Next to your Git cheat sheet?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: &lt;em>laughs&lt;/em> Absolutely. I just have it stuck there. I just took a look at it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Someone that you will have been able to delegate and defer to is Rey Lejano from Rancher Labs and SUSE, who is the release lead to be for 1.23.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I want to tell Rey to beware of the team's mental health. Schedule in such a way that it avoids burnout. Check in, and make sure that everyone is doing good. If they need some kind of help, create a safe space where they can actually ask for help, if they want to step back, if they need someone to cover.&lt;/p>
&lt;p>I think that is most important. The releases are successful based on the thousands and thousands of contributors. But when it comes to a release team, you need to have a healthy team where people feel they are in a good place and they just want to make good contributions, which means they want to be heard. That's one thing that I want to tell Rey.&lt;/p>
&lt;p>Also collaborate and learn from each other. I constantly learn. I think the team was 39 folks, including me. Every day I learned something or the other, even starting from how to interact.&lt;/p>
&lt;p>Sometimes I have learned more leadership skills from my release lead shadows. They are awesome, and they are mature. I constantly learn from them, and I admire them a lot.&lt;/p>
&lt;p>It also helps to have good, strong individuals in the team who can step up and help when needed. For example, unfortunately, we lost one of our teammates after the start of the release cycle. That was tragic. His name was &lt;a href="https://github.com/cncf/memorials/blob/main/peeyush-gupta.md">Peeyush Gupta&lt;/a>. He was an awesome and wonderful human — very warm.&lt;/p>
&lt;p>I didn't get more of a chance to interact with him. I had exchanged a few Slack messages, but I got his warm personality. I just want to take a couple of seconds to remember him. He was awesome.&lt;/p>
&lt;p>After we lost him, we had this strong person from the team step up and lead the communications, who had never been a part of the release team before at all. He was a shadow for the first time. His name is Jesse Butler. So he stepped up, and he just took it away. He ran the comms show for 1.22.&lt;/p>
&lt;p>That's what the community is about. You take care of team members, and the team will take care of you. So that's one other thing that I want to let Rey know, and maybe whoever — I think it's applicable overall.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There's a link to a &lt;a href="https://milaap.org/fundraisers/support-peeyush-gupta-family-education">family education fund for Peeyush Gupta&lt;/a>, which you can find in the show notes.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>Five releases in a row now you've been a member of the release team. Will you be putting your feet up now for 1.23?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I am going to take a break for a while. In the future, I want to be contributing, if not the release team, the SIG Release and the release management effort. But right now, I have been there for five releases. And I feel like, OK, I just need a little bit of fresh air.&lt;/p>
&lt;p>And also the pandemic and the burnout has caught up, so I'm going to take a break from certain contributions. You will see me in the future. I will be around, but I might not be actively participating in the release team activities. I will be around the community. Anyone can reach out to me. They all know my Slack, so they can just reach out to me via Slack or Twitter.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yes, your Twitter handle is CoffeeArtGirl. Does that mean that you'll be spending some time working on your lattes?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I am very bad at making lattes. The coffee art means that I used to &lt;a href="https://twitter.com/KubernetesPod/status/1423188323347177474/photo/1">make art with coffee&lt;/a>. You get instant coffee powder and just mix it with water. You get the colours, very beautiful brown colours. I used to make art using that.&lt;/p>
&lt;p>And I love coffee. So I just combined all the words together. And I had to come up with it in a span of one hour or so because I was joining this 'meet our contributors' panel. And Paris asked me, &amp;quot;do you have a Twitter handle?&amp;quot; I was planning to create one, but I didn't have the time.&lt;/p>
&lt;p>I'm like, well, let me just think what I could just come up with real quick. So I just came up with that. So that's the story behind my Twitter handle. Everyone's interested in it. You are not the first person you have asked me or mentioned about it. So many others are like, why coffee art?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And you are also interested in art with perhaps other materials?&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Yes. My interests keep changing. I used to do pebble art. It's just collecting pebbles from wherever I go, and I used to paint on them. I used to use watercolour, but I want to come back to watercolour sometime.&lt;/p>
&lt;p>My recent interests are coloured pencils, which came back. When I was very young, I used to do a lot of coloured pencils. And then I switched to watercolours and oil painting. So I just go around in circles.&lt;/p>
&lt;p>One of the hobbies that I picked up during a pandemic is crochet. I made a scarf for Mother's Day. My mum and my dad were here last year. They got stuck because of the pandemic, and they couldn't go back home. So they stayed with me for 10 months. That is the jackpot that I had, that I got to spend so much time with my parents after I moved to the US.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And they got rewarded with a scarf.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: Yeah.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One to share between them.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: I started making a blanket for my dad. And it became so heavy, I might have to just pick up some lighter yarn. I still don't know the differences between different kinds of yarns, but I'm getting better.&lt;/p>
&lt;p>I started out because I wanted to make these little toys. They call them &lt;a href="https://en.wikipedia.org/wiki/Amigurumi">amigurumi&lt;/a> in the crochet world. I wanted to make them. That's why I started out. I'm trying. I made &lt;a href="https://twitter.com/KubernetesPod/status/1423188323347177474/photo/2">a little cat&lt;/a> which doesn't look like a cat, but it is a cat. I have to tell everyone that it's a cat so that they don't mock me later, but.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's an artistic interpretation of a cat.&lt;/strong>&lt;/p>
&lt;p>SAVITHA RAGHUNATHAN: It definitely is!&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/coffeeartgirl">Savitha Raghunathan&lt;/a>, now a Senior Software Engineer at Red Hat, served as the Kubernetes 1.22 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Quality-of-Service for Memory Resources</title><link>https://kubernetes.io/blog/2021/11/26/qos-memory-resources/</link><pubDate>Fri, 26 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/11/26/qos-memory-resources/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Tim Xu (Tencent Cloud)&lt;/p>
&lt;p>Kubernetes v1.22, released in August 2021, introduced a new alpha feature that improves how Linux nodes implement memory resource requests and limits.&lt;/p>
&lt;p>In prior releases, Kubernetes did not support memory quality guarantees.
For example, if you set container resources as follows:&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: example
spec:
containers:
- name: nginx
resources:
requests:
memory: &amp;quot;64Mi&amp;quot;
cpu: &amp;quot;250m&amp;quot;
limits:
memory: &amp;quot;64Mi&amp;quot;
cpu: &amp;quot;500m&amp;quot;
&lt;/code>&lt;/pre>&lt;p>&lt;code>spec.containers[].resources.requests&lt;/code>(e.g. cpu, memory) is designed for scheduling. When you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each node has a maximum capacity for each of the resource types: the amount of CPU and memory it can provide for Pods. The scheduler ensures that, for each resource type, the sum of the resource requests of the scheduled Containers is less than the capacity of the node.&lt;/p>
&lt;p>&lt;code>spec.containers[].resources.limits&lt;/code> is passed to the container runtime when the kubelet starts a container. CPU is considered a &amp;quot;compressible&amp;quot; resource. If your app starts hitting your CPU limits, Kubernetes starts throttling your container, giving your app potentially worse performance. However, it won’t be terminated. That is what &amp;quot;compressible&amp;quot; means.&lt;/p>
&lt;p>In cgroup v1, and prior to this feature, the container runtime never took into account and effectively ignored spec.containers[].resources.requests[&amp;quot;memory&amp;quot;]. This is unlike CPU, in which the container runtime consider both requests and limits. Furthermore, memory actually can't be compressed in cgroup v1. Because there is no way to throttle memory usage, if a container goes past its memory limit it will be terminated by the kernel with an OOM (Out of Memory) kill.&lt;/p>
&lt;p>Fortunately, cgroup v2 brings a new design and implementation to achieve full protection on memory. The new feature relies on cgroups v2 which most current operating system releases for Linux already provide. With this experimental feature, &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/">quality-of-service for pods and containers&lt;/a> extends to cover not just CPU time but memory as well.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>Memory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes. Memory requests and limits of containers in pod are used to set specific interfaces &lt;code>memory.min&lt;/code> and &lt;code>memory.high&lt;/code> provided by the memory controller. When &lt;code>memory.min&lt;/code> is set to memory requests, memory resources are reserved and never reclaimed by the kernel; this is how Memory QoS ensures the availability of memory for Kubernetes pods. And if memory limits are set in the container, this means that the system needs to limit container memory usage, Memory QoS uses &lt;code>memory.high&lt;/code> to throttle workload approaching it's memory limit, ensuring that the system is not overwhelmed by instantaneous memory allocation.&lt;/p>
&lt;p>&lt;img src="./memory-qos-cal.svg" alt="">&lt;/p>
&lt;p>The following table details the specific functions of these two parameters and how they correspond to Kubernetes container resources.&lt;/p>
&lt;table>
&lt;tr>
&lt;th style="text-align:center">File&lt;/th>
&lt;th style="text-align:center">Description&lt;/th>
&lt;/tr>
&lt;tr>
&lt;td>memory.min&lt;/td>
&lt;td>&lt;code>memory.min&lt;/code> specifies a minimum amount of memory the cgroup must always retain, i.e., memory that can never be reclaimed by the system. If the cgroup's memory usage reaches this low limit and can’t be increased, the system OOM killer will be invoked.
&lt;br>
&lt;br>
&lt;i>We map it to the container's memory request&lt;/i>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory.high&lt;/td>
&lt;td>&lt;code>memory.high&lt;/code> is the memory usage throttle limit. This is the main mechanism to control a cgroup's memory use. If a cgroup's memory use goes over the high boundary specified here, the cgroup’s processes are throttled and put under heavy reclaim pressure. The default is max, meaning there is no limit.
&lt;br>
&lt;br>
&lt;i>We use a formula to calculate &lt;code>memory.high&lt;/code>, depending on container's memory limit or node allocatable memory (if container's memory limit is empty) and a throttling factor. Please refer to the KEP for more details on the formula.&lt;/i>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>When container memory requests are made, kubelet passes &lt;code>memory.min&lt;/code> to the back-end CRI runtime (possibly containerd, cri-o) via the &lt;code>Unified&lt;/code> field in CRI during container creation. The &lt;code>memory.min&lt;/code> in container level cgroup will be set to:&lt;/p>
&lt;p>&lt;img src="./container-memory-min.svg" alt="">&lt;br>
&lt;sub>i: the i&lt;sup>th&lt;/sup> container in one pod&lt;/sub>&lt;/p>
&lt;p>Since the &lt;code>memory.min&lt;/code> interface requires that the ancestor cgroup directories are all set, the pod and node cgroup directories need to be set correctly.&lt;/p>
&lt;p>&lt;code>memory.min&lt;/code> in pod level cgroup:&lt;br>
&lt;img src="./pod-memory-min.svg" alt="">&lt;br>
&lt;sub>i: the i&lt;sup>th&lt;/sup> container in one pod&lt;/sub>&lt;/p>
&lt;p>&lt;code>memory.min&lt;/code> in node level cgroup:&lt;br>
&lt;img src="./node-memory-min.svg" alt="">&lt;br>
&lt;sub>i: the i&lt;sup>th&lt;/sup> pod in one node, j: the j&lt;sup>th&lt;/sup> container in one pod&lt;/sub>&lt;/p>
&lt;p>Kubelet will manage the cgroup hierarchy of the pod level and node level cgroups directly using runc libcontainer library, while container cgroup limits are managed by the container runtime.&lt;/p>
&lt;p>For memory limits, in addition to the original way of limiting memory usage, Memory QoS adds an additional feature of throttling memory allocation. A throttling factor is introduced as a multiplier (default is 0.8). If the result of multiplying memory limits by the factor is greater than memory requests, kubelet will set &lt;code>memory.high&lt;/code> to the value and use &lt;code>Unified&lt;/code> via CRI. And if the container does not specify memory limits, kubelet will use node allocatable memory instead. The &lt;code>memory.high&lt;/code> in container level cgroup is set to:&lt;/p>
&lt;p>&lt;img src="./container-memory-high.svg" alt="">&lt;br>
&lt;sub>i: the i&lt;sup>th&lt;/sup> container in one pod&lt;/sub>&lt;/p>
&lt;p>This can can help improve stability when pod memory usage increases, ensuring that memory is throttled as it approaches the memory limit.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>Here are the prerequisites for enabling Memory QoS on your Linux node, some of these are related to &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2254-cgroup-v2">Kubernetes support for cgroup v2&lt;/a>.&lt;/p>
&lt;ol>
&lt;li>Kubernetes since v1.22&lt;/li>
&lt;li>&lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> since v1.0.0-rc93; &lt;a href="https://containerd.io/">containerd&lt;/a> since 1.4; &lt;a href="https://cri-o.io/">cri-o&lt;/a> since 1.20&lt;/li>
&lt;li>Linux kernel minimum version: 4.15, recommended version: 5.2+&lt;/li>
&lt;li>Linux image with cgroupv2 enabled or enabling cgroupv2 unified_cgroup_hierarchy manually&lt;/li>
&lt;/ol>
&lt;p>OCI runtimes such as runc and crun already support cgroups v2 &lt;a href="https://github.com/opencontainers/runtime-spec/blob/master/config-linux.md#unified">&lt;code>Unified&lt;/code>&lt;/a>, and Kubernetes CRI has also made the desired changes to support passing &lt;a href="https://github.com/kubernetes/kubernetes/pull/102578">&lt;code>Unified&lt;/code>&lt;/a>. However, CRI Runtime support is required as well. Memory QoS in Alpha phase is designed to support containerd and cri-o. Related PR &lt;a href="https://github.com/containerd/containerd/pull/5627">Feature: containerd-cri support LinuxContainerResources.Unified #5627&lt;/a> has been merged and will be released in containerd 1.6. CRI-O &lt;a href="https://github.com/cri-o/cri-o/pull/5207">implement kube alpha features for 1.22 #5207&lt;/a> is still in WIP.&lt;/p>
&lt;p>With those prerequisites met, you can enable the memory QoS feature gate (see &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/">Set kubelet parameters via a config file&lt;/a>).&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>You can find more details as follows:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2570-memory-qos/#readme">Support Memory QoS with cgroup v2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2254-cgroup-v2/#readme">cgroup v2&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>You can reach SIG Node by several means:&lt;/p>
&lt;ul>
&lt;li>Slack: &lt;a href="https://kubernetes.slack.com/messages/sig-node">#sig-node&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-node">Mailing list&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/labels/sig%2Fnode">Open Community Issues/PRs&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>You can also contact me directly:&lt;/p>
&lt;ul>
&lt;li>GitHub / Slack: @xiaoxubeii&lt;/li>
&lt;li>Email: &lt;a href="mailto:xiaoxubeii@gmail.com">xiaoxubeii@gmail.com&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Dockershim removal is coming. Are you ready?</title><link>https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Sergey Kanzhelev, Google. With reviews from Davanum Srinivas, Elana Hashman, Noah Kantrowitz, Rey Lejano.&lt;/p>
&lt;p>Last year we announced that Dockershim is being deprecated: &lt;a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">Dockershim Deprecation FAQ&lt;/a>.
Our current plan is to remove dockershim from the Kubernetes codebase soon.
We are looking for feedback from you whether you are ready for dockershim
removal and to ensure that you are ready when the time comes.
&lt;strong>Please fill out this survey: &lt;a href="https://forms.gle/svCJmhvTv78jGdSx8">https://forms.gle/svCJmhvTv78jGdSx8&lt;/a>&lt;/strong>.&lt;/p>
&lt;p>The dockershim component that enables Docker as a Kubernetes container runtime is
being deprecated in favor of runtimes that directly use the &lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface&lt;/a>
created for Kubernetes. Many Kubernetes users have migrated to
other container runtimes without problems. However we see that dockershim is
still very popular. You may see some public numbers in recent &lt;a href="https://www.datadoghq.com/container-report/#8">Container Report&lt;/a> from DataDog.
Some Kubernetes hosting vendors just recently enabled other runtimes support
(especially for Windows nodes). And we know that many third party tools vendors
are still not ready: &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/#telemetry-and-security-agent-vendors">migrating telemetry and security agents&lt;/a>.&lt;/p>
&lt;p>At this point, we believe that there is feature parity between Docker and the
other runtimes. Many end-users have used our &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">migration guide&lt;/a>
and are running production workload using these different runtimes. The plan of
record today is that dockershim will be removed in version 1.24, slated for
release around April of next year. For those developing or running alpha and
beta versions, dockershim will be removed in December at the beginning of the
1.24 release development cycle.&lt;/p>
&lt;p>There is only one month left to give us feedback. We want you to tell us how
ready you are.&lt;/p>
&lt;p>&lt;strong>We are collecting opinions through this survey: &lt;a href="https://forms.gle/svCJmhvTv78jGdSx8">https://forms.gle/svCJmhvTv78jGdSx8&lt;/a>&lt;/strong>
To better understand preparedness for the dockershim removal, our survey is
asking the version of Kubernetes you are currently using, and an estimate of
when you think you will adopt Kubernetes 1.24. All the aggregated information
on dockershim removal readiness will be published.
Free form comments will be reviewed by SIG Node leadership. If you want to
discuss any details of migrating from dockershim, report bugs or adoption
blockers, you can use one of the SIG Node contact options any time:
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#contact">https://github.com/kubernetes/community/tree/master/sig-node#contact&lt;/a>&lt;/p>
&lt;p>Kubernetes is a mature project. This deprecation is another
step in the effort to get away from permanent beta features and providing more
stability and compatibility guarantees. With the migration from dockershim you
will get more flexibility and choice of container runtime features as well as
less dependencies of your apps on specific underlying technology. Please take
time to review the &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">dockershim migration documentation&lt;/a>
and consult your Kubernetes hosting vendor (if you have one) what container runtime options are available for you.
Read up &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#container-runtimes">container runtime documentation with instructions on how to use containerd and CRI-O&lt;/a>
to help prepare you when you're ready to upgrade to 1.24. CRI-O, containerd, and
Docker with &lt;a href="https://github.com/Mirantis/cri-dockerd">Mirantis cri-dockerd&lt;/a> are
not the only container runtime options, we encourage you to explore the &lt;a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category">CNCF landscape on container runtimes&lt;/a>
in case another suits you better.&lt;/p>
&lt;p>Thank you!&lt;/p></description></item><item><title>Blog: Non-root Containers And Devices</title><link>https://kubernetes.io/blog/2021/11/09/non-root-containers-and-devices/</link><pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/11/09/non-root-containers-and-devices/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Mikko Ylinen (Intel)&lt;/p>
&lt;p>The user/group ID related security settings in Pod's &lt;code>securityContext&lt;/code> trigger a problem when users want to
deploy containers that use accelerator devices (via &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">Kubernetes Device Plugins&lt;/a>) on Linux. In this blog
post I talk about the problem and describe the work done so far to address it. It's not meant to be a long story about getting the &lt;a href="https://github.com/kubernetes/kubernetes/issues/92211">k/k issue&lt;/a> fixed.&lt;/p>
&lt;p>Instead, this post aims to raise awareness of the issue and to highlight important device use-cases too. This is needed as Kubernetes works on new related features such as support for user namespaces.&lt;/p>
&lt;h2 id="why-non-root-containers-can-t-use-devices-and-why-it-matters">Why non-root containers can't use devices and why it matters&lt;/h2>
&lt;p>One of the key security principles for running containers in Kubernetes is the
principle of least privilege. The Pod/container &lt;code>securityContext&lt;/code> specifies the config
options to set, e.g., Linux capabilities, MAC policies, and user/group ID values to achieve this.&lt;/p>
&lt;p>Furthermore, the cluster admins are supported with tools like &lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">PodSecurityPolicy&lt;/a> (deprecated) or
&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission&lt;/a> (alpha) to enforce the desired security settings for pods that are being deployed in
the cluster. These settings could, for instance, require that containers must be &lt;code>runAsNonRoot&lt;/code> or
that they are forbidden from running with root's group ID in &lt;code>runAsGroup&lt;/code> or &lt;code>supplementalGroups&lt;/code>.&lt;/p>
&lt;p>In Kubernetes, the kubelet builds the list of &lt;a href="https://pkg.go.dev/k8s.io/cri-api@v0.22.1/pkg/apis/runtime/v1#Device">&lt;code>Device&lt;/code>&lt;/a> resources to be made available to a container
(based on inputs from the Device Plugins) and the list is included in the CreateContainer CRI message
sent to the CRI container runtime. Each &lt;code>Device&lt;/code> contains little information: host/container device
paths and the desired devices cgroups permissions.&lt;/p>
&lt;p>The &lt;a href="https://github.com/opencontainers/runtime-spec/blob/master/config-linux.md">OCI Runtime Spec for Linux Container Configuration&lt;/a>
expects that in addition to the devices cgroup fields, more detailed information about the devices
must be provided:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">{&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;type&amp;#34;: &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;lt;string&amp;gt;&amp;#34;&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;path&amp;#34;: &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;lt;string&amp;gt;&amp;#34;&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;major&amp;#34;: &lt;/span>&amp;lt;int64&amp;gt;,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;minor&amp;#34;: &lt;/span>&amp;lt;int64&amp;gt;,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;fileMode&amp;#34;: &lt;/span>&amp;lt;uint32&amp;gt;,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;uid&amp;#34;: &lt;/span>&amp;lt;uint32&amp;gt;,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;gid&amp;#34;: &lt;/span>&amp;lt;uint32&amp;gt;&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>},&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The CRI container runtimes (containerd, CRI-O) are responsible for obtaining this information
from the host for each &lt;code>Device&lt;/code>. By default, the runtimes copy the host device's user and group IDs:&lt;/p>
&lt;ul>
&lt;li>&lt;code>uid&lt;/code> (uint32, OPTIONAL) - id of device owner in the container namespace.&lt;/li>
&lt;li>&lt;code>gid&lt;/code> (uint32, OPTIONAL) - id of device group in the container namespace.&lt;/li>
&lt;/ul>
&lt;p>Similarly, the runtimes prepare other mandatory &lt;code>config.json&lt;/code> sections based on the CRI fields,
including the ones defined in &lt;code>securityContext&lt;/code>: &lt;code>runAsUser&lt;/code>/&lt;code>runAsGroup&lt;/code>, which become part of the POSIX
platforms user structure via:&lt;/p>
&lt;ul>
&lt;li>&lt;code>uid&lt;/code> (int, REQUIRED) specifies the user ID in the container namespace.&lt;/li>
&lt;li>&lt;code>gid&lt;/code> (int, REQUIRED) specifies the group ID in the container namespace.&lt;/li>
&lt;li>&lt;code>additionalGids&lt;/code> (array of ints, OPTIONAL) specifies additional group IDs in the container namespace to be added to the process.&lt;/li>
&lt;/ul>
&lt;p>However, the resulting &lt;code>config.json&lt;/code> triggers a problem when trying to run containers with
both devices added and with non-root uid/gid set via &lt;code>runAsUser&lt;/code>/&lt;code>runAsGroup&lt;/code>: the container user process
has no permission to use the device even when its group id (gid, copied from host) was permissive to
non-root groups. This is because the container user does not belong to that host group (e.g., via &lt;code>additionalGids&lt;/code>).&lt;/p>
&lt;p>Being able to run applications that use devices as non-root user is normal and expected to work so that
the security principles can be met. Therefore, several alternatives were considered to get the gap filled with what the PodSec/CRI/OCI supports today.&lt;/p>
&lt;h2 id="what-was-done-to-solve-the-issue">What was done to solve the issue?&lt;/h2>
&lt;p>You might have noticed from the problem definition that it would at least be possible to workaround
the problem by manually adding the device gid(s) to &lt;code>supplementalGroups&lt;/code>, or in
the case of just one device, set &lt;code>runAsGroup&lt;/code> to the device's group id. However, this is problematic because the device gid(s) may have
different values depending on the nodes' distro/version in the cluster. For example, with GPUs the following commands for different distros and versions return different gids:&lt;/p>
&lt;p>Fedora 33:&lt;/p>
&lt;pre>&lt;code>$ ls -l /dev/dri/
total 0
drwxr-xr-x. 2 root root 80 19.10. 10:21 by-path
crw-rw----+ 1 root video 226, 0 19.10. 10:42 card0
crw-rw-rw-. 1 root render 226, 128 19.10. 10:21 renderD128
$ grep -e video -e render /etc/group
video:x:39:
render:x:997:
&lt;/code>&lt;/pre>&lt;p>Ubuntu 20.04:&lt;/p>
&lt;pre>&lt;code>$ ls -l /dev/dri/
total 0
drwxr-xr-x 2 root root 80 19.10. 17:36 by-path
crw-rw---- 1 root video 226, 0 19.10. 17:36 card0
crw-rw---- 1 root render 226, 128 19.10. 17:36 renderD128
$ grep -e video -e render /etc/group
video:x:44:
render:x:133:
&lt;/code>&lt;/pre>&lt;p>Which number to choose in your &lt;code>securityContext&lt;/code>? Also, what if the &lt;code>runAsGroup&lt;/code>/&lt;code>runAsUser&lt;/code> values cannot be hard-coded because
they are automatically assigned during pod admission time via external security policies?&lt;/p>
&lt;p>Unlike volumes with &lt;code>fsGroup&lt;/code>, the devices have no official notion of &lt;code>deviceGroup&lt;/code>/&lt;code>deviceUser&lt;/code> that the CRI runtimes (or kubelet)
would be able to use. We considered using container annotations set by the device plugins (e.g., &lt;code>io.kubernetes.cri.hostDeviceSupplementalGroup/&lt;/code>) to get custom OCI &lt;code>config.json&lt;/code> uid/gid values.
This would have required changes to all existing device plugins which was not ideal.&lt;/p>
&lt;p>Instead, a solution that is &lt;em>seamless&lt;/em> to end-users without getting the device plugin vendors involved was preferred. The selected approach was
to re-use &lt;code>runAsUser&lt;/code> and &lt;code>runAsGroup&lt;/code> values in &lt;code>config.json&lt;/code> for devices:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">{&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;type&amp;#34;: &lt;/span>&lt;span style="color:#b44">&amp;#34;c&amp;#34;&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;path&amp;#34;: &lt;/span>&lt;span style="color:#b44">&amp;#34;/dev/foo&amp;#34;&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;major&amp;#34;: &lt;/span>&lt;span style="color:#666">123&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;minor&amp;#34;: &lt;/span>&lt;span style="color:#666">4&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;fileMode&amp;#34;: &lt;/span>&lt;span style="color:#666">438&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;uid&amp;#34;: &lt;/span>&amp;lt;runAsUser&amp;gt;,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;gid&amp;#34;: &lt;/span>&amp;lt;runAsGroup&amp;gt;&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>},&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With &lt;code>runc&lt;/code> OCI runtime (in non-rootless mode), the device is created (&lt;code>mknod(2)&lt;/code>) in
the container namespace and the ownership is changed to &lt;code>runAsUser&lt;/code>/&lt;code>runAsGroup&lt;/code> using &lt;code>chmod(2)&lt;/code>.&lt;/p>
&lt;p>&lt;div class="alert alert-info note callout" role="alert">
&lt;strong>Note:&lt;/strong> &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/">Rootless mode&lt;/a> and devices is not supported.
&lt;/div>
Having the ownership updated in the container namespace is justified as the user process is the only one accessing the device. Only &lt;code>runAsUser&lt;/code>/&lt;code>runAsGroup&lt;/code>
are taken into account, and, e.g., the &lt;code>USER&lt;/code> setting in the container is currently ignored.&lt;/p>
&lt;p>While it is likely that the &amp;quot;faulty&amp;quot; deployments (i.e., non-root &lt;code>securityContext&lt;/code> + devices) do not exist, to be absolutely sure no
deployments break, an opt-in config entry in both containerd and CRI-O to enable the new behavior was added. The following:&lt;/p>
&lt;p>&lt;code>device_ownership_from_security_context (bool)&lt;/code>&lt;/p>
&lt;p>defaults to &lt;code>false&lt;/code> and must be enabled to use the feature.&lt;/p>
&lt;h2 id="see-non-root-containers-using-devices-after-the-fix">See non-root containers using devices after the fix&lt;/h2>
&lt;p>To demonstrate the new behavior, let's use a Data Plane Development Kit (DPDK) application using hardware accelerators, Kubernetes CPU manager, and HugePages as an example. The cluster runs containerd with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-toml" data-lang="toml">[plugins]
[plugins.&lt;span style="color:#b44">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>]
device_ownership_from_security_context = &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>or CRI-O with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-toml" data-lang="toml">[crio.runtime]
device_ownership_from_security_context = &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>and the &lt;code>Guaranteed&lt;/code> QoS Class Pod that runs DPDK's crypto-perf test utility with this YAML:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#00f;font-weight:bold">...&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>qat-dpdk&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">runAsUser&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">runAsGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">2000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fsGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>crypto-perf&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>intel/crypto-perf:devel&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;3&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;128Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">qat.intel.com/generic&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;4&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">hugepages-2Mi&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;128Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;3&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;128Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">qat.intel.com/generic&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;4&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">hugepages-2Mi&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;128Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To verify the results, check the user and group ID that the container runs as:&lt;/p>
&lt;pre>&lt;code>$ kubectl exec -it qat-dpdk -c crypto-perf -- id
&lt;/code>&lt;/pre>&lt;p>They are set to non-zero values as expected:&lt;/p>
&lt;pre>&lt;code>uid=1000 gid=2000 groups=2000,3000
&lt;/code>&lt;/pre>&lt;p>Next, check the device node permissions (&lt;code>qat.intel.com/generic&lt;/code> exposes &lt;code>/dev/vfio/&lt;/code> devices) are accessible to &lt;code>runAsUser&lt;/code>/&lt;code>runAsGroup&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ kubectl exec -it qat-dpdk -c crypto-perf -- ls -la /dev/vfio
total 0
drwxr-xr-x 2 root root 140 Sep 7 10:55 .
drwxr-xr-x 7 root root 380 Sep 7 10:55 ..
crw------- 1 1000 2000 241, 0 Sep 7 10:55 58
crw------- 1 1000 2000 241, 2 Sep 7 10:55 60
crw------- 1 1000 2000 241, 10 Sep 7 10:55 68
crw------- 1 1000 2000 241, 11 Sep 7 10:55 69
crw-rw-rw- 1 1000 2000 10, 196 Sep 7 10:55 vfio
&lt;/code>&lt;/pre>&lt;p>Finally, check the non-root container is also allowed to create HugePages:&lt;/p>
&lt;pre>&lt;code>$ kubectl exec -it qat-dpdk -c crypto-perf -- ls -la /dev/hugepages/
&lt;/code>&lt;/pre>&lt;p>&lt;code>fsGroup&lt;/code> gives a &lt;code>runAsUser&lt;/code> writable HugePages emptyDir mountpoint:&lt;/p>
&lt;pre>&lt;code>total 0
drwxrwsr-x 2 root 3000 0 Sep 7 10:55 .
drwxr-xr-x 7 root root 380 Sep 7 10:55 ..
&lt;/code>&lt;/pre>&lt;h2 id="help-us-test-it-and-provide-feedback">Help us test it and provide feedback!&lt;/h2>
&lt;p>The functionality described here is expected to help with cluster security and the configurability of device permissions. To allow
non-root containers to use devices requires cluster admins to opt-in to the functionality by setting
&lt;code>device_ownership_from_security_context = true&lt;/code>. To make it a default setting, please test it and provide your feedback (via SIG-Node meetings or issues)!
The flag is available in CRI-O v1.22 release and queued for containerd v1.6.&lt;/p>
&lt;p>More work is needed to get it &lt;em>properly&lt;/em> supported. It is known to work with &lt;code>runc&lt;/code> but it also needs to be made to function
with other OCI runtimes too, where applicable. For instance, Kata Containers supports device passthrough and allows it to make devices
available to containers in VM sandboxes too.&lt;/p>
&lt;p>Moreover, the additional challenge comes with support of user names and devices. This problem is still &lt;a href="https://github.com/kubernetes/enhancements/pull/2101">open&lt;/a>
and requires more brainstorming.&lt;/p>
&lt;p>Finally, it needs to be understood whether &lt;code>runAsUser&lt;/code>/&lt;code>runAsGroup&lt;/code> are enough or if device specific settings similar to &lt;code>fsGroups&lt;/code> are needed in PodSpec/CRI v2.&lt;/p>
&lt;h2 id="thanks">Thanks&lt;/h2>
&lt;p>My thanks goes to Mike Brown (IBM, containerd), Peter Hunt (Redhat, CRI-O), and Alexander Kanevskiy (Intel) for providing all the feedback and good conversations.&lt;/p></description></item><item><title>Blog: Announcing the 2021 Steering Committee Election Results</title><link>https://kubernetes.io/blog/2021/11/08/steering-committee-results-2021/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/11/08/steering-committee-results-2021/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Kaslin Fields&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes/community/tree/master/events/elections/2021">2021 Steering Committee Election&lt;/a> is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2021. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.&lt;/p>
&lt;p>This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their &lt;a href="https://github.com/kubernetes/steering/blob/master/charter.md">charter&lt;/a>.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Christoph Blecker (&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>), Red Hat&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Stephen Augustus (&lt;a href="https://github.com/justaugustus">@justaugustus&lt;/a>), Cisco&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Paris Pittman (&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>), Apple&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Tim Pepper (&lt;a href="https://github.com/tpepper">@tpepper&lt;/a>), VMware&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>They join continuing members:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Davanum Srinivas (&lt;a href="https://github.com/dims">@dims&lt;/a>), VMware&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Jordan Liggitt (&lt;a href="https://github.com/liggitt">@liggitt&lt;/a>), Google&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Bob Killen (&lt;a href="https://github.com/mrbobbytables">@mrbobbytables&lt;/a>), Google&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Paris Pittman and Christoph Blecker are returning Steering Committee Members.&lt;/p>
&lt;h2 id="big-thanks">Big Thanks&lt;/h2>
&lt;p>Thank you and congratulations on a successful election to this round’s election officers:&lt;/p>
&lt;ul>
&lt;li>Alison Dowdney, (&lt;a href="https://github.com/alisondy">@alisondy&lt;/a>)&lt;/li>
&lt;li>Noah Kantrowitz (&lt;a href="https://github.com/coderanger">@coderanger&lt;/a>)&lt;/li>
&lt;li>Josh Berkus (&lt;a href="https://github.com/jberkus">@jberkus&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>Special thanks to Arnaud Meukam (&lt;a href="https://github.com/ameukam">@ameukam&lt;/a>), k8s-infra liaison, who enabled our voting software on community-owned infrastructure.&lt;/p>
&lt;p>Thanks to the Emeritus Steering Committee Members. Your prior service is appreciated by the community:&lt;/p>
&lt;ul>
&lt;li>Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>)&lt;/li>
&lt;li>Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>And thank you to all the candidates who came forward to run for election.&lt;/p>
&lt;h2 id="get-involved-with-the-steering-committee">Get Involved with the Steering Committee&lt;/h2>
&lt;p>This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee &lt;a href="https://github.com/kubernetes/steering/projects/1">backlog items&lt;/a> and weigh in by filing an issue or creating a PR against their &lt;a href="https://github.com/kubernetes/steering">repo&lt;/a>. They have an open meeting on &lt;a href="https://github.com/kubernetes/steering">the first Monday at 9:30am PT of every month&lt;/a> and regularly attend Meet Our Contributors. They can also be contacted at their public mailing list &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a>.&lt;/p>
&lt;p>You can see what the Steering Committee meetings are all about by watching past meetings on the &lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube Playlist&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>&lt;em>This post was written by the &lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing">Upstream Marketing Working Group&lt;/a>. If you want to write stories about the Kubernetes community, learn more about us.&lt;/em>&lt;/p></description></item><item><title>Blog: Use KPNG to Write Specialized kube-proxiers</title><link>https://kubernetes.io/blog/2021/10/18/use-kpng-to-write-specialized-kube-proxiers/</link><pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/10/18/use-kpng-to-write-specialized-kube-proxiers/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Lars Ekman (Ericsson)&lt;/p>
&lt;p>The post will show you how to create a specialized service kube-proxy
style network proxier using Kubernetes Proxy NG
&lt;a href="https://github.com/kubernetes-sigs/kpng">kpng&lt;/a> without interfering
with the existing kube-proxy. The kpng project aims at renewing the
the default Kubernetes Service implementation, the &amp;quot;kube-proxy&amp;quot;. An
important feature of kpng is that it can be used as a library to
create proxiers outside K8s. While this is useful for CNI-plugins that
replaces the kube-proxy it also opens the possibility for anyone to
create a proxier for a special purpose.&lt;/p>
&lt;h2 id="define-a-service-that-uses-a-specialized-proxier">Define a service that uses a specialized proxier&lt;/h2>
&lt;pre>&lt;code>apiVersion: v1
kind: Service
metadata:
name: kpng-example
labels:
service.kubernetes.io/service-proxy-name: kpng-example
spec:
clusterIP: None
ipFamilyPolicy: RequireDualStack
externalIPs:
- 10.0.0.55
- 1000::55
selector:
app: kpng-alpine
ports:
- port: 6000
&lt;/code>&lt;/pre>&lt;p>If the &lt;code>service.kubernetes.io/service-proxy-name&lt;/code> label is defined the
&lt;code>kube-proxy&lt;/code> will ignore the service. A custom controller can watch
services with the label set to it's own name, &amp;quot;kpng-example&amp;quot; in
this example, and setup specialized load-balancing.&lt;/p>
&lt;p>The &lt;code>service.kubernetes.io/service-proxy-name&lt;/code> label is &lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#servicekubernetesioservice-proxy-name">not
new&lt;/a>,
but so far is has been quite hard to write a specialized proxier.&lt;/p>
&lt;p>The common use for a specialized proxier is assumed to be handling
external traffic for some use-case not supported by K8s. In that
case &lt;code>ClusterIP&lt;/code> is not needed, so we use a &amp;quot;headless&amp;quot; service in this
example.&lt;/p>
&lt;h2 id="specialized-proxier-using-kpng">Specialized proxier using kpng&lt;/h2>
&lt;p>A &lt;a href="https://github.com/kubernetes-sigs/kpng">kpng&lt;/a> based proxier
consists of the &lt;code>kpng&lt;/code> controller handling all the K8s api related
functions, and a &amp;quot;backend&amp;quot; implementing the load-balancing. The
backend can be linked with the &lt;code>kpng&lt;/code> controller binary or be a
separate program communicating with the controller using gRPC.&lt;/p>
&lt;pre>&lt;code>kpng kube --service-proxy-name=kpng-example to-api
&lt;/code>&lt;/pre>&lt;p>This starts the &lt;code>kpng&lt;/code> controller and tell it to watch only services
with the &amp;quot;kpng-example&amp;quot; service proxy name. The &amp;quot;to-api&amp;quot; parameter
will open a gRPC server for backends.&lt;/p>
&lt;p>You can test this yourself outside your cluster. Please see the example
below.&lt;/p>
&lt;p>Now we start a backend that simply prints the updates from the
controller.&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f kpng-example.yaml
$ kpng-json | jq # (this is the backend)
{
&amp;quot;Service&amp;quot;: {
&amp;quot;Namespace&amp;quot;: &amp;quot;default&amp;quot;,
&amp;quot;Name&amp;quot;: &amp;quot;kpng-example&amp;quot;,
&amp;quot;Type&amp;quot;: &amp;quot;ClusterIP&amp;quot;,
&amp;quot;IPs&amp;quot;: {
&amp;quot;ClusterIPs&amp;quot;: {},
&amp;quot;ExternalIPs&amp;quot;: {
&amp;quot;V4&amp;quot;: [
&amp;quot;10.0.0.55&amp;quot;
],
&amp;quot;V6&amp;quot;: [
&amp;quot;1000::55&amp;quot;
]
},
&amp;quot;Headless&amp;quot;: true
},
&amp;quot;Ports&amp;quot;: [
{
&amp;quot;Protocol&amp;quot;: 1,
&amp;quot;Port&amp;quot;: 6000,
&amp;quot;TargetPort&amp;quot;: 6000
}
]
},
&amp;quot;Endpoints&amp;quot;: [
{
&amp;quot;IPs&amp;quot;: {
&amp;quot;V6&amp;quot;: [
&amp;quot;1100::202&amp;quot;
]
},
&amp;quot;Local&amp;quot;: true
},
{
&amp;quot;IPs&amp;quot;: {
&amp;quot;V4&amp;quot;: [
&amp;quot;11.0.2.2&amp;quot;
]
},
&amp;quot;Local&amp;quot;: true
},
{
&amp;quot;IPs&amp;quot;: {
&amp;quot;V4&amp;quot;: [
&amp;quot;11.0.1.2&amp;quot;
]
}
},
{
&amp;quot;IPs&amp;quot;: {
&amp;quot;V6&amp;quot;: [
&amp;quot;1100::102&amp;quot;
]
}
}
]
}
&lt;/code>&lt;/pre>&lt;p>A real backend would use some mechanism to load-balance traffic from
the external IPs to the endpoints.&lt;/p>
&lt;h2 id="writing-a-backend">Writing a backend&lt;/h2>
&lt;p>The &lt;code>kpng-json&lt;/code> backend looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> main
&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#b44">&amp;#34;os&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;encoding/json&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;sigs.k8s.io/kpng/client&amp;#34;&lt;/span>
)
&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">main&lt;/span>() {
client.&lt;span style="color:#00a000">Run&lt;/span>(jsonPrint)
}
&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">jsonPrint&lt;/span>(items []&lt;span style="color:#666">*&lt;/span>client.ServiceEndpoints) {
enc &lt;span style="color:#666">:=&lt;/span> json.&lt;span style="color:#00a000">NewEncoder&lt;/span>(os.Stdout)
&lt;span style="color:#a2f;font-weight:bold">for&lt;/span> _, item &lt;span style="color:#666">:=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">range&lt;/span> items {
_ = enc.&lt;span style="color:#00a000">Encode&lt;/span>(item)
}
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>(yes, that is the entire program)&lt;/p>
&lt;p>A real backend would of course be much more complex, but this
illustrates how &lt;code>kpng&lt;/code> let you focus on load-balancing.&lt;/p>
&lt;p>You can have several backends connected to a &lt;code>kpng&lt;/code> controller, so
during development or debug it can be useful to let something like the
&lt;code>kpng-json&lt;/code> backend run in parallel with your real backend.&lt;/p>
&lt;h2 id="example">Example&lt;/h2>
&lt;p>The complete example can be found &lt;a href="https://github.com/kubernetes-sigs/kpng/tree/master/examples/pipe-exec">here&lt;/a>.&lt;/p>
&lt;p>As an example we implement an &amp;quot;all-ip&amp;quot; backend. It direct all traffic
for the externalIPs to a local endpoint, regardless of ports and upper
layer protocols. There is a
&lt;a href="https://github.com/kubernetes/enhancements/pull/2611">KEP&lt;/a> for this
function and this example is a much simplified version.&lt;/p>
&lt;p>To direct all traffic from an external address to a local POD &lt;a href="https://github.com/kubernetes/enhancements/pull/2611#issuecomment-895061013">only
one iptables rule is
needed&lt;/a>,
for instance;&lt;/p>
&lt;pre>&lt;code>ip6tables -t nat -A PREROUTING -d 1000::55/128 -j DNAT --to-destination 1100::202
&lt;/code>&lt;/pre>&lt;p>As you can see the addresses are in the call to the backend and all it
have to do is:&lt;/p>
&lt;ul>
&lt;li>Extract the addresses with &lt;code>Local: true&lt;/code>&lt;/li>
&lt;li>Setup iptables rules for the &lt;code>ExternalIPs&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>A script doing that may look like:&lt;/p>
&lt;pre>&lt;code>xip=$(cat /tmp/out | jq -r .Service.IPs.ExternalIPs.V6[0])
podip=$(cat /tmp/out | jq -r '.Endpoints[]|select(.Local == true)|select(.IPs.V6 != null)|.IPs.V6[0]')
ip6tables -t nat -A PREROUTING -d $xip/128 -j DNAT --to-destination $podip
&lt;/code>&lt;/pre>&lt;p>Assuming the JSON output above is stored in &lt;code>/tmp/out&lt;/code> (&lt;a href="https://stedolan.github.io/jq/">jq&lt;/a> is an &lt;em>awesome&lt;/em> program!).&lt;/p>
&lt;p>As this is an example we make it really simple for ourselves by using
a minor variation of the &lt;code>kpng-json&lt;/code> backend above. Instead of just
printing, a program is called and the JSON output is passed as &lt;code>stdin&lt;/code>
to that program. The backend can be tested stand-alone:&lt;/p>
&lt;pre>&lt;code>CALLOUT=jq kpng-callout
&lt;/code>&lt;/pre>&lt;p>Where &lt;code>jq&lt;/code> can be replaced with your own program or script. A script
may look like the example above. For more info and the complete
example please see &lt;a href="https://github.com/kubernetes-sigs/kpng/tree/master/examples/pipe-exec">https://github.com/kubernetes-sigs/kpng/tree/master/examples/pipe-exec&lt;/a>.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>While &lt;a href="https://github.com/kubernetes-sigs/kpng">kpng&lt;/a> is in early
stage of development this post wants to show how you may build your
own specialized K8s proxiers in the future. The only thing your
applications need to do is to add the
&lt;code>service.kubernetes.io/service-proxy-name&lt;/code> label in the Service
manifest.&lt;/p>
&lt;p>It is a tedious process to get new features into the &lt;code>kube-proxy&lt;/code> and
it is not unlikely that they will be rejected, so to write a
specialized proxier may be the only option.&lt;/p></description></item><item><title>Blog: Introducing ClusterClass and Managed Topologies in Cluster API</title><link>https://kubernetes.io/blog/2021/10/08/capi-clusterclass-and-managed-topologies/</link><pubDate>Fri, 08 Oct 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/10/08/capi-clusterclass-and-managed-topologies/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Fabrizio Pandini (VMware)&lt;/p>
&lt;p>The &lt;a href="https://cluster-api.sigs.k8s.io/">Cluster API community&lt;/a> is happy to announce the implementation of &lt;em>ClusterClass and Managed Topologies&lt;/em>, a new feature that will greatly simplify how you can provision, upgrade, and operate multiple Kubernetes clusters in a declarative way.&lt;/p>
&lt;h2 id="a-little-bit-of-context">A little bit of context…&lt;/h2>
&lt;p>Before getting into the details, let's take a step back and look at the history of Cluster API.&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/cluster-api/">Cluster API project&lt;/a> started three years ago, and the first releases focused on extensibility and implementing a declarative API that allows a seamless experience across infrastructure providers. This was a success with many cloud providers: AWS, Azure, Digital Ocean, GCP, Metal3, vSphere and still counting.&lt;/p>
&lt;p>With extensibility addressed, the focus shifted to features, like automatic control plane and etcd management, health-based machine remediation, machine rollout strategies and more.&lt;/p>
&lt;p>Fast forwarding to 2021, with lots of companies using Cluster API to manage fleets of Kubernetes clusters running workloads in production, the community focused its effort on stabilization of both code, APIs, documentation, and on extensive test signals which inform Kubernetes releases.&lt;/p>
&lt;p>With solid foundations in place, and a vibrant and welcoming community that still continues to grow, it was time to plan another iteration on our UX for both new and advanced users.&lt;/p>
&lt;p>Enter ClusterClass and Managed Topologies, tada!&lt;/p>
&lt;h2 id="clusterclass">ClusterClass&lt;/h2>
&lt;p>As the name suggests, ClusterClass and managed topologies are built in two parts.&lt;/p>
&lt;p>The idea behind ClusterClass is simple: define the shape of your cluster once, and reuse it many times, abstracting the complexities and the internals of a Kubernetes cluster away.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-10-08-clusterclass-and-managed-topologies/clusterclass.svg" alt="Defining a ClusterClass">&lt;/p>
&lt;p>ClusterClass, at its heart, is a collection of Cluster and Machine templates. You can use it as a “stamp” that can be leveraged to create many clusters of a similar shape.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClusterClass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-amazing-cluster-class&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>controlplane.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KubeadmControlPlaneTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>high-availability-control-plane&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">machineInfrastructure&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerMachineTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>control-plane-machine&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">workers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">machineDeployments&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">class&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type1-workers&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">bootstrap&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bootstrap.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KubeadmConfigTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type1-bootstrap&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">infrastructure&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerMachineTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type1-machine&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">class&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type2-workers&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">bootstrap&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bootstrap.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KubeadmConfigTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type2-bootstrap&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">infrastructure&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerMachineTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type2-machine&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">infrastructure&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ref&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerClusterTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cluster-infrastructure&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The possibilities are endless; you can get a default ClusterClass from the community, “off-the-shelf” classes from your vendor of choice, “certified” classes from the platform admin in your company, or even create custom ones for advanced scenarios.&lt;/p>
&lt;h2 id="managed-topologies">Managed Topologies&lt;/h2>
&lt;p>Managed Topologies let you put the power of ClusterClass into action.&lt;/p>
&lt;p>Given a ClusterClass, you can create many Clusters of a similar shape by providing a single resource, the Cluster.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-10-08-clusterclass-and-managed-topologies/create-cluster.svg" alt="Create a Cluster with ClusterClass">&lt;/p>
&lt;p>Here is an example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cluster.x-k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Cluster&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-amazing-cluster&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bar&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">topology&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># define a managed topology&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">class&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-amazing-cluster-class&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># use the ClusterClass mentioned earlier&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1.21.2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">workers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">machineDeployments&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">class&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type1-workers&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>big-pool-of-machines&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">5&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">class&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>type2-workers&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>small-pool-of-machines&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But there is more than simplified cluster creation. Now the Cluster acts as a single control point for your entire topology.&lt;/p>
&lt;p>All the power of Cluster API, extensibility, lifecycle automation, stability, all the features required for managing an enterprise grade Kubernetes cluster on the infrastructure provider of your choice are now at your fingertips: you can create your Cluster, add new machines, upgrade to the next Kubernetes version, and all from a single place.&lt;/p>
&lt;p>It is just as simple as it looks!&lt;/p>
&lt;h2 id="what-s-next">What’s next&lt;/h2>
&lt;p>While the amazing Cluster API community is working hard to deliver the first version of ClusterClass and managed topologies later this year, we are already looking forward to what comes next for the project and its ecosystem.&lt;/p>
&lt;p>There are a lot of great ideas and opportunities ahead!&lt;/p>
&lt;p>We want to make managed topologies even more powerful and flexible, allowing users to dynamically change bits of a ClusterClass according to the specific needs of a Cluster; this will ensure the same simple and intuitive UX for solving complex problems like e.g. selecting machine image for a specific Kubernetes version and for a specific region of your infrastructure provider, or injecting proxy configurations in the entire Cluster, and so on.&lt;/p>
&lt;p>Stay tuned for what comes next, and if you have any questions, comments or suggestions:&lt;/p>
&lt;ul>
&lt;li>Chat with us on the Kubernetes &lt;a href="http://slack.k8s.io/">Slack&lt;/a>:&lt;a href="https://kubernetes.slack.com/archives/C8TSNPY4T">#cluster-api&lt;/a>&lt;/li>
&lt;li>Join the SIG Cluster Lifecycle &lt;a href="https://groups.google.com/g/kubernetes-sig-cluster-lifecycle">Google Group&lt;/a> to receive calendar invites and gain access to documents&lt;/li>
&lt;li>Join our &lt;a href="https://zoom.us/j/861487554">Zoom meeting&lt;/a>, every Wednesday at 10:00 Pacific Time&lt;/li>
&lt;li>Check out the &lt;a href="https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-classes.html">ClusterClass tutorial&lt;/a> in the Cluster API book.&lt;/li>
&lt;/ul></description></item><item><title>Blog: A Closer Look at NSA/CISA Kubernetes Hardening Guidance</title><link>https://kubernetes.io/blog/2021/10/05/nsa-cisa-kubernetes-hardening-guidance/</link><pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/10/05/nsa-cisa-kubernetes-hardening-guidance/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Jim Angel (Google), Pushkar Joglekar (VMware), and Savitha
Raghunathan (Red Hat)&lt;/p>
&lt;div class="alert alert-primary" role="alert">
&lt;h4 class="alert-heading">Disclaimer&lt;/h4>
The open source tools listed in this article are to serve as examples only
and are in no way a direct recommendation from the Kubernetes community or authors.
&lt;/div>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>USA's National Security Agency (NSA) and the Cybersecurity and Infrastructure
Security Agency (CISA)
released, &amp;quot;&lt;a href="https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF">Kubernetes Hardening Guidance&lt;/a>&amp;quot;
on August 3rd, 2021. The guidance details threats to Kubernetes environments
and provides secure configuration guidance to minimize risk.&lt;/p>
&lt;p>The following sections of this blog correlate to the sections in the NSA/CISA guidance.
Any missing sections are skipped because of limited opportunities to add
anything new to the existing content.&lt;/p>
&lt;p>&lt;em>Note&lt;/em>: This blog post is not a substitute for reading the guide. Reading the published
guidance is recommended before proceeding as the following content is
complementary.&lt;/p>
&lt;h2 id="introduction-and-threat-model">Introduction and Threat Model&lt;/h2>
&lt;p>Note that the threats identified as important by the NSA/CISA, or the intended audience of this guidance, may be different from the threats that other enterprise users of Kubernetes consider important. This section
is still useful for organizations that care about data, resource theft and
service unavailability.&lt;/p>
&lt;p>The guidance highlights the following three sources of compromises:&lt;/p>
&lt;ul>
&lt;li>Supply chain risks&lt;/li>
&lt;li>Malicious threat actors&lt;/li>
&lt;li>Insider threats (administrators, users, or cloud service providers)&lt;/li>
&lt;/ul>
&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Threat_model">threat model&lt;/a> tries to take a step back and review threats that not only
exist within the boundary of a Kubernetes cluster but also include the underlying
infrastructure and surrounding workloads that Kubernetes does not manage.&lt;/p>
&lt;p>For example, when a workload outside the cluster shares the same physical
network, it has access to the kubelet and to control plane components: etcd, controller manager, scheduler and API
server. Therefore, the guidance recommends having network level isolation
separating Kubernetes clusters from other workloads that do not need connectivity
to Kubernetes control plane nodes. Specifically, scheduler, controller-manager,
etcd only need to be accessible to the API server. Any interactions with Kubernetes
from outside the cluster can happen by providing access to API server port.&lt;/p>
&lt;p>List of ports and protocols for each of these components are
defined in &lt;a href="https://kubernetes.io/docs/reference/ports-and-protocols/">Ports and Protocols&lt;/a>
within the Kubernetes documentation.&lt;/p>
&lt;blockquote>
&lt;p>Special note: kube-scheduler and kube-controller-manager uses different ports than the ones mentioned in the guidance&lt;/p>
&lt;/blockquote>
&lt;p>The &lt;a href="https://cnsmap.netlify.app/threat-modelling">Threat modelling&lt;/a> section
from the CNCF &lt;a href="https://github.com/cncf/tag-security/tree/main/security-whitepaper">Cloud Native Security Whitepaper + Map&lt;/a>
provides another perspective on approaching threat modelling Kubernetes, from a
cloud native lens.&lt;/p>
&lt;h2 id="kubernetes-pod-security">Kubernetes Pod security&lt;/h2>
&lt;p>Kubernetes by default does not guarantee strict workload isolation between pods
running in the same node in a cluster. However, the guidance provides several
techniques to enhance existing isolation and reduce the attack surface in case of a
compromise.&lt;/p>
&lt;h3 id="non-root-containers-and-rootless-container-engines">&amp;quot;Non-root&amp;quot; containers and &amp;quot;rootless&amp;quot; container engines&lt;/h3>
&lt;p>Several best practices related to basic security principle of least privilege
i.e. provide only the permissions are needed; no more, no less, are worth a
second look.&lt;/p>
&lt;p>The guide recommends setting non-root user at build time instead of relying on
setting &lt;code>runAsUser&lt;/code> at runtime in your Pod spec. This is a good practice and provides
some level of defense in depth. For example, if the container image is built with user &lt;code>10001&lt;/code>
and the Pod spec misses adding the &lt;code>runAsuser&lt;/code> field in its &lt;code>Deployment&lt;/code> object. In this
case there are certain edge cases that are worth exploring for awareness:&lt;/p>
&lt;ol>
&lt;li>Pods can fail to start, if the user defined at build time is different from
the one defined in pod spec and some files are as a result inaccessible.&lt;/li>
&lt;li>Pods can end up sharing User IDs unintentionally. This can be problematic
even if the User IDs are non-zero in a situation where a container escape to
host file system is possible. Once the attacker has access to the host file
system, they get access to all the file resources that are owned by other
unrelated pods that share the same UID.&lt;/li>
&lt;li>Pods can end up sharing User IDs, with other node level processes not managed
by Kubernetes e.g. node level daemons for auditing, vulnerability scanning,
telemetry. The threat is similar to the one above where host file system
access can give attacker full access to these node level daemons without
needing to be root on the node.&lt;/li>
&lt;/ol>
&lt;p>However, none of these cases will have as severe an impact as a container
running as root being able to escape as a root user on the host, which can provide
an attacker with complete control of the worker node, further allowing lateral
movement to other worker or control plane nodes.&lt;/p>
&lt;p>Kubernetes 1.22 introduced
an &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-in-userns/">alpha feature&lt;/a>
that specifically reduces the impact of such a control plane component running
as root user to a non-root user through user namespaces.&lt;/p>
&lt;p>That (&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-stages">alpha stage&lt;/a>) support for user namespaces / rootless mode is available with
the following container runtimes:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.docker.com/engine/security/rootless/">Docker Engine&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2020/09/25/rootless-containers-with-podman-the-basics">Podman&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Some distributions support running in rootless mode, like the following:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kind.sigs.k8s.io/docs/user/rootless/">kind&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://rancher.com/docs/k3s/latest/en/advanced/#running-k3s-with-rootless-mode-experimental">k3s&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/rootless-containers/usernetes">Usernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="immutable-container-filesystems">Immutable container filesystems&lt;/h3>
&lt;p>The NSA/CISA Kubernetes Hardening Guidance highlights an often overlooked feature &lt;code>readOnlyRootFileSystem&lt;/code>, with a
working example in &lt;a href="https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF#page=42">Appendix B&lt;/a>. This example limits execution and tampering of
containers at runtime. Any read/write activity can then be limited to few
directories by using &lt;code>tmpfs&lt;/code> volume mounts.&lt;/p>
&lt;p>However, some applications that modify the container filesystem at runtime, like exploding a WAR or JAR file at container startup,
could face issues when enabling this feature. To avoid this issue, consider making minimal changes to the filesystem at runtime
when possible.&lt;/p>
&lt;h3 id="building-secure-container-images">Building secure container images&lt;/h3>
&lt;p>Kubernetes Hardening Guidance also recommends running a scanner at deploy time as an admission controller,
to prevent vulnerable or misconfigured pods from running in the cluster.
Theoretically, this sounds like a good approach but there are several caveats to
consider before this can be implemented in practice:&lt;/p>
&lt;ul>
&lt;li>Depending on network bandwidth, available resources and scanner of choice,
scanning for vulnerabilities for an image can take an indeterminate amount of
time. This could lead to slower or unpredictable pod start up times, which
could result in spikes of unavailability when apps are serving peak load.&lt;/li>
&lt;li>If the policy that allows or denies pod startup is made using incorrect or
incomplete data it could result in several false positive or false negative
outcomes like the following:
&lt;ul>
&lt;li>inside a container image, the &lt;code>openssl&lt;/code> package is detected as vulnerable. However,
the application is written in Golang and uses the Go &lt;code>crypto&lt;/code> package for TLS. Therefore, this vulnerability
is not in the code execution path and as such has minimal impact if it
remains unfixed.&lt;/li>
&lt;li>A vulnerability is detected in the &lt;code>openssl&lt;/code> package for a Debian base image.
However, the upstream Debian community considers this as a Minor impact
vulnerability and as a result does not release a patch fix for this
vulnerability. The owner of this image is now stuck with a vulnerability that
cannot be fixed and a cluster that does not allow the image to run because
of predefined policy that does not take into account whether the fix for a
vulnerability is available or not&lt;/li>
&lt;li>A Golang app is built on top of a &lt;a href="https://github.com/GoogleContainerTools/distroless">distroless&lt;/a>
image, but it is compiled with a Golang version that uses a vulnerable &lt;a href="https://pkg.go.dev/std">standard library&lt;/a>.
The scanner has
no visibility into golang version but only on OS level packages. So it
allows the pod to run in the cluster in spite of the image containing an
app binary built on vulnerable golang.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>To be clear, relying on vulnerability scanners is absolutely a good idea but
policy definitions should be flexible enough to allow:&lt;/p>
&lt;ul>
&lt;li>Creation of exception lists for images or vulnerabilities through labelling&lt;/li>
&lt;li>Overriding the severity with a risk score based on impact of a vulnerability&lt;/li>
&lt;li>Applying the same policies at build time to catch vulnerable images with
fixable vulnerabilities before they can be deployed into Kubernetes clusters&lt;/li>
&lt;/ul>
&lt;p>Special considerations like offline vulnerability database fetch, may also be
needed, if the clusters run in an air-gapped environment and the scanners
require internet access to update the vulnerability database.&lt;/p>
&lt;h3 id="pod-security-policies">Pod Security Policies&lt;/h3>
&lt;p>Since Kubernetes v1.21, the &lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">PodSecurityPolicy&lt;/a>
API and related features are &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">deprecated&lt;/a>,
but some of the guidance in this section will still apply for the next few years, until cluster operators
upgrade their clusters to newer Kubernetes versions.&lt;/p>
&lt;p>The Kubernetes project is working on a replacement for PodSecurityPolicy.
Kubernetes v1.22 includes an alpha feature called &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission&lt;/a>
that is intended to allow enforcing a minimum level of isolation between pods.&lt;/p>
&lt;p>The built-in isolation levels for Pod Security Admission are derived
from &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a>, which is a superset of all the components mentioned in Table I &lt;a href="https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF#page=17">page 10&lt;/a> of
the guidance.&lt;/p>
&lt;p>Information about migrating from PodSecurityPolicy to the Pod Security
Admission feature is available
in
&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller&lt;/a>.&lt;/p>
&lt;p>One important behavior mentioned in the guidance that remains the same between
Pod Security Policy and its replacement is that enforcing either of them does
not affect pods that are already running. With both PodSecurityPolicy and Pod Security Admission,
the enforcement happens during the pod creation
stage.&lt;/p>
&lt;h3 id="hardening-container-engines">Hardening container engines&lt;/h3>
&lt;p>Some container workloads are less trusted than others but may need to run in the
same cluster. In those cases, running them on dedicated nodes that include
hardened container runtimes that provide stricter pod isolation boundaries can
act as a useful security control.&lt;/p>
&lt;p>Kubernetes supports
an API called &lt;a href="https://kubernetes.io/docs/concepts/containers/runtime-class/">RuntimeClass&lt;/a> that is
stable / GA (and, therefore, enabled by default) stage as of Kubernetes v1.20.
RuntimeClass allows you to ensure that Pods requiring strong isolation are scheduled onto
nodes that can offer it.&lt;/p>
&lt;p>Some third-party projects that you can use in conjunction with RuntimeClass are:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kata-containers/kata-containers/blob/main/docs/how-to/how-to-use-k8s-with-cri-containerd-and-kata.md#create-runtime-class-for-kata-containers">kata containers&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gvisor.dev/docs/user_guide/containerd/quick_start/">gvisor&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>As discussed here and in the guidance, many features and tooling exist in and around
Kubernetes that can enhance the isolation boundaries between
pods. Based on relevant threats and risk posture, you should pick and choose
between them, instead of trying to apply all the recommendations. Having said that, cluster
level isolation i.e. running workloads in dedicated clusters, remains the strictest workload
isolation mechanism, in spite of improvements mentioned earlier here and in the guide.&lt;/p>
&lt;h2 id="network-separation-and-hardening">Network Separation and Hardening&lt;/h2>
&lt;p>Kubernetes Networking can be tricky and this section focuses on how to secure
and harden the relevant configurations. The guide identifies the following as key
takeaways:&lt;/p>
&lt;ul>
&lt;li>Using NetworkPolicies to create isolation between resources,&lt;/li>
&lt;li>Securing the control plane&lt;/li>
&lt;li>Encrypting traffic and sensitive data&lt;/li>
&lt;/ul>
&lt;h3 id="network-policies">Network Policies&lt;/h3>
&lt;p>Network policies can be created with the help of network plugins. In order to
make the creation and visualization easier for users, Cilium supports
a &lt;a href="https://editor.cilium.io">web GUI tool&lt;/a>. That web GUI lets you create Kubernetes
NetworkPolicies (a generic API that nevertheless requires a compatible CNI plugin),
and / or Cilium network policies (CiliumClusterwideNetworkPolicy and CiliumNetworkPolicy,
which only work in clusters that use the Cilium CNI plugin).
You can use these APIs to restrict network traffic between pods, and therefore minimize the
attack vector.&lt;/p>
&lt;p>Another scenario that is worth exploring is the usage of external IPs. Some
services, when misconfigured, can create random external IPs. An attacker can take
advantage of this misconfiguration and easily intercept traffic. This vulnerability
has been reported
in &lt;a href="https://www.cvedetails.com/cve/CVE-2020-8554/">CVE-2020-8554&lt;/a>.
Using &lt;a href="https://github.com/kubernetes-sigs/externalip-webhook">externalip-webhook&lt;/a>
can mitigate this vulnerability by preventing the services from using random
external IPs. &lt;a href="https://github.com/kubernetes-sigs/externalip-webhook">externalip-webhook&lt;/a>
only allows creation of services that don't require external IPs or whose
external IPs are within the range specified by the administrator.&lt;/p>
&lt;blockquote>
&lt;p>CVE-2020-8554 - Kubernetes API server in all versions allow an attacker
who is able to create a ClusterIP service and set the &lt;code>spec.externalIPs&lt;/code> field,
to intercept traffic to that IP address. Additionally, an attacker who is able to
patch the &lt;code>status&lt;/code> (which is considered a privileged operation and should not
typically be granted to users) of a LoadBalancer service can set the
&lt;code>status.loadBalancer.ingress.ip&lt;/code> to similar effect.&lt;/p>
&lt;/blockquote>
&lt;h3 id="resource-policies">Resource Policies&lt;/h3>
&lt;p>In addition to configuring ResourceQuotas and limits, consider restricting how many process
IDs (PIDs) a given Pod can use, and also to reserve some PIDs for node-level use to avoid
resource exhaustion. More details to apply these limits can be
found in &lt;a href="https://kubernetes.io/docs/concepts/policy/pid-limiting/">Process ID Limits And Reservations&lt;/a>.&lt;/p>
&lt;h3 id="control-plane-hardening">Control Plane Hardening&lt;/h3>
&lt;p>In the next section, the guide covers control plane hardening. It is worth
noting that
from &lt;a href="https://github.com/kubernetes/kubernetes/issues/91506">Kubernetes 1.20&lt;/a>,
insecure port from API server, has been removed.&lt;/p>
&lt;h3 id="etcd">Etcd&lt;/h3>
&lt;p>As a general rule, the etcd server should be configured to only trust
certificates assigned to the API server. It limits the attack surface and prevents a
malicious attacker from gaining access to the cluster. It might be beneficial to
use a separate CA for etcd, as it by default trusts all the certificates issued
by the root CA.&lt;/p>
&lt;h3 id="kubeconfig-files">Kubeconfig Files&lt;/h3>
&lt;p>In addition to specifying the token and certificates directly, &lt;code>.kubeconfig&lt;/code>
supports dynamic retrieval of temporary tokens using auth provider plugins.
Beware of the possibility of malicious
shell &lt;a href="https://banzaicloud.com/blog/kubeconfig-security/">code execution&lt;/a> in a
&lt;code>kubeconfig&lt;/code> file. Once attackers gain access to the cluster, they can steal ssh
keys/secrets or more.&lt;/p>
&lt;h3 id="secrets">Secrets&lt;/h3>
&lt;p>Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets&lt;/a> is the native way of managing secrets as a Kubernetes
API object. However, in some scenarios such as a desire to have a single source of truth for all app secrets, irrespective of whether they run on Kubernetes or not, secrets can be managed loosely coupled with
Kubernetes and consumed by pods through side-cars or init-containers with minimal usage of Kubernetes Secrets API.&lt;/p>
&lt;p>&lt;a href="https://github.com/external-secrets/kubernetes-external-secrets">External secrets providers&lt;/a>
and &lt;a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">csi-secrets-store&lt;/a>
are some of these alternatives to Kubernetes Secrets&lt;/p>
&lt;h2 id="log-auditing">Log Auditing&lt;/h2>
&lt;p>The NSA/CISA guidance stresses monitoring and alerting based on logs. The key points
include logging at the host level, application level, and on the cloud. When
running Kubernetes in production, it's important to understand who's
responsible, and who's accountable, for each layer of logging.&lt;/p>
&lt;h3 id="kubernetes-api-auditing">Kubernetes API auditing&lt;/h3>
&lt;p>One area that deserves more focus is what exactly should alert or be logged. The
document outlines a sample policy in &lt;a href="https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF#page=55">Appendix L: Audit Policy&lt;/a> that logs all
RequestResponse's including metadata and request / response bodies. While helpful for a demo, it may not be practical for production.&lt;/p>
&lt;p>Each organization needs to evaluate their
own threat model and build an audit policy that complements or helps troubleshooting incident response. Think
about how someone would attack your organization and what audit trail could identify it. Review more advanced options for tuning audit logs in the official &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy">audit logging documentation&lt;/a>.
It's crucial to tune your audit logs to only include events that meet your threat model. A minimal audit policy that logs everything at &lt;code>metadata&lt;/code> level can also be a good starting point.&lt;/p>
&lt;p>Audit logging configurations can also be tested with
kind following these &lt;a href="https://kind.sigs.k8s.io/docs/user/auditing">instructions&lt;/a>.&lt;/p>
&lt;h3 id="streaming-logs-and-auditing">Streaming logs and auditing&lt;/h3>
&lt;p>Logging is important for threat and anomaly detection. As the document outlines,
it's a best practice to scan and alert on logs as close to real time as possible
and to protect logs from tampering if a compromise occurs. It's important to
reflect on the various levels of logging and identify the critical areas such as
API endpoints.&lt;/p>
&lt;p>Kubernetes API audit logging can stream to a webhook and there's an example in &lt;a href="https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF#page=58">Appendix N: Webhook configuration&lt;/a>. Using a webhook could be a method that
stores logs off cluster and/or centralizes all audit logs. Once logs are
centrally managed, look to enable alerting based on critical events. Also ensure
you understand what the baseline is for normal activities.&lt;/p>
&lt;h3 id="alert-identification">Alert identification&lt;/h3>
&lt;p>While the guide stressed the importance of notifications, there is not a blanket
event list to alert from. The alerting requirements vary based on your own
requirements and threat model. Examples include the following events:&lt;/p>
&lt;ul>
&lt;li>Changes to the &lt;code>securityContext&lt;/code> of a Pod&lt;/li>
&lt;li>Updates to admission controller configs&lt;/li>
&lt;li>Accessing certain files / URLs&lt;/li>
&lt;/ul>
&lt;h3 id="additional-logging-resources">Additional logging resources&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=OPuu8wsu2Zc">Seccomp Security Profiles and You: A Practical Guide - Duffie Cooley&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=ZJgaGJm9NJE">TGI Kubernetes 119: Gatekeeper and OPA&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.lacework.com/blog/hiding-in-plaintext-sight-abusing-the-lack-of-kubernetes-auditing-policies/">Abusing The Lack of Kubernetes Auditing Policies&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/blog/2021/08/25/seccomp-default/">Enable seccomp for all workloads with a new v1.22 alpha feature&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.twitch.tv/videos/1147889860">This Week in Cloud Native: Auditing / Pod Security&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="upgrading-and-application-security-practices">Upgrading and Application Security practices&lt;/h2>
&lt;p>Kubernetes releases three times per year, so upgrade-related toil is a common problem for
people running production clusters. In addition to this, operators must
regularly upgrade the underlying node's operating system and running
applications. This is a best practice to ensure continued support and to reduce
the likelihood of bugs or vulnerabilities.&lt;/p>
&lt;p>Kubernetes supports the three most recent stable releases. While each Kubernetes
release goes through a large number of tests before being published, some
teams aren't comfortable running the latest stable release until some time has
passed. No matter what version you're running, ensure that patch upgrades
happen frequently or automatically. More information can be found in
the &lt;a href="https://kubernetes.io/releases/version-skew-policy/">version skew&lt;/a> policy
pages.&lt;/p>
&lt;p>When thinking about how you'll manage node OS upgrades, consider ephemeral
nodes. Having the ability to destroy and add nodes allows your team to respond
quicker to node issues. In addition, having deployments that tolerate node
instability (and a culture that encourages frequent deployments) allows for
easier cluster upgrades.&lt;/p>
&lt;p>Additionally, it's worth reiterating from the guidance that periodic
vulnerability scans and penetration tests can be performed on the various system
components to proactively look for insecure configurations and vulnerabilities.&lt;/p>
&lt;h3 id="finding-release-security-information">Finding release &amp;amp; security information&lt;/h3>
&lt;p>To find the most recent Kubernetes supported versions, refer to
&lt;a href="https://k8s.io/releases">https://k8s.io/releases&lt;/a>, which includes minor versions. It's good to stay up to date with
your minor version patches.&lt;/p>
&lt;p>If you're running a managed Kubernetes offering, look for their release
documentation and find their various security channels.&lt;/p>
&lt;p>Subscribe to
the &lt;a href="https://groups.google.com/g/kubernetes-announce">Kubernetes Announce mailing list&lt;/a>.
The Kubernetes Announce mailing list is searchable for terms such
as &amp;quot;&lt;a href="https://groups.google.com/g/kubernetes-announce/search?q=%5BSecurity%20Advisory%5D">Security Advisories&lt;/a>&amp;quot;.
You can set up alerts and email notifications as long as you know what key
words to alert on.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In summary, it is fantastic to see security practitioners sharing this
level of detailed guidance in public. This guidance further highlights
Kubernetes going mainstream and how securing Kubernetes clusters and the
application containers running on Kubernetes continues to need attention and focus of
practitioners. Only a few weeks after the guidance was published, an open source
tool &lt;a href="https://github.com/armosec/kubescape">kubescape&lt;/a> to validate cluster
against this guidance became available.&lt;/p>
&lt;p>This tool can be a great starting point to check the current state of your
clusters, after which you can use the information in this blog post and in the guidance to assess
where improvements can be made.&lt;/p>
&lt;p>Finally, it is worth reiterating that not all controls in this guidance will
make sense for all practitioners. The best way to know which controls matter is
to rely on the threat model of your own Kubernetes environment.&lt;/p>
&lt;p>&lt;em>A special shout out and thanks to Rory McCune (@raesene) for his inputs to this blog post&lt;/em>&lt;/p></description></item><item><title>Blog: How to Handle Data Duplication in Data-Heavy Kubernetes Environments</title><link>https://kubernetes.io/blog/2021/09/29/how-to-handle-data-duplication-in-data-heavy-kubernetes-environments/</link><pubDate>Wed, 29 Sep 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/09/29/how-to-handle-data-duplication-in-data-heavy-kubernetes-environments/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong>
Augustinas Stirbis (CAST AI)&lt;/p>
&lt;h2 id="why-duplicate-data">Why Duplicate Data?&lt;/h2>
&lt;p>It’s convenient to create a copy of your application with a copy of its state for each team.
For example, you might want a separate database copy to test some significant schema changes
or develop other disruptive operations like bulk insert/delete/update...&lt;/p>
&lt;p>&lt;strong>Duplicating data takes a lot of time.&lt;/strong> That’s because you need first to download
all the data from a source block storage provider to compute and then send
it back to a storage provider again. There’s a lot of network traffic and CPU/RAM used in this process.
Hardware acceleration by offloading certain expensive operations to dedicated hardware is
&lt;strong>always a huge performance boost&lt;/strong>. It reduces the time required to complete an operation by orders
of magnitude.&lt;/p>
&lt;h2 id="volume-snapshots-to-the-rescue">Volume Snapshots to the rescue&lt;/h2>
&lt;p>Kubernetes introduced &lt;a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots/">VolumeSnapshots&lt;/a> as alpha in 1.12,
beta in 1.17, and the Generally Available version in 1.20.
VolumeSnapshots use specialized APIs from storage providers to duplicate volume of data.&lt;/p>
&lt;p>Since data is already in the same storage device (array of devices), duplicating data is usually
a metadata operation for storage providers with local snapshots (majority of on-premise storage providers).
All you need to do is point a new disk to an immutable snapshot and only
save deltas (or let it do a full-disk copy). As an operation that is inside the storage back-end,
it’s much quicker and usually doesn’t involve sending traffic over the network.
Public Clouds storage providers under the hood work a bit differently. They save snapshots
to Object Storage and then copy back from Object storage to Block storage when &amp;quot;duplicating&amp;quot; disk.
Technically there is a lot of Compute and network resources spent on Cloud providers side,
but from Kubernetes user perspective VolumeSnapshots work the same way whether is it local or
remote snapshot storage provider and no Compute and Network resources are involved in this operation.&lt;/p>
&lt;h2 id="sounds-like-we-have-our-solution-right">Sounds like we have our solution, right?&lt;/h2>
&lt;p>Actually, VolumeSnapshots are namespaced, and Kubernetes protects namespaced data from
being shared between tenants (Namespaces). This Kubernetes limitation is a conscious design
decision so that a Pod running in a different namespace can’t mount another application’s
&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim&lt;/a> (PVC).&lt;/p>
&lt;p>One way around it would be to create multiple volumes with duplicate data in one namespace.
However, you could easily reference the wrong copy.&lt;/p>
&lt;p>So the idea is to separate teams/initiatives by namespaces to avoid that and generally
limit access to the production namespace.&lt;/p>
&lt;h2 id="solution-creating-a-golden-snapshot-externally">Solution? Creating a Golden Snapshot externally&lt;/h2>
&lt;p>Another way around this design limitation is to create Snapshot externally (not through Kubernetes).
This is also called pre-provisioning a snapshot manually. Next, I will import it
as a multi-tenant golden snapshot that can be used for many namespaces. Below illustration will be
for AWS EBS (Elastic Block Storage) and GCE PD (Persistent Disk) services.&lt;/p>
&lt;h3 id="high-level-plan-for-preparing-the-golden-snapshot">High-level plan for preparing the Golden Snapshot&lt;/h3>
&lt;ol>
&lt;li>Identify Disk (EBS/Persistent Disk) that you want to clone with data in the cloud provider&lt;/li>
&lt;li>Make a Disk Snapshot (in cloud provider console)&lt;/li>
&lt;li>Get Disk Snapshot ID&lt;/li>
&lt;/ol>
&lt;h3 id="high-level-plan-for-cloning-data-for-each-team">High-level plan for cloning data for each team&lt;/h3>
&lt;ol>
&lt;li>Create Namespace “sandbox01”&lt;/li>
&lt;li>Import Disk Snapshot (ID) as VolumeSnapshotContent to Kubernetes&lt;/li>
&lt;li>Create VolumeSnapshot in the Namespace &amp;quot;sandbox01&amp;quot; mapped to VolumeSnapshotContent&lt;/li>
&lt;li>Create the PersistentVolumeClaim from VolumeSnapshot&lt;/li>
&lt;li>Install Deployment or StatefulSet with PVC&lt;/li>
&lt;/ol>
&lt;h2 id="step-1-identify-disk">Step 1: Identify Disk&lt;/h2>
&lt;p>First, you need to identify your golden source. In my case, it’s a PostgreSQL database
on PersistentVolumeClaim “postgres-pv-claim” in the “production” namespace.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">kubectl -n &amp;lt;namespace&amp;gt; get pvc &amp;lt;pvc-name&amp;gt; -o jsonpath='{.spec.volumeName}'
&lt;/code>&lt;/pre>&lt;p>The output will look similar to:&lt;/p>
&lt;pre>&lt;code>pvc-3096b3ba-38b6-4fd1-a42f-ec99176ed0d90
&lt;/code>&lt;/pre>&lt;h2 id="step-2-prepare-your-golden-source">Step 2: Prepare your golden source&lt;/h2>
&lt;p>You need to do this once or every time you want to refresh your golden data.&lt;/p>
&lt;h3 id="make-a-disk-snapshot">Make a Disk Snapshot&lt;/h3>
&lt;p>Go to AWS EC2 or GCP Compute Engine console and search for an EBS volume
(on AWS) or Persistent Disk (on GCP), that has a label matching the last output.
In this case I saw: &lt;code>pvc-3096b3ba-38b6-4fd1-a42f-ec99176ed0d9&lt;/code>.&lt;/p>
&lt;p>Click on Create snapshot and give it a name. You can do it in Console manually,
in AWS CloudShell / Google Cloud Shell, or in the terminal. To create a snapshot in the
terminal you must have the AWS CLI tool (&lt;code>aws&lt;/code>) or Google's CLI (&lt;code>gcloud&lt;/code>)
installed and configured.&lt;/p>
&lt;p>Here’s the command to create snapshot on GCP:&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">gcloud compute disks snapshot &amp;lt;cloud-disk-id&amp;gt; --project=&amp;lt;gcp-project-id&amp;gt; --snapshot-names=&amp;lt;set-new-snapshot-name&amp;gt; --zone=&amp;lt;availability-zone&amp;gt; --storage-location=&amp;lt;region&amp;gt;
&lt;/code>&lt;/pre>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-09-07-data-duplication-in-data-heavy-k8s-env/create-volume-snapshot-gcp.png"
alt="Screenshot of a terminal showing volume snapshot creation on GCP"/> &lt;figcaption>
&lt;h4>GCP snapshot creation&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>GCP identifies the disk by its PVC name, so it’s direct mapping. In AWS, you need to
find volume by the CSIVolumeName AWS tag with PVC name value first that will be used for snapshot creation.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-09-07-data-duplication-in-data-heavy-k8s-env/identify-volume-aws.png"
alt="Screenshot of AWS web console, showing EBS volume identification"/> &lt;figcaption>
&lt;h4>Identify disk ID on AWS&lt;/h4>
&lt;/figcaption>
&lt;/figure>
&lt;p>Mark done Volume (volume-id) &lt;code>vol-00c7ecd873c6fb3ec&lt;/code> and ether create EBS snapshot in AWS Console, or use &lt;code>aws cli&lt;/code>.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">aws ec2 create-snapshot --volume-id '&amp;lt;volume-id&amp;gt;' --description '&amp;lt;set-new-snapshot-name&amp;gt;' --tag-specifications 'ResourceType=snapshot'
&lt;/code>&lt;/pre>&lt;h2 id="step-3-get-your-disk-snapshot-id">Step 3: Get your Disk Snapshot ID&lt;/h2>
&lt;p>In AWS, the command above will output something similar to:&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">&amp;quot;SnapshotId&amp;quot;: &amp;quot;snap-09ed24a70bc19bbe4&amp;quot;
&lt;/code>&lt;/pre>&lt;p>If you’re using the GCP cloud, you can get the snapshot ID from the gcloud command by querying for the snapshot’s given name:&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">gcloud compute snapshots --project=&amp;lt;gcp-project-id&amp;gt; describe &amp;lt;new-snapshot-name&amp;gt; | grep id:
&lt;/code>&lt;/pre>&lt;p>You should get similar output to:&lt;/p>
&lt;pre>&lt;code>id: 6645363163809389170
&lt;/code>&lt;/pre>&lt;h2 id="step-4-create-a-development-environment-for-each-team">Step 4: Create a development environment for each team&lt;/h2>
&lt;p>Now I have my Golden Snapshot, which is immutable data. Each team will get a copy
of this data, and team members can modify it as they see fit, given that a new EBS/persistent
disk will be created for each team.&lt;/p>
&lt;p>Below I will define a manifest for each namespace. To save time, you can replace
the namespace name (such as changing “sandbox01” → “sandbox42”) using tools
such as &lt;code>sed&lt;/code> or &lt;code>yq&lt;/code>, with Kubernetes-aware templating tools like
&lt;a href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/">Kustomize&lt;/a>,
or using variable substitution in a CI/CD pipeline.&lt;/p>
&lt;p>Here's an example manifest:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotContent&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>postgresql-orders-db-sandbox01&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>sandbox01&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deletionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Retain&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pd.csi.storage.gke.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">snapshotHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;gcp/projects/staging-eu-castai-vt5hy2/global/snapshots/6645363163809389170&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>postgresql-orders-db-snap&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>sandbox01&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>postgresql-orders-db-snap&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>sandbox01&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotContentName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>postgresql-orders-db-sandbox01&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In Kubernetes, VolumeSnapshotContent (VSC) objects are not namespaced.
However, I need a separate VSC for each different namespace to use, so the
&lt;code>metadata.name&lt;/code> of each VSC must also be different. To make that straightfoward,
I used the target namespace as part of the name.&lt;/p>
&lt;p>Now it’s time to replace the driver field with the CSI (Container Storage Interface) driver
installed in your K8s cluster. Major cloud providers have CSI driver for block storage that
support VolumeSnapshots but quite often CSI drivers are not installed by default, consult
with your Kubernetes provider.&lt;/p>
&lt;p>That manifest above defines a VSC that works on GCP.
On AWS, driver and SnashotHandle values might look like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ebs.csi.aws.com&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">snapshotHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;snap-07ff83d328c981c98&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>At this point, I need to use the &lt;em>Retain&lt;/em> policy, so that the CSI driver doesn’t try to
delete my manually created EBS disk snapshot.&lt;/p>
&lt;p>For GCP, you will have to build this string by hand - add a full project ID and snapshot ID.
For AWS, it’s just a plain snapshot ID.&lt;/p>
&lt;p>VSC also requires specifying which VolumeSnapshot (VS) will use it, so VSC and VS are
referencing each other.&lt;/p>
&lt;p>Now I can create PersistentVolumeClaim from VS above. It’s important to set this first:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>postgres-pv-claim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>sandbox01&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">dataSource&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>postgresql-orders-db-snap&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>21Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If default StorageClass has &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode">WaitForFirstConsumer&lt;/a> policy,
then the actual Cloud Disk will be created from the Golden Snapshot only when some Pod bounds that PVC.&lt;/p>
&lt;p>Now I assign that PVC to my Pod (in my case, it’s Postgresql) as I would with any other PVC.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">kubectl -n &amp;lt;namespace&amp;gt; get volumesnapshotContent,volumesnapshot,pvc,pod
&lt;/code>&lt;/pre>&lt;p>Both VS and VSC should be &lt;em>READYTOUSE&lt;/em> true, PVC bound, and the Pod (from Deployment or StatefulSet) running.&lt;/p>
&lt;p>&lt;strong>To keep on using data from my Golden Snapshot, I just need to repeat this for the
next namespace and voilà! No need to waste time and compute resources on the duplication process.&lt;/strong>&lt;/p></description></item><item><title>Blog: Spotlight on SIG Node</title><link>https://kubernetes.io/blog/2021/09/27/sig-node-spotlight-2021/</link><pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/09/27/sig-node-spotlight-2021/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Dewan Ahmed, Red Hat&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In Kubernetes, a &lt;em>Node&lt;/em> is a representation of a single machine in your cluster. &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node&lt;/a> owns that very important Node component and supports various subprojects such as Kubelet, Container Runtime Interface (CRI) and more to support how the pods and host resources interact. In this blog, we have summarized our conversation with &lt;a href="https://twitter.com/ehashdn">Elana Hashman (EH)&lt;/a> &amp;amp; &lt;a href="https://twitter.com/SergeyKanzhelev">Sergey Kanzhelev (SK)&lt;/a>, who walk us through the various aspects of being a part of the SIG and share some insights about how others can get involved.&lt;/p>
&lt;h2 id="a-summary-of-our-conversation">A summary of our conversation&lt;/h2>
&lt;h3 id="could-you-tell-us-a-little-about-what-sig-node-does">Could you tell us a little about what SIG Node does?&lt;/h3>
&lt;p>SK: SIG Node is a vertical SIG responsible for the components that support the controlled interactions between the pods and host resources. We manage the lifecycle of pods that are scheduled to a node. This SIG's focus is to enable a broad set of workload types, including workloads with hardware specific or performance sensitive requirements. All while maintaining isolation boundaries between pods on a node, as well as the pod and the host. This SIG maintains quite a few components and has many external dependencies (like container runtimes or operating system features), which makes the complexity we deal with huge. We tame the complexity and aim to continuously improve node reliability.&lt;/p>
&lt;h3 id="sig-node-is-a-vertical-sig-could-you-explain-a-bit-more">&amp;quot;SIG Node is a vertical SIG&amp;quot; could you explain a bit more?&lt;/h3>
&lt;p>EH: There are two kinds of SIGs: horizontal and vertical. Horizontal SIGs are concerned with a particular function of every component in Kubernetes: for example, SIG Security considers security aspects of every component in Kubernetes, or SIG Instrumentation looks at the logs, metrics, traces and events of every component in Kubernetes. Such SIGs don't tend to own a lot of code.&lt;/p>
&lt;p>Vertical SIGs, on the other hand, own a single component, and are responsible for approving and merging patches to that code base. SIG Node owns the &amp;quot;Node&amp;quot; vertical, pertaining to the kubelet and its lifecycle. This includes the code for the kubelet itself, as well as the node controller, the container runtime interface, and related subprojects like the node problem detector.&lt;/p>
&lt;h3 id="how-did-the-ci-subproject-start-is-this-specific-to-sig-node-and-how-does-it-help-the-sig">How did the CI subproject start? Is this specific to SIG Node and how does it help the SIG?&lt;/h3>
&lt;p>SK: The subproject started as a follow up after one of the releases was blocked by numerous test failures of critical tests. These tests haven’t started falling all at once, rather continuous lack of attention led to slow degradation of tests quality. SIG Node was always prioritizing quality and reliability, and forming of the subproject was a way to highlight this priority.&lt;/p>
&lt;h3 id="as-the-3rd-largest-sig-in-terms-of-number-of-issues-and-prs-how-does-your-sig-juggle-so-much-work">As the 3rd largest SIG in terms of number of issues and PRs, how does your SIG juggle so much work?&lt;/h3>
&lt;p>EH: It helps to be organized. When I increased my contributions to the SIG in January of 2021, I found myself overwhelmed by the volume of pull requests and issues and wasn't sure where to start. We were already tracking test-related issues and pull requests on the CI subproject board, but that was missing a lot of our bugfixes and feature work. So I began putting together a triage board for the rest of our pull requests, which allowed me to sort each one by status and what actions to take, and documented its use for other contributors. We closed or merged over 500 issues and pull requests tracked by our two boards in each of the past two releases. The Kubernetes devstats showed that we have significantly increased our velocity as a result.&lt;/p>
&lt;p>In June, we ran our first bug scrub event to work through the backlog of issues filed against SIG Node, ensuring they were properly categorized. We closed over 130 issues over the course of this 48 hour global event, but as of writing we still have 333 open issues.&lt;/p>
&lt;h3 id="why-should-new-and-existing-contributors-consider-joining-sig-node">Why should new and existing contributors consider joining SIG Node?&lt;/h3>
&lt;p>SK: Being a SIG Node contributor gives you skills and recognition that are rewarding and useful. Understanding under the hood of a kubelet helps architecting better apps, tune and optimize those apps, and gives leg up in issues troubleshooting. If you are a new contributor, SIG Node gives you the foundational knowledge that is key to understanding why other Kubernetes components are designed the way they are. Existing contributors may benefit as many features will require SIG Node changes one way or another. So being a SIG Node contributor helps building features in other SIGs faster.&lt;/p>
&lt;p>SIG Node maintains numerous components, many of which have dependency on external projects or OS features. This makes the onboarding process quite lengthy and demanding. But if you are up for a challenge, there is always a place for you, and a group of people to support.&lt;/p>
&lt;h3 id="what-do-you-do-to-help-new-contributors-get-started">What do you do to help new contributors get started?&lt;/h3>
&lt;p>EH: Getting started in SIG Node can be intimidating, since there is so much work to be done, our SIG meetings are very large, and it can be hard to find a place to start.&lt;/p>
&lt;p>I always encourage new contributors to work on things that they have some investment in already. In SIG Node, that might mean volunteering to help fix a bug that you have personally been affected by, or helping to triage bugs you care about by priority.&lt;/p>
&lt;p>To come up to speed on any open source code base, there are two strategies you can take: start by exploring a particular issue deeply, and follow that to expand the edges of your knowledge as needed, or briefly review as many issues and change requests as you possibly can to get a higher level picture of how the component works. Ultimately, you will need to do both if you want to become a Node reviewer or approver.&lt;/p>
&lt;p>&lt;a href="https://twitter.com/dims">Davanum Srinivas&lt;/a> and I each ran a cohort of group mentoring to help teach new contributors the skills to become Node reviewers, and if there's interest we can work to find a mentor to run another session. I also encourage new contributors to attend our Node CI Subproject meeting: it's a smaller audience and we don't record the triage sessions, so it can be a less intimidating way to get started with the SIG.&lt;/p>
&lt;h3 id="are-there-any-particular-skills-you-d-like-to-recruit-for-what-skills-are-contributors-to-sig-usability-likely-to-learn">Are there any particular skills you’d like to recruit for? What skills are contributors to SIG Usability likely to learn?&lt;/h3>
&lt;p>SK: SIG Node works on many workstreams in very different areas. All of these areas are on system level. For the typical code contributions you need to have a passion for building and utilizing low level APIs and writing performant and reliable components. Being a contributor you will learn how to debug and troubleshoot, profile, and monitor these components, as well as user workload that is run by these components. Often, with the limited to no access to Nodes, as they are running production workloads.&lt;/p>
&lt;p>The other way of contribution is to help document SIG node features. This type of contribution requires a deep understanding of features, and ability to explain them in simple terms.&lt;/p>
&lt;p>Finally, we are always looking for feedback on how best to run your workload. Come and explain specifics of it, and what features in SIG Node components may help to run it better.&lt;/p>
&lt;h3 id="what-are-you-getting-positive-feedback-on-and-what-s-coming-up-next-for-sig-node">What are you getting positive feedback on, and what’s coming up next for SIG Node?&lt;/h3>
&lt;p>EH: Over the past year SIG Node has adopted some new processes to help manage our feature development and Kubernetes enhancement proposals, and other SIGs have looked to us for inspiration in managing large workloads. I hope that this is an area we can continue to provide leadership in and further iterate on.&lt;/p>
&lt;p>We have a great balance of new features and deprecations in flight right now. Deprecations of unused or difficult to maintain features help us keep technical debt and maintenance load under control, and examples include the dockershim and DynamicKubeletConfiguration deprecations. New features will unlock additional functionality in end users' clusters, and include exciting features like support for cgroups v2, swap memory, graceful node shutdowns, and device management policies.&lt;/p>
&lt;h3 id="any-closing-thoughts-resources-you-d-like-to-share">Any closing thoughts/resources you’d like to share?&lt;/h3>
&lt;p>SK/EH: It takes time and effort to get to any open source community. SIG Node may overwhelm you at first with the number of participants, volume of work, and project scope. But it is totally worth it. Join our welcoming community! &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node GitHub Repo&lt;/a> contains many useful resources including Slack, mailing list and other contact info.&lt;/p>
&lt;h2 id="wrap-up">Wrap Up&lt;/h2>
&lt;p>SIG Node hosted a &lt;a href="https://www.youtube.com/watch?v=z5aY4e2RENA">KubeCon + CloudNativeCon Europe 2021 talk&lt;/a> with an intro and deep dive to their awesome SIG. Join the SIG's meetings to find out about the most recent research results, what the plans are for the forthcoming year, and how to get involved in the upstream Node team as a contributor!&lt;/p></description></item><item><title>Blog: Introducing Single Pod Access Mode for PersistentVolumes</title><link>https://kubernetes.io/blog/2021/09/13/read-write-once-pod-access-mode-alpha/</link><pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/09/13/read-write-once-pod-access-mode-alpha/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Chris Henzie (Google)&lt;/p>
&lt;p>Last month's release of Kubernetes v1.22 introduced a new ReadWriteOncePod access mode for &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistent-volumes">PersistentVolumes&lt;/a> and &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims&lt;/a>.
With this alpha feature, Kubernetes allows you to restrict volume access to a single pod in the cluster.&lt;/p>
&lt;h2 id="what-are-access-modes-and-why-are-they-important">What are access modes and why are they important?&lt;/h2>
&lt;p>When using storage, there are different ways to model how that storage is consumed.&lt;/p>
&lt;p>For example, a storage system like a network file share can have many users all reading and writing data simultaneously.
In other cases maybe everyone is allowed to read data but not write it.
For highly sensitive data, maybe only one user is allowed to read and write data but nobody else.&lt;/p>
&lt;p>In the world of Kubernetes, &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">access modes&lt;/a> are the way you can define how durable storage is consumed.
These access modes are a part of the spec for PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>shared-cache&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteMany&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Allow many nodes to access shared-cache simultaneously.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Before v1.22, Kubernetes offered three access modes for PVs and PVCs:&lt;/p>
&lt;ul>
&lt;li>ReadWriteOnce – the volume can be mounted as read-write by a single node&lt;/li>
&lt;li>ReadOnlyMany – the volume can be mounted read-only by many nodes&lt;/li>
&lt;li>ReadWriteMany – the volume can be mounted as read-write by many nodes&lt;/li>
&lt;/ul>
&lt;p>These access modes are enforced by Kubernetes components like the &lt;code>kube-controller-manager&lt;/code> and &lt;code>kubelet&lt;/code> to ensure only certain pods are allowed to access a given PersistentVolume.&lt;/p>
&lt;h2 id="what-is-this-new-access-mode-and-how-does-it-work">What is this new access mode and how does it work?&lt;/h2>
&lt;p>Kubernetes v1.22 introduced a fourth access mode for PVs and PVCs, that you can use for CSI volumes:&lt;/p>
&lt;ul>
&lt;li>ReadWriteOncePod – the volume can be mounted as read-write by a single pod&lt;/li>
&lt;/ul>
&lt;p>If you create a pod with a PVC that uses the ReadWriteOncePod access mode, Kubernetes ensures that pod is the only pod across your whole cluster that can read that PVC or write to it.&lt;/p>
&lt;p>If you create another pod that references the same PVC with this access mode, the pod will fail to start because the PVC is already in use by another pod.
For example:&lt;/p>
&lt;pre>&lt;code>Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Warning FailedScheduling 1s default-scheduler 0/1 nodes are available: 1 node has pod using PersistentVolumeClaim with the same name and ReadWriteOncePod access mode.
&lt;/code>&lt;/pre>&lt;h3 id="how-is-this-different-than-the-readwriteonce-access-mode">How is this different than the ReadWriteOnce access mode?&lt;/h3>
&lt;p>The ReadWriteOnce access mode restricts volume access to a single &lt;em>node&lt;/em>, which means it is possible for multiple pods on the same node to read from and write to the same volume.
This could potentially be a major problem for some applications, especially if they require at most one writer for data safety guarantees.&lt;/p>
&lt;p>With ReadWriteOncePod these issues go away.
Set the access mode on your PVC, and Kubernetes guarantees that only a single pod has access.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>The ReadWriteOncePod access mode is in alpha for Kubernetes v1.22 and is only supported for CSI volumes.
As a first step you need to enable the ReadWriteOncePod &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates">feature gate&lt;/a> for &lt;code>kube-apiserver&lt;/code>, &lt;code>kube-scheduler&lt;/code>, and &lt;code>kubelet&lt;/code>.
You can enable the feature by setting command line arguments:&lt;/p>
&lt;pre>&lt;code>--feature-gates=&amp;quot;...,ReadWriteOncePod=true&amp;quot;
&lt;/code>&lt;/pre>&lt;p>You also need to update the following CSI sidecars to these versions or greater:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0">csi-provisioner:v3.0.0+&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0">csi-attacher:v3.3.0+&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0">csi-resizer:v1.3.0+&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="creating-a-persistentvolumeclaim">Creating a PersistentVolumeClaim&lt;/h3>
&lt;p>In order to use the ReadWriteOncePod access mode for your PVs and PVCs, you will need to create a new PVC with the access mode:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>single-writer-only&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOncePod&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Allow only a single pod to access single-writer-only.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If your storage plugin supports &lt;a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/">dynamic provisioning&lt;/a>, new PersistentVolumes will be created with the ReadWriteOncePod access mode applied.&lt;/p>
&lt;h4 id="migrating-existing-persistentvolumes">Migrating existing PersistentVolumes&lt;/h4>
&lt;p>If you have existing PersistentVolumes, they can be migrated to use ReadWriteOncePod.&lt;/p>
&lt;p>In this example, we already have a &amp;quot;cat-pictures-pvc&amp;quot; PersistentVolumeClaim that is bound to a &amp;quot;cat-pictures-pv&amp;quot; PersistentVolume, and a &amp;quot;cat-pictures-writer&amp;quot; Deployment that uses this PersistentVolumeClaim.&lt;/p>
&lt;p>As a first step, you need to edit your PersistentVolume's &lt;code>spec.persistentVolumeReclaimPolicy&lt;/code> and set it to &lt;code>Retain&lt;/code>.
This ensures your PersistentVolume will not be deleted when we delete the corresponding PersistentVolumeClaim:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl patch pv cat-pictures-pv -p &lt;span style="color:#b44">&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;persistentVolumeReclaimPolicy&amp;#34;:&amp;#34;Retain&amp;#34;}}&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next you need to stop any workloads that are using the PersistentVolumeClaim bound to the PersistentVolume you want to migrate, and then delete the PersistentVolumeClaim.&lt;/p>
&lt;p>Once that is done, you need to clear your PersistentVolume's &lt;code>spec.claimRef.uid&lt;/code> to ensure PersistentVolumeClaims can bind to it upon recreation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl scale --replicas&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">0&lt;/span> deployment cat-pictures-writer
kubectl delete pvc cat-pictures-pvc
kubectl patch pv cat-pictures-pv -p &lt;span style="color:#b44">&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;claimRef&amp;#34;:{&amp;#34;uid&amp;#34;:&amp;#34;&amp;#34;}}}&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>After that you need to replace the PersistentVolume's access modes with ReadWriteOncePod:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl patch pv cat-pictures-pv -p &lt;span style="color:#b44">&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;accessModes&amp;#34;:[&amp;#34;ReadWriteOncePod&amp;#34;]}}&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="alert alert-info note callout" role="alert">
&lt;strong>Note:&lt;/strong> The ReadWriteOncePod access mode cannot be combined with other access modes.
Make sure ReadWriteOncePod is the only access mode on the PersistentVolume when updating, otherwise the request will fail.
&lt;/div>
&lt;p>Next you need to modify your PersistentVolumeClaim to set ReadWriteOncePod as the only access mode.
You should also set your PersistentVolumeClaim's &lt;code>spec.volumeName&lt;/code> to the name of your PersistentVolume.&lt;/p>
&lt;p>Once this is done, you can recreate your PersistentVolumeClaim and start up your workloads:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># IMPORTANT: Make sure to edit your PVC in cat-pictures-pvc.yaml before applying. You need to:&lt;/span>
&lt;span style="color:#080;font-style:italic"># - Set ReadWriteOncePod as the only access mode&lt;/span>
&lt;span style="color:#080;font-style:italic"># - Set spec.volumeName to &amp;#34;cat-pictures-pv&amp;#34;&lt;/span>
kubectl apply -f cat-pictures-pvc.yaml
kubectl apply -f cat-pictures-writer-deployment.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Lastly you may edit your PersistentVolume's &lt;code>spec.persistentVolumeReclaimPolicy&lt;/code> and set to it back to &lt;code>Delete&lt;/code> if you previously changed it.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl patch pv cat-pictures-pv -p &lt;span style="color:#b44">&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;persistentVolumeReclaimPolicy&amp;#34;:&amp;#34;Delete&amp;#34;}}&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can read &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">Configure a Pod to Use a PersistentVolume for Storage&lt;/a> for more details on working with PersistentVolumes and PersistentVolumeClaims.&lt;/p>
&lt;h2 id="what-volume-plugins-support-this">What volume plugins support this?&lt;/h2>
&lt;p>The only volume plugins that support this are CSI drivers.
SIG Storage does not plan to support this for in-tree plugins because they are being deprecated as part of &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#what-is-the-timeline-status">CSI migration&lt;/a>.
Support may be considered for beta for users that prefer to use the legacy in-tree volume APIs with CSI migration enabled.&lt;/p>
&lt;h2 id="as-a-storage-vendor-how-do-i-add-support-for-this-access-mode-to-my-csi-driver">As a storage vendor, how do I add support for this access mode to my CSI driver?&lt;/h2>
&lt;p>The ReadWriteOncePod access mode will work out of the box without any required updates to CSI drivers, but &lt;a href="#update-your-csi-sidecars">does require updates to CSI sidecars&lt;/a>.
With that being said, if you would like to stay up to date with the latest changes to the CSI specification (v1.5.0+), read on.&lt;/p>
&lt;p>Two new access modes were introduced to the CSI specification in order to disambiguate the legacy &lt;a href="https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L418-L420">&lt;code>SINGLE_NODE_WRITER&lt;/code>&lt;/a> access mode.
They are &lt;a href="https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L437-L447">&lt;code>SINGLE_NODE_SINGLE_WRITER&lt;/code> and &lt;code>SINGLE_NODE_MULTI_WRITER&lt;/code>&lt;/a>.
In order to communicate to sidecars (like the &lt;a href="https://github.com/kubernetes-csi/external-provisioner">external-provisioner&lt;/a>) that your driver understands and accepts these two new CSI access modes, your driver will also need to advertise the &lt;code>SINGLE_NODE_MULTI_WRITER&lt;/code> capability for the &lt;a href="https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L1073-L1081">controller service&lt;/a> and &lt;a href="https://github.com/container-storage-interface/spec/blob/v1.5.0/csi.proto#L1515-L1524">node service&lt;/a>.&lt;/p>
&lt;p>If you'd like to read up on the motivation for these access modes and capability bits, you can also read the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/2485-read-write-once-pod-pv-access-mode/README.md#csi-specification-changes-volume-capabilities">CSI Specification Changes, Volume Capabilities&lt;/a> section of KEP-2485 (ReadWriteOncePod PersistentVolume Access Mode).&lt;/p>
&lt;h3 id="update-your-csi-driver-to-use-the-new-interface">Update your CSI driver to use the new interface&lt;/h3>
&lt;p>As a first step you will need to update your driver's &lt;code>container-storage-interface&lt;/code> dependency to v1.5.0+, which contains support for these new access modes and capabilities.&lt;/p>
&lt;h3 id="accept-new-csi-access-modes">Accept new CSI access modes&lt;/h3>
&lt;p>If your CSI driver contains logic for validating CSI access modes for requests , it may need updating.
If it currently accepts &lt;code>SINGLE_NODE_WRITER&lt;/code>, it should be updated to also accept &lt;code>SINGLE_NODE_SINGLE_WRITER&lt;/code> and &lt;code>SINGLE_NODE_MULTI_WRITER&lt;/code>.&lt;/p>
&lt;p>Using the &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/blob/v1.2.2/pkg/gce-pd-csi-driver/utils.go#L116-L130">GCP PD CSI driver validation logic&lt;/a> as an example, here is how it can be extended:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-diff" data-lang="diff">&lt;span style="color:#000080;font-weight:bold">diff --git a/pkg/gce-pd-csi-driver/utils.go b/pkg/gce-pd-csi-driver/utils.go
&lt;/span>&lt;span style="color:#000080;font-weight:bold">index 281242c..b6c5229 100644
&lt;/span>&lt;span style="color:#000080;font-weight:bold">&lt;/span>&lt;span style="color:#a00000">--- a/pkg/gce-pd-csi-driver/utils.go
&lt;/span>&lt;span style="color:#a00000">&lt;/span>&lt;span style="color:#00a000">+++ b/pkg/gce-pd-csi-driver/utils.go
&lt;/span>&lt;span style="color:#00a000">&lt;/span>&lt;span style="color:#800080;font-weight:bold">@@ -123,6 +123,8 @@ func validateAccessMode(am *csi.VolumeCapability_AccessMode) error {
&lt;/span>&lt;span style="color:#800080;font-weight:bold">&lt;/span> case csi.VolumeCapability_AccessMode_SINGLE_NODE_READER_ONLY:
case csi.VolumeCapability_AccessMode_MULTI_NODE_READER_ONLY:
case csi.VolumeCapability_AccessMode_MULTI_NODE_MULTI_WRITER:
&lt;span style="color:#00a000">+ case csi.VolumeCapability_AccessMode_SINGLE_NODE_SINGLE_WRITER:
&lt;/span>&lt;span style="color:#00a000">+ case csi.VolumeCapability_AccessMode_SINGLE_NODE_MULTI_WRITER:
&lt;/span>&lt;span style="color:#00a000">&lt;/span> default:
return fmt.Errorf(&amp;#34;%v access mode is not supported for for PD&amp;#34;, am.GetMode())
}
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="advertise-new-csi-controller-and-node-service-capabilities">Advertise new CSI controller and node service capabilities&lt;/h3>
&lt;p>Your CSI driver will also need to return the new &lt;code>SINGLE_NODE_MULTI_WRITER&lt;/code> capability as part of the &lt;code>ControllerGetCapabilities&lt;/code> and &lt;code>NodeGetCapabilities&lt;/code> RPCs.&lt;/p>
&lt;p>Using the &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/blob/v1.2.2/pkg/gce-pd-csi-driver/gce-pd-driver.go#L54-L77">GCP PD CSI driver capability advertisement logic&lt;/a> as an example, here is how it can be extended:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-diff" data-lang="diff">&lt;span style="color:#000080;font-weight:bold">diff --git a/pkg/gce-pd-csi-driver/gce-pd-driver.go b/pkg/gce-pd-csi-driver/gce-pd-driver.go
&lt;/span>&lt;span style="color:#000080;font-weight:bold">index 45903f3..0d7ea26 100644
&lt;/span>&lt;span style="color:#000080;font-weight:bold">&lt;/span>&lt;span style="color:#a00000">--- a/pkg/gce-pd-csi-driver/gce-pd-driver.go
&lt;/span>&lt;span style="color:#a00000">&lt;/span>&lt;span style="color:#00a000">+++ b/pkg/gce-pd-csi-driver/gce-pd-driver.go
&lt;/span>&lt;span style="color:#00a000">&lt;/span>&lt;span style="color:#800080;font-weight:bold">@@ -56,6 +56,8 @@ func (gceDriver *GCEDriver) SetupGCEDriver(name, vendorVersion string, extraVolu
&lt;/span>&lt;span style="color:#800080;font-weight:bold">&lt;/span> csi.VolumeCapability_AccessMode_SINGLE_NODE_WRITER,
csi.VolumeCapability_AccessMode_MULTI_NODE_READER_ONLY,
csi.VolumeCapability_AccessMode_MULTI_NODE_MULTI_WRITER,
&lt;span style="color:#00a000">+ csi.VolumeCapability_AccessMode_SINGLE_NODE_SINGLE_WRITER,
&lt;/span>&lt;span style="color:#00a000">+ csi.VolumeCapability_AccessMode_SINGLE_NODE_MULTI_WRITER,
&lt;/span>&lt;span style="color:#00a000">&lt;/span> }
gceDriver.AddVolumeCapabilityAccessModes(vcam)
csc := []csi.ControllerServiceCapability_RPC_Type{
&lt;span style="color:#800080;font-weight:bold">@@ -67,12 +69,14 @@ func (gceDriver *GCEDriver) SetupGCEDriver(name, vendorVersion string, extraVolu
&lt;/span>&lt;span style="color:#800080;font-weight:bold">&lt;/span> csi.ControllerServiceCapability_RPC_EXPAND_VOLUME,
csi.ControllerServiceCapability_RPC_LIST_VOLUMES,
csi.ControllerServiceCapability_RPC_LIST_VOLUMES_PUBLISHED_NODES,
&lt;span style="color:#00a000">+ csi.ControllerServiceCapability_RPC_SINGLE_NODE_MULTI_WRITER,
&lt;/span>&lt;span style="color:#00a000">&lt;/span> }
gceDriver.AddControllerServiceCapabilities(csc)
ns := []csi.NodeServiceCapability_RPC_Type{
csi.NodeServiceCapability_RPC_STAGE_UNSTAGE_VOLUME,
csi.NodeServiceCapability_RPC_EXPAND_VOLUME,
csi.NodeServiceCapability_RPC_GET_VOLUME_STATS,
&lt;span style="color:#00a000">+ csi.NodeServiceCapability_RPC_SINGLE_NODE_MULTI_WRITER,
&lt;/span>&lt;span style="color:#00a000">&lt;/span> }
gceDriver.AddNodeServiceCapabilities(ns)
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="implement-nodepublishvolume-behavior">Implement &lt;code>NodePublishVolume&lt;/code> behavior&lt;/h3>
&lt;p>The CSI spec outlines expected behavior for the &lt;code>NodePublishVolume&lt;/code> RPC when called more than once for the same volume but with different arguments (like the target path).
Please refer to &lt;a href="https://github.com/container-storage-interface/spec/blob/v1.5.0/spec.md#nodepublishvolume">the second table in the NodePublishVolume section of the CSI spec&lt;/a> for more details on expected behavior when implementing in your driver.&lt;/p>
&lt;h3 id="update-your-csi-sidecars">Update your CSI sidecars&lt;/h3>
&lt;p>When deploying your CSI drivers, you must update the following CSI sidecars to versions that depend on CSI spec v1.5.0+ and the Kubernetes v1.22 API.
The minimum required versions are:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-provisioner/releases/tag/v3.0.0">csi-provisioner:v3.0.0+&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-attacher/releases/tag/v3.3.0">csi-attacher:v3.3.0+&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-resizer/releases/tag/v1.3.0">csi-resizer:v1.3.0+&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>As part of the beta graduation for this feature, SIG Storage plans to update the Kubenetes scheduler to support pod preemption in relation to ReadWriteOncePod storage.
This means if two pods request a PersistentVolumeClaim with ReadWriteOncePod, the pod with highest priority will gain access to the PersistentVolumeClaim and any pod with lower priority will be preempted from the node and be unable to access the PersistentVolumeClaim.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>Please see &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/2485-read-write-once-pod-pv-access-mode/README.md">KEP-2485&lt;/a> for more details on the ReadWriteOncePod access mode and motivations for CSI spec changes.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>The &lt;a href="https://kubernetes.slack.com/messages/csi">Kubernetes #csi Slack channel&lt;/a> and any of the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">standard SIG Storage communication channels&lt;/a> are great mediums to reach out to the SIG Storage and the CSI teams.&lt;/p>
&lt;p>Special thanks to the following people for their insightful reviews and design considerations:&lt;/p>
&lt;ul>
&lt;li>Abdullah Gharaibeh (ahg-g)&lt;/li>
&lt;li>Aldo Culquicondor (alculquicondor)&lt;/li>
&lt;li>Ben Swartzlander (bswartz)&lt;/li>
&lt;li>Deep Debroy (ddebroy)&lt;/li>
&lt;li>Hemant Kumar (gnufied)&lt;/li>
&lt;li>Humble Devassy Chirammal (humblec)&lt;/li>
&lt;li>James DeFelice (jdef)&lt;/li>
&lt;li>Jan Šafránek (jsafrane)&lt;/li>
&lt;li>Jing Xu (jingxu97)&lt;/li>
&lt;li>Jordan Liggitt (liggitt)&lt;/li>
&lt;li>Michelle Au (msau42)&lt;/li>
&lt;li>Saad Ali (saad-ali)&lt;/li>
&lt;li>Tim Hockin (thockin)&lt;/li>
&lt;li>Xing Yang (xing-yang)&lt;/li>
&lt;/ul>
&lt;p>If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group&lt;/a> (SIG).
We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Alpha in Kubernetes v1.22: API Server Tracing</title><link>https://kubernetes.io/blog/2021/09/03/api-server-tracing/</link><pubDate>Fri, 03 Sep 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/09/03/api-server-tracing/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> David Ashpole (Google)&lt;/p>
&lt;p>In distributed systems, it can be hard to figure out where problems are. You grep through one component's logs just to discover that the source of your problem is in another component. You search there only to discover that you need to enable debug logs to figure out what really went wrong... And it goes on. The more complex the path your request takes, the harder it is to answer questions about where it went. I've personally spent many hours doing this dance with a variety of Kubernetes components. Distributed tracing is a tool which is designed to help in these situations, and the Kubernetes API Server is, perhaps, the most important Kubernetes component to be able to debug. At Kubernetes' Sig Instrumentation, our mission is to make it easier to understand what's going on in your cluster, and we are happy to announce that distributed tracing in the Kubernetes API Server reached alpha in 1.22.&lt;/p>
&lt;h2 id="what-is-tracing">What is Tracing?&lt;/h2>
&lt;p>Distributed tracing links together a bunch of super-detailed information from multiple different sources, and structures that telemetry into a single tree for that request. Unlike logging, which limits the quantity of data ingested by using log levels, tracing collects all of the details and uses sampling to collect only a small percentage of requests. This means that once you have a trace which demonstrates an issue, you should have all the information you need to root-cause the problem--no grepping for object UID required! My favorite aspect, though, is how useful the visualizations of traces are. Even if you don't understand the inner workings of the API Server, or don't have a clue what an etcd &amp;quot;Transaction&amp;quot; is, I'd wager you (yes, you!) could tell me roughly what the order of events was, and which components were involved in the request. If some step takes a long time, it is easy to tell where the problem is.&lt;/p>
&lt;h2 id="why-opentelemetry">Why OpenTelemetry?&lt;/h2>
&lt;p>It's important that Kubernetes works well for everyone, regardless of who manages your infrastructure, or which vendors you choose to integrate with. That is particularly true for Kubernetes' integrations with telemetry solutions. OpenTelemetry, being a CNCF project, shares these core values, and is creating exactly what we need in Kubernetes: A set of open standards for Tracing client library APIs and a standard trace format. By using OpenTelemetry, we can ensure users have the freedom to choose their backend, and ensure vendors have a level playing field. The timing couldn't be better: the OpenTelemetry golang API and SDK are very close to their 1.0 release, and will soon offer backwards-compatibility for these open standards.&lt;/p>
&lt;h2 id="why-instrument-the-api-server">Why instrument the API Server?&lt;/h2>
&lt;p>The Kubernetes API Server is a great candidate for tracing for a few reasons:&lt;/p>
&lt;ul>
&lt;li>It follows the standard &amp;quot;RPC&amp;quot; model (serve a request by making requests to downstream components), which makes it easy to instrument.&lt;/li>
&lt;li>Users are latency-sensitive: If a request takes more than 10 seconds to complete, many clients will time-out.&lt;/li>
&lt;li>It has a complex service topology: A single request could require consulting a dozen webhooks, or involve multiple requests to etcd.&lt;/li>
&lt;/ul>
&lt;h2 id="trying-out-apiserver-tracing-with-a-webhook">Trying out APIServer Tracing with a webhook&lt;/h2>
&lt;h3 id="enabling-api-server-tracing">Enabling API Server Tracing&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Enable the APIServerTracing &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature-gate&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Set our configuration for tracing by pointing the &lt;code>--tracing-config-file&lt;/code> flag on the kube-apiserver at our config file, which contains:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apiserver.config.k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>TracingConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># 1% sampling rate&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">samplingRatePerMillion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="enabling-etcd-tracing">Enabling Etcd Tracing&lt;/h3>
&lt;p>Add &lt;code>--experimental-enable-distributed-tracing&lt;/code>, &lt;code>--experimental-distributed-tracing-address=0.0.0.0:4317&lt;/code>, &lt;code>--experimental-distributed-tracing-service-name=etcd&lt;/code> flags to etcd to enable tracing. Note that this traces every request, so it will probably generate a lot of traces if you enable it.&lt;/p>
&lt;h3 id="example-trace-list-nodes">Example Trace: List Nodes&lt;/h3>
&lt;p>I could've used any trace backend, but decided to use Jaeger, since it is one of the most popular open-source tracing projects. I deployed &lt;a href="https://hub.docker.com/r/jaegertracing/all-in-one">the Jaeger All-in-one container&lt;/a> in my cluster, deployed &lt;a href="https://github.com/open-telemetry/opentelemetry-collector">the OpenTelemetry collector&lt;/a> on my control-plane node (&lt;a href="https://github.com/dashpole/dashpole_demos/tree/master/otel/controlplane">example&lt;/a>), and captured traces like this one:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-09-03-api-server-tracing/example-trace-1.png" alt="Jaeger screenshot showing API server and etcd trace" title="Jaeger screenshot showing API server and etcd trace">&lt;/p>
&lt;p>The teal lines are from the API Server, and includes it serving a request to &lt;code>/api/v1/nodes&lt;/code>, and issuing a grpc &lt;code>Range&lt;/code> RPC to ETCD. The yellow-ish line is from ETCD handling the &lt;code>Range&lt;/code> RPC.&lt;/p>
&lt;h3 id="example-trace-create-pod-with-mutating-webhook">Example Trace: Create Pod with Mutating Webhook&lt;/h3>
&lt;p>I instrumented the &lt;a href="https://github.com/kubernetes-sigs/controller-runtime/tree/master/examples/builtins">example webhook&lt;/a> with OpenTelemetry (I had to &lt;a href="https://github.com/dashpole/controller-runtime/commit/85fdda7ba03dd2c22ef62c1a3dbdf5aa651f90da">patch&lt;/a> controller-runtime, but it makes a neat demo), and routed traces to Jaeger as well. I collected traces like this one:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-09-03-api-server-tracing/example-trace-2.png" alt="Jaeger screenshot showing API server, admission webhook, and etcd trace" title="Jaeger screenshot showing API server, admission webhook, and etcd trace">&lt;/p>
&lt;p>Compared with the previous trace, there are two new spans: A teal span from the API Server making a request to the admission webhook, and a brown span from the admission webhook serving the request. Even if you didn't instrument your webhook, you would still get the span from the API Server making the request to the webhook.&lt;/p>
&lt;h2 id="get-involved">Get involved!&lt;/h2>
&lt;p>As this is our first attempt at adding distributed tracing to a Kubernetes component, there is probably a lot we can improve! If my struggles resonated with you, or if you just want to try out the latest Kubernetes has to offer, please give the feature a try and open issues with any problem you encountered and ways you think the feature could be improved.&lt;/p>
&lt;p>This is just the very beginning of what we can do with distributed tracing in Kubernetes. If there are other components you think would benefit from distributed tracing, or want to help bring API Server Tracing to GA, join sig-instrumentation at our &lt;a href="https://github.com/kubernetes/community/tree/master/sig-instrumentation#instrumentation-special-interest-group">regular meetings&lt;/a> and get involved!&lt;/p></description></item><item><title>Blog: Kubernetes 1.22: A New Design for Volume Populators</title><link>https://kubernetes.io/blog/2021/08/30/volume-populators-redesigned/</link><pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/30/volume-populators-redesigned/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong>
Ben Swartzlander (NetApp)&lt;/p>
&lt;p>Kubernetes v1.22, released earlier this month, introduced a redesigned approach for volume
populators. Originally implemented
in v1.18, the API suffered from backwards compatibility issues. Kubernetes v1.22 includes a new API
field called &lt;code>dataSourceRef&lt;/code> that fixes these problems.&lt;/p>
&lt;h2 id="data-sources">Data sources&lt;/h2>
&lt;p>Earlier Kubernetes releases already added a &lt;code>dataSource&lt;/code> field into the
&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaim&lt;/a> API,
used for cloning volumes and creating volumes from snapshots. You could use the &lt;code>dataSource&lt;/code> field when
creating a new PVC, referencing either an existing PVC or a VolumeSnapshot in the same namespace.
That also modified the normal provisioning process so that instead of yielding an empty volume, the
new PVC contained the same data as either the cloned PVC or the cloned VolumeSnapshot.&lt;/p>
&lt;p>Volume populators embrace the same design idea, but extend it to any type of object, as long
as there exists a &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resource&lt;/a>
to define the data source, and a populator controller to implement the logic. Initially,
the &lt;code>dataSource&lt;/code> field was directly extended to allow arbitrary objects, if the &lt;code>AnyVolumeDataSource&lt;/code>
feature gate was enabled on a cluster. That change unfortunately caused backwards compatibility
problems, and so the new &lt;code>dataSourceRef&lt;/code> field was born.&lt;/p>
&lt;p>In v1.22 if the &lt;code>AnyVolumeDataSource&lt;/code> feature gate is enabled, the &lt;code>dataSourceRef&lt;/code> field is
added, which behaves similarly to the &lt;code>dataSource&lt;/code> field except that it allows arbitrary
objects to be specified. The API server ensures that the two fields always have the same
contents, and neither of them are mutable. The differences is that at creation time
&lt;code>dataSource&lt;/code> allows only PVCs or VolumeSnapshots, and ignores all other values, while
&lt;code>dataSourceRef&lt;/code> allows most types of objects, and in the few cases it doesn't allow an
object (core objects other than PVCs) a validation error occurs.&lt;/p>
&lt;p>When this API change graduates to stable, we would deprecate using &lt;code>dataSource&lt;/code> and recommend
using &lt;code>dataSourceRef&lt;/code> field for all use cases.
In the v1.22 release, &lt;code>dataSourceRef&lt;/code> is available (as an alpha feature) specifically for cases
where you want to use for custom volume populators.&lt;/p>
&lt;h2 id="using-populators">Using populators&lt;/h2>
&lt;p>Every volume populator must have one or more CRDs that it supports. Administrators may
install the CRD and the populator controller and then PVCs with a &lt;code>dataSourceRef&lt;/code> specifies
a CR of the type that the populator supports will be handled by the populator controller
instead of the CSI driver directly.&lt;/p>
&lt;p>Underneath the covers, the CSI driver is still invoked to create an empty volume, which
the populator controller fills with the appropriate data. The PVC doesn't bind to the PV
until it's fully populated, so it's safe to define a whole application manifest including
pod and PVC specs and the pods won't begin running until everything is ready, just as if
the PVC was a clone of another PVC or VolumeSnapshot.&lt;/p>
&lt;h2 id="how-it-works">How it works&lt;/h2>
&lt;p>PVCs with data sources are still noticed by the external-provisioner sidecar for the
related storage class (assuming a CSI provisioner is used), but because the sidecar
doesn't understand the data source kind, it doesn't do anything. The populator controller
is also watching for PVCs with data sources of a kind that it understands and when it
sees one, it creates a temporary PVC of the same size, volume mode, storage class,
and even on the same topology (if topology is used) as the original PVC. The populator
controller creates a worker pod that attaches to the volume and writes the necessary
data to it, then detaches from the volume and the populator controller rebinds the PV
from the temporary PVC to the orignal PVC.&lt;/p>
&lt;h2 id="trying-it-out">Trying it out&lt;/h2>
&lt;p>The following things are required to use volume populators:&lt;/p>
&lt;ul>
&lt;li>Enable the &lt;code>AnyVolumeDataSource&lt;/code> feature gate&lt;/li>
&lt;li>Install a CRD for the specific data source / populator&lt;/li>
&lt;li>Install the populator controller itself&lt;/li>
&lt;/ul>
&lt;p>Populator controllers may use the &lt;a href="https://github.com/kubernetes-csi/lib-volume-populator">lib-volume-populator&lt;/a>
library to do most of the Kubernetes API level work. Individual populators only need to
provide logic for actually writing data into the volume based on a particular CR
instance. This library provides a sample populator implementation.&lt;/p>
&lt;p>These optional components improve user experience:&lt;/p>
&lt;ul>
&lt;li>Install the VolumePopulator CRD&lt;/li>
&lt;li>Create a VolumePopulator custom respource for each specific data source&lt;/li>
&lt;li>Install the &lt;a href="https://github.com/kubernetes-csi/volume-data-source-validator">volume data source validator&lt;/a>
controller (alpha)&lt;/li>
&lt;/ul>
&lt;p>The purpose of these components is to generate warning events on PVCs with data sources
for which there is no populator.&lt;/p>
&lt;h2 id="putting-it-all-together">Putting it all together&lt;/h2>
&lt;p>To see how this works, you can install the sample &amp;quot;hello&amp;quot; populator and try it
out.&lt;/p>
&lt;p>First install the volume-data-source-validator controller.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">kubectl apply -f https://github.com/kubernetes-csi/volume-data-source-validator/blob/master/deploy/kubernetes/rbac-data-source-validator.yaml
kubectl apply -f https://github.com/kubernetes-csi/volume-data-source-validator/blob/master/deploy/kubernetes/setup-data-source-validator.yaml
&lt;/code>&lt;/pre>&lt;p>Next install the example populator.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">kubectl apply -f https://github.com/kubernetes-csi/lib-volume-populator/blob/master/example/hello-populator/crd.yaml
kubectl apply -f https://github.com/kubernetes-csi/lib-volume-populator/blob/master/example/hello-populator/deploy.yaml
&lt;/code>&lt;/pre>&lt;p>Create an instance of the &lt;code>Hello&lt;/code> CR, with some text.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hello.k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-hello&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fileName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example.txt&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fileContents&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello, world!&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create a PVC that refers to that CR as its data source.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>10Mi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">dataSourceRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hello.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-hello&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Filesystem&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, run a job that reads the file in the PVC.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-container&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>busybox:latest&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- cat&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- /mnt/example.txt&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>vol&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/mnt&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>vol&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">persistentVolumeClaim&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">claimName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Wait for the job to complete (including all of its dependencies).&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">kubectl wait --for=condition=Complete job/example-job
&lt;/code>&lt;/pre>&lt;p>And last examine the log from the job.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">kubectl logs job/example-job
Hello, world!
&lt;/code>&lt;/pre>&lt;p>Note that the volume already contained a text file with the string contents from
the CR. This is only the simplest example. Actual populators can set up the volume
to contain arbitrary contents.&lt;/p>
&lt;h2 id="how-to-write-your-own-volume-populator">How to write your own volume populator&lt;/h2>
&lt;p>Developers interested in writing new poplators are encouraged to use the
&lt;a href="https://github.com/kubernetes-csi/lib-volume-populator">lib-volume-populator&lt;/a> library
and to only supply a small controller wrapper around the library, and a pod image
capable of attaching to volumes and writing the appropriate data to the volume.&lt;/p>
&lt;p>Individual populators can be extremely generic such that they work with every type
of PVC, or they can do vendor specific things to rapidly fill a volume with data
if the volume was provisioned by a specific CSI driver from the same vendor, for
example, by communicating directly with the storage for that volume.&lt;/p>
&lt;h2 id="the-future">The future&lt;/h2>
&lt;p>As this feature is still in alpha, we expect to update the out of tree controllers
with more tests and documentation. The community plans to eventually re-implement
the populator library as a sidecar, for ease of operations.&lt;/p>
&lt;p>We hope to see some official community-supported populators for some widely-shared
use cases. Also, we expect that volume populators will be used by backup vendors
as a way to &amp;quot;restore&amp;quot; backups to volumes, and possibly a standardized API to do
this will evolve.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>The enhancement proposal,
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1495-volume-populators">Volume Populators&lt;/a>, includes lots of detail about the history and technical implementation
of this feature.&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-populators-and-data-sources">Volume populators and data sources&lt;/a>, within the documentation topic about persistent volumes,
explains how to use this feature in your cluster.&lt;/p>
&lt;p>Please get involved by joining the Kubernetes storage SIG to help us enhance this
feature. There are a lot of good ideas already and we'd be thrilled to have more!&lt;/p></description></item><item><title>Blog: Minimum Ready Seconds for StatefulSets</title><link>https://kubernetes.io/blog/2021/08/27/minreadyseconds-statefulsets/</link><pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/27/minreadyseconds-statefulsets/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Ravi Gudimetla (Red Hat), Maciej Szulik (Red Hat)&lt;/p>
&lt;p>This blog describes the notion of Availability for &lt;code>StatefulSet&lt;/code> workloads, and a new alpha feature in Kubernetes 1.22 which adds &lt;code>minReadySeconds&lt;/code> configuration for &lt;code>StatefulSets&lt;/code>.&lt;/p>
&lt;h2 id="what-problems-does-this-solve">What problems does this solve?&lt;/h2>
&lt;p>Prior to Kubernetes 1.22 release, once a &lt;code>StatefulSet&lt;/code> &lt;code>Pod&lt;/code> is in the &lt;code>Ready&lt;/code> state it is considered &lt;code>Available&lt;/code> to receive traffic. For some of the &lt;code>StatefulSet&lt;/code> workloads, it may not be the case. For example, a workload like Prometheus with multiple instances of Alertmanager, it should be considered &lt;code>Available&lt;/code> only when Alertmanager's state transfer is complete, not when the &lt;code>Pod&lt;/code> is in &lt;code>Ready&lt;/code> state. Since &lt;code>minReadySeconds&lt;/code> adds buffer, the state transfer may be complete before the &lt;code>Pod&lt;/code> becomes &lt;code>Available&lt;/code>. While this is not a fool proof way of identifying if the state transfer is complete or not, it gives a way to the end user to express their intention of waiting for sometime before the &lt;code>Pod&lt;/code> is considered &lt;code>Available&lt;/code> and it is ready to serve requests.&lt;/p>
&lt;p>Another case, where &lt;code>minReadySeconds&lt;/code> helps is when using &lt;code>LoadBalancer&lt;/code> &lt;code>Services&lt;/code> with cloud providers. Since &lt;code>minReadySeconds&lt;/code> adds latency after a &lt;code>Pod&lt;/code> is &lt;code>Ready&lt;/code>, it provides buffer time to prevent killing pods in rotation before new pods show up. Imagine a load balancer in unhappy path taking 10-15s to propagate. If you have 2 replicas then, you'd kill the second replica only after the first one is up but in reality, first replica cannot be seen because it is not yet ready to serve requests.&lt;/p>
&lt;p>So, in general, the notion of &lt;code>Availability&lt;/code> in &lt;code>StatefulSets&lt;/code> is pretty useful and this feature helps in solving the above problems. This is a feature that already exists for &lt;code>Deployments&lt;/code> and &lt;code>DaemonSets&lt;/code> and we now have them for &lt;code>StatefulSets&lt;/code> too to give users consistent workload experience.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>The statefulSet controller watches for both &lt;code>StatefulSets&lt;/code> and the &lt;code>Pods&lt;/code> associated with them. When the feature gate associated with this feature is enabled, the statefulSet controller identifies how long a particular &lt;code>Pod&lt;/code> associated with a &lt;code>StatefulSet&lt;/code> has been in the &lt;code>Running&lt;/code> state.&lt;/p>
&lt;p>If this value is greater than or equal to the time specified by the end user in &lt;code>.spec.minReadySeconds&lt;/code> field, the statefulSet controller updates a field called &lt;code>availableReplicas&lt;/code> in the &lt;code>StatefulSet&lt;/code>'s status subresource to include this &lt;code>Pod&lt;/code>. The &lt;code>status.availableReplicas&lt;/code> in &lt;code>StatefulSet&lt;/code>'s status is an integer field which tracks the number of pods that are &lt;code>Available&lt;/code>.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>You are required to prepare the following things in order to try out the feature:&lt;/p>
&lt;ul>
&lt;li>Download and install a kubectl greater than v1.22.0 version&lt;/li>
&lt;li>Switch on the feature gate with the command line flag &lt;code>--feature-gates=StatefulSetMinReadySeconds=true&lt;/code> on &lt;code>kube-apiserver&lt;/code> and &lt;code>kube-controller-manager&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>After successfully starting &lt;code>kube-apiserver&lt;/code> and &lt;code>kube-controller-manager&lt;/code>, you will see &lt;code>AvailableReplicas&lt;/code> in the status and &lt;code>minReadySeconds&lt;/code> of spec (with a default value of 0).&lt;/p>
&lt;p>Specify a value for &lt;code>minReadySeconds&lt;/code> for any StatefulSet and you can check if &lt;code>Pods&lt;/code> are available or not by checking &lt;code>AvailableReplicas&lt;/code> field using:
&lt;code>kubectl get statefulset/&amp;lt;name_of_the_statefulset&amp;gt; -o yaml&lt;/code>&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;ul>
&lt;li>Read the KEP: &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/2599-minreadyseconds-for-statefulsets#readme">minReadySeconds for StatefulSets&lt;/a>&lt;/li>
&lt;li>Read the documentation: &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds">Minimum ready seconds&lt;/a> for StatefulSet&lt;/li>
&lt;li>Review the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/">API definition&lt;/a> for StatefulSet&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>Please reach out to us in the &lt;a href="https://kubernetes.slack.com/archives/C18NZM5K9">#sig-apps&lt;/a> channel on Slack (visit &lt;a href="https://slack.k8s.io/">https://slack.k8s.io/&lt;/a> for an invitation if you need one), or on the SIG Apps mailing list: &lt;a href="mailto:kubernetes-sig-apps@googlegroups.com">kubernetes-sig-apps@googlegroups.com&lt;/a>&lt;/p></description></item><item><title>Blog: Enable seccomp for all workloads with a new v1.22 alpha feature</title><link>https://kubernetes.io/blog/2021/08/25/seccomp-default/</link><pubDate>Wed, 25 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/25/seccomp-default/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Sascha Grunert, Red Hat&lt;/p>
&lt;p>This blog post is about a new Kubernetes feature introduced in v1.22, which adds
an additional security layer on top of the existing seccomp support. Seccomp is
a security mechanism for Linux processes to filter system calls (syscalls) based
on a set of defined rules. Applying seccomp profiles to containerized workloads
is one of the key tasks when it comes to enhancing the security of the
application deployment. Developers, site reliability engineers and
infrastructure administrators have to work hand in hand to create, distribute
and maintain the profiles over the applications life-cycle.&lt;/p>
&lt;p>You can use the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1">&lt;code>securityContext&lt;/code>&lt;/a> field of Pods and their
containers can be used to adjust security related configurations of the
workload. Kubernetes introduced dedicated &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1">seccomp related API
fields&lt;/a> in this &lt;code>SecurityContext&lt;/code> with the &lt;a href="https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/#graduated-to-stable">graduation of seccomp to
General Availability (GA)&lt;/a> in v1.19.0. This enhancement allowed an easier
way to specify if the whole pod or a specific container should run as:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Unconfined&lt;/code>: seccomp will not be enabled&lt;/li>
&lt;li>&lt;code>RuntimeDefault&lt;/code>: the container runtimes default profile will be used&lt;/li>
&lt;li>&lt;code>Localhost&lt;/code>: a node local profile will be applied, which is being referenced
by a relative path to the seccomp profile root (&lt;code>&amp;lt;kubelet-root-dir&amp;gt;/seccomp&lt;/code>)
of the kubelet&lt;/li>
&lt;/ul>
&lt;p>With the graduation of seccomp, nothing has changed from an overall security
perspective, because &lt;code>Unconfined&lt;/code> is still the default. This is totally fine if
you consider this from the upgrade path and backwards compatibility perspective of
Kubernetes releases. But it also means that it is more likely that a workload
runs without seccomp at all, which should be fixed in the long term.&lt;/p>
&lt;h2 id="seccompdefault-to-the-rescue">&lt;code>SeccompDefault&lt;/code> to the rescue&lt;/h2>
&lt;p>Kubernetes v1.22.0 introduces a new kubelet &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates">feature gate&lt;/a>
&lt;code>SeccompDefault&lt;/code>, which has been added in &lt;code>alpha&lt;/code> state as every other new
feature. This means that it is disabled by default and can be enabled manually
for every single Kubernetes node.&lt;/p>
&lt;p>What does the feature do? Well, it just changes the default seccomp profile from
&lt;code>Unconfined&lt;/code> to &lt;code>RuntimeDefault&lt;/code>. If not specified differently in the pod
manifest, then the feature will add a higher set of security constraints by
using the default profile of the container runtime. These profiles may differ
between runtimes like &lt;a href="https://github.com/cri-o/cri-o/blob/fe30d62/vendor/github.com/containers/common/pkg/seccomp/default_linux.go#L45">CRI-O&lt;/a> or &lt;a href="https://github.com/containerd/containerd/blob/e1445df/contrib/seccomp/seccomp_default.go#L51">containerd&lt;/a>. They also differ for
its used hardware architectures. But generally speaking, those default profiles
allow a common amount of syscalls while blocking the more dangerous ones, which
are unlikely or unsafe to be used in a containerized application.&lt;/p>
&lt;h3 id="enabling-the-feature">Enabling the feature&lt;/h3>
&lt;p>Two kubelet configuration changes have to be made to enable the feature:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Enable the feature&lt;/strong> gate by setting the &lt;code>SeccompDefault=true&lt;/code> via the command
line (&lt;code>--feature-gates&lt;/code>) or the &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file">kubelet configuration&lt;/a> file.&lt;/li>
&lt;li>&lt;strong>Turn on the feature&lt;/strong> by enabling the feature by adding the
&lt;code>--seccomp-default&lt;/code> command line flag or via the &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file">kubelet
configuration&lt;/a> file (&lt;code>seccompDefault: true&lt;/code>).&lt;/li>
&lt;/ol>
&lt;p>The kubelet will error on startup if only one of the above steps have been done.&lt;/p>
&lt;h3 id="trying-it-out">Trying it out&lt;/h3>
&lt;p>If the feature is enabled on a node, then you can create a new workload like
this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-container&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx:1.21&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now it is possible to inspect the used seccomp profile by using
&lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> while investigating the containers &lt;a href="https://github.com/opencontainers/runtime-spec/blob/0c021c1/config-linux.md#seccomp">runtime
specification&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#b8860b">CONTAINER_ID&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>sudo crictl ps -q --name&lt;span style="color:#666">=&lt;/span>test-container&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
sudo crictl inspect &lt;span style="color:#b8860b">$CONTAINER_ID&lt;/span> | jq .info.runtimeSpec.linux.seccomp
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">{&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;defaultAction&amp;#34;: &lt;/span>&lt;span style="color:#b44">&amp;#34;SCMP_ACT_ERRNO&amp;#34;&lt;/span>,&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;architectures&amp;#34;: &lt;/span>[&lt;span style="color:#b44">&amp;#34;SCMP_ARCH_X86_64&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;SCMP_ARCH_X86&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;SCMP_ARCH_X32&amp;#34;&lt;/span>],&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;syscalls&amp;#34;: &lt;/span>[&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;names&amp;#34;: &lt;/span>[&lt;span style="color:#b44">&amp;#34;_llseek&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;_newselect&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;accept&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>…, &amp;#34;write&amp;#34;, &amp;#34;writev&amp;#34;],&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">&amp;#34;action&amp;#34;: &lt;/span>&lt;span style="color:#b44">&amp;#34;SCMP_ACT_ALLOW&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>},&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>…&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>}&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can see that the lower level container runtime (&lt;a href="https://github.com/cri-o/cri-o">CRI-O&lt;/a> and
&lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> in our case), successfully applied the default seccomp profile.
This profile denies all syscalls per default, while allowing commonly used ones
like &lt;a href="https://man7.org/linux/man-pages/man2/accept.2.html">&lt;code>accept&lt;/code>&lt;/a> or &lt;a href="https://man7.org/linux/man-pages/man2/write.2.html">&lt;code>write&lt;/code>&lt;/a>.&lt;/p>
&lt;p>Please note that the feature will not influence any Kubernetes API for now.
Therefore, it is not possible to retrieve the used seccomp profile via &lt;code>kubectl&lt;/code>
&lt;code>get&lt;/code> or &lt;code>describe&lt;/code> if the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1">&lt;code>SeccompProfile&lt;/code>&lt;/a> field is unset within the
&lt;code>SecurityContext&lt;/code>.&lt;/p>
&lt;p>The feature also works when using multiple containers within a pod, for example
if you create a pod like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-container-nginx&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx:1.21&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">seccompProfile&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Unconfined&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-container-redis&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>redis:6.2&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>then you should see that the &lt;code>test-container-nginx&lt;/code> runs without a seccomp profile:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">sudo crictl inspect &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>sudo crictl ps -q --name&lt;span style="color:#666">=&lt;/span>test-container-nginx&lt;span style="color:#a2f;font-weight:bold">)&lt;/span> |
jq &lt;span style="color:#b44">&amp;#39;.info.runtimeSpec.linux.seccomp == null&amp;#39;&lt;/span>
&lt;span style="color:#a2f">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Whereas the container &lt;code>test-container-redis&lt;/code> runs with &lt;code>RuntimeDefault&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">sudo crictl inspect &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>sudo crictl ps -q --name&lt;span style="color:#666">=&lt;/span>test-container-redis&lt;span style="color:#a2f;font-weight:bold">)&lt;/span> |
jq &lt;span style="color:#b44">&amp;#39;.info.runtimeSpec.linux.seccomp != null&amp;#39;&lt;/span>
&lt;span style="color:#a2f">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The same applies to the pod itself, which also runs with the default profile:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">sudo crictl inspectp &lt;span style="color:#666">(&lt;/span>sudo crictl pods -q --name test-pod&lt;span style="color:#666">)&lt;/span> |
jq &lt;span style="color:#b44">&amp;#39;.info.runtimeSpec.linux.seccomp != null&amp;#39;&lt;/span>
&lt;span style="color:#a2f">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="upgrade-strategy">Upgrade strategy&lt;/h3>
&lt;p>It is recommended to enable the feature in multiple steps, whereas different
risks and mitigations exist for each one.&lt;/p>
&lt;h4 id="feature-gate-enabling">Feature gate enabling&lt;/h4>
&lt;p>Enabling the feature gate at the kubelet level will not turn on the feature, but
will make it possible by using the &lt;code>SeccompDefault&lt;/code> kubelet configuration or the
&lt;code>--seccomp-default&lt;/code> CLI flag. This can be done by an administrator for the whole
cluster or only a set of nodes.&lt;/p>
&lt;h4 id="testing-the-application">Testing the Application&lt;/h4>
&lt;p>If you're trying this within a dedicated test environment, you have to ensure
that the application code does not trigger syscalls blocked by the
&lt;code>RuntimeDefault&lt;/code> profile before enabling the feature on a node. This can be done
by:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>Recommended&lt;/em>: Analyzing the code (manually or by running the application with
&lt;a href="https://man7.org/linux/man-pages/man1/strace.1.html">strace&lt;/a>) for any executed syscalls which may be blocked by the
default profiles. If that's the case, then you can override the default by
explicitly setting the pod or container to run as &lt;code>Unconfined&lt;/code>. Alternatively,
you can create a custom seccomp profile (see optional step below).
profile based on the default by adding the additional syscalls to the
&lt;code>&amp;quot;action&amp;quot;: &amp;quot;SCMP_ACT_ALLOW&amp;quot;&lt;/code> section.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Recommended&lt;/em>: Manually set the profile to the target workload and use a
rolling upgrade to deploy into production. Rollback the deployment if the
application does not work as intended.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Optional&lt;/em>: Run the application against an end-to-end test suite to trigger
all relevant code paths with &lt;code>RuntimeDefault&lt;/code> enabled. If a test fails, use
the same mitigation as mentioned above.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Optional&lt;/em>: Create a custom seccomp profile based on the default and change
its default action from &lt;code>SCMP_ACT_ERRNO&lt;/code> to &lt;code>SCMP_ACT_LOG&lt;/code>. This means that
the seccomp filter for unknown syscalls will have no effect on the application
at all, but the system logs will now indicate which syscalls may be blocked.
This requires at least a Kernel version 4.14 as well as a recent &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a>
release. Monitor the application hosts audit logs (defaults to
&lt;code>/var/log/audit/audit.log&lt;/code>) or syslog entries (defaults to &lt;code>/var/log/syslog&lt;/code>)
for syscalls via &lt;code>type=SECCOMP&lt;/code> (for audit) or &lt;code>type=1326&lt;/code> (for syslog).
Compare the syscall ID with those &lt;a href="https://github.com/torvalds/linux/blob/7bb7f2a/arch/x86/entry/syscalls/syscall_64.tbl">listed in the Linux Kernel
sources&lt;/a> and add them to the custom profile. Be aware that custom
audit policies may lead into missing syscalls, depending on the configuration
of auditd.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Optional&lt;/em>: Use cluster additions like the &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator">Security Profiles Operator&lt;/a>
for profiling the application via its &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/c90ef3a/installation-usage.md#record-profiles-from-workloads-with-profilerecordings">log enrichment&lt;/a> capabilities or
recording a profile by using its &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/c90ef3a/installation-usage.md#using-the-log-enricher">recording feature&lt;/a>. This makes the
above mentioned manual log investigation obsolete.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="deploying-the-modified-application">Deploying the modified application&lt;/h4>
&lt;p>Based on the outcome of the application tests, it may be required to change the
application deployment by either specifying &lt;code>Unconfined&lt;/code> or a custom seccomp
profile. This is not the case if the application works as intended with
&lt;code>RuntimeDefault&lt;/code>.&lt;/p>
&lt;h4 id="enable-the-kubelet-configuration">Enable the kubelet configuration&lt;/h4>
&lt;p>If everything went well, then the feature is ready to be enabled by the kubelet
configuration or its corresponding CLI flag. This should be done on a per-node
basis to reduce the overall risk of missing a syscall during the investigations
when running the application tests. If it's possible to monitor audit logs
within the cluster, then it's recommended to do this for eventually missed
seccomp events. If the application works as intended then the feature can be
enabled for further nodes within the cluster.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Thank you for reading this blog post! I hope you enjoyed to see how the usage of
seccomp profiles has been evolved in Kubernetes over the past releases as much
as I do. On your own cluster, change the default seccomp profile to
&lt;code>RuntimeDefault&lt;/code> (using this new feature) and see the security benefits, and, of
course, feel free to reach out any time for feedback or questions.&lt;/p>
&lt;hr>
&lt;p>&lt;em>Editor's note: If you have any questions or feedback about this blog post, feel
free to reach out via the &lt;a href="https://kubernetes.slack.com/messages/sig-node">Kubernetes slack in #sig-node&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Blog: Alpha in v1.22: Windows HostProcess Containers</title><link>https://kubernetes.io/blog/2021/08/16/windows-hostprocess-containers/</link><pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/16/windows-hostprocess-containers/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Brandon Smith (Microsoft)&lt;/p>
&lt;p>Kubernetes v1.22 introduced a new alpha feature for clusters that
include Windows nodes: HostProcess containers.&lt;/p>
&lt;p>HostProcess containers aim to extend the Windows container model to enable a wider
range of Kubernetes cluster management scenarios. HostProcess containers run
directly on the host and maintain behavior and access similar to that of a regular
process. With HostProcess containers, users can package and distribute management
operations and functionalities that require host access while retaining versioning
and deployment methods provided by containers. This allows Windows containers to
be used for a variety of device plugin, storage, and networking management scenarios
in Kubernetes. With this comes the enablement of host network mode—allowing
HostProcess containers to be created within the host's network namespace instead of
their own. HostProcess containers can also be built on top of existing Windows server
2019 (or later) base images, managed through the Windows container runtime, and run
as any user that is available on or in the domain of the host machine.&lt;/p>
&lt;p>Linux privileged containers are currently used for a variety of key scenarios in
Kubernetes, including kube-proxy (via kubeadm), storage, and networking scenarios.
Support for these scenarios in Windows previously required workarounds via proxies
or other implementations. Using HostProcess containers, cluster operators no longer
need to log onto and individually configure each Windows node for administrative
tasks and management of Windows services. Operators can now utilize the container
model to deploy management logic to as many clusters as needed with ease.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>Windows HostProcess containers are implemented with Windows &lt;em>Job Objects&lt;/em>, a break from the
previous container model using server silos. Job objects are components of the Windows OS which offer the ability to
manage a group of processes as a group (a.k.a. &lt;em>jobs&lt;/em>) and assign resource constraints to the
group as a whole. Job objects are specific to the Windows OS and are not associated with the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Job API&lt;/a>. They have no process or file system isolation,
enabling the privileged payload to view and edit the host file system with the
correct permissions, among other host resources. The init process, and any processes
it launches or that are explicitly launched by the user, are all assigned to the
job object of that container. When the init process exits or is signaled to exit,
all the processes in the job will be signaled to exit, the job handle will be
closed and the storage will be unmounted.&lt;/p>
&lt;p>HostProcess and Linux privileged containers enable similar scenarios but differ
greatly in their implementation (hence the naming difference). HostProcess containers
have their own pod security policies. Those used to configure Linux privileged
containers &lt;strong>do not&lt;/strong> apply. Enabling privileged access to a Windows host is a
fundamentally different process than with Linux so the configuration and
capabilities of each differ significantly. Below is a diagram detailing the
overall architecture of Windows HostProcess containers:&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2021/08/16/windows-hostprocess-containers/hostprocess-architecture.png"
alt="HostProcess Architecture"/>
&lt;/figure>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>HostProcess containers can be run from within a
&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod">HostProcess Pod&lt;/a>.
With the feature enabled on Kubernetes version 1.22, a containerd container runtime of
1.5.4 or higher, and the latest version of hcsshim, deploying a pod spec with the
&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/#before-you-begin">correct HostProcess configuration&lt;/a>
will enable you to run HostProcess containers. To get started with running
Windows containers see the general guidance for &lt;a href="https://kubernetes.io/docs/setup/production-environment/windows/">Windows in Kubernetes&lt;/a>&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Work through &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/">Create a Windows HostProcess Pod&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Read about Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Read the enhancement proposal &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-windows/1981-windows-privileged-container-support">Windows Privileged Containers and Host Networking Mode&lt;/a> (KEP-1981)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>HostProcess containers are in active development. SIG Windows welcomes suggestions from the community.
Get involved with &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows">SIG Windows&lt;/a>
to contribute!&lt;/p></description></item><item><title>Blog: Kubernetes Memory Manager moves to beta</title><link>https://kubernetes.io/blog/2021/08/11/kubernetes-1-22-feature-memory-manager-moves-to-beta/</link><pubDate>Wed, 11 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/11/kubernetes-1-22-feature-memory-manager-moves-to-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Artyom Lukianov (Red Hat), Cezary Zukowski (Samsung)&lt;/p>
&lt;p>The blog post explains some of the internals of the &lt;em>Memory manager&lt;/em>, a beta feature
of Kubernetes 1.22. In Kubernetes, the Memory Manager is a
&lt;a href="https://kubernetes.io/docs/concepts/overview/components/#kubelet">kubelet&lt;/a> subcomponent.
The memory manage provides guaranteed memory (and hugepages)
allocation for pods in the &lt;code>Guaranteed&lt;/code> &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#qos-classes">QoS class&lt;/a>.&lt;/p>
&lt;p>This blog post covers:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="#Why-do-you-need-it?">Why do you need it?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#How-does-it-work?">The internal details of how the &lt;strong>MemoryManager&lt;/strong> works&lt;/a>&lt;/li>
&lt;li>&lt;a href="#Current-limitations">Current limitations of the &lt;strong>MemoryManager&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="#Future-work-for-the-Memory-Manager">Future work for the &lt;strong>MemoryManager&lt;/strong>&lt;/a>&lt;/li>
&lt;/ol>
&lt;h2 id="why-do-you-need-it">Why do you need it?&lt;/h2>
&lt;p>Some Kubernetes workloads run on nodes with
&lt;a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">non-uniform memory access&lt;/a> (NUMA).
Suppose you have NUMA nodes in your cluster. In that case, you'll know about the potential for extra latency when
compute resources need to access memory on the different NUMA locality.&lt;/p>
&lt;p>To get the best performance and latency for your workload, container CPUs,
peripheral devices, and memory should all be aligned to the same NUMA
locality.
Before Kubernetes v1.22, the kubelet already provided a set of managers to
align CPUs and PCI devices, but you did not have a way to align memory.
The Linux kernel was able to make best-effort attempts to allocate
memory for tasks from the same NUMA node where the container is
executing are placed, but without any guarantee about that placement.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>The memory manager is doing two main things:&lt;/p>
&lt;ul>
&lt;li>provides the topology hint to the Topology Manager&lt;/li>
&lt;li>allocates the memory for containers and updates the state&lt;/li>
&lt;/ul>
&lt;p>The overall sequence of the Memory Manager under the Kubelet&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-08-11-memory-manager-moves-to-beta/MemoryManagerDiagram.svg" alt="MemoryManagerDiagram" title="MemoryManagerDiagram">&lt;/p>
&lt;p>During the Admission phase:&lt;/p>
&lt;ol>
&lt;li>When first handling a new pod, the kubelet calls the TopologyManager's &lt;code>Admit()&lt;/code> method.&lt;/li>
&lt;li>The Topology Manager is calling &lt;code>GetTopologyHints()&lt;/code> for every hint provider including the Memory Manager.&lt;/li>
&lt;li>The Memory Manager calculates all possible NUMA nodes combinations for every container inside the pod and returns hints to the Topology Manager.&lt;/li>
&lt;li>The Topology Manager calls to &lt;code>Allocate()&lt;/code> for every hint provider including the Memory Manager.&lt;/li>
&lt;li>The Memory Manager allocates the memory under the state according to the hint that the Topology Manager chose.&lt;/li>
&lt;/ol>
&lt;p>During Pod creation:&lt;/p>
&lt;ol>
&lt;li>The kubelet calls &lt;code>PreCreateContainer()&lt;/code>.&lt;/li>
&lt;li>For each container, the Memory Manager looks the NUMA nodes where it allocated the
memory for the container and then returns that information to the kubelet.&lt;/li>
&lt;li>The kubelet creates the container, via CRI, using a container specification
that incorporates information from the Memory Manager information.&lt;/li>
&lt;/ol>
&lt;h3 id="let-s-talk-about-the-configuration">Let's talk about the configuration&lt;/h3>
&lt;p>By default, the Memory Manager runs with the &lt;code>None&lt;/code> policy, meaning it will just
relax and not do anything. To make use of the Memory Manager, you should set
two command line options for the kubelet:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--memory-manager-policy=Static&lt;/code>&lt;/li>
&lt;li>&lt;code>--reserved-memory=&amp;quot;&amp;lt;numaNodeID&amp;gt;:&amp;lt;resourceName&amp;gt;=&amp;lt;quantity&amp;gt;&amp;quot;&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>The value for &lt;code>--memory-manager-policy&lt;/code> is straightforward: &lt;code>Static&lt;/code>. Deciding what to specify for &lt;code>--reserved-memory&lt;/code> takes more thought. To configure it correctly, you should follow two main rules:&lt;/p>
&lt;ul>
&lt;li>The amount of reserved memory for the &lt;code>memory&lt;/code> resource must be greater than zero.&lt;/li>
&lt;li>The amount of reserved memory for the resource type must be equal
to &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">NodeAllocatable&lt;/a>
(&lt;code>kube-reserved + system-reserved + eviction-hard&lt;/code>) for the resource.
You can read more about memory reservations in &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">Reserve Compute Resources for System Daemons&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-08-11-memory-manager-moves-to-beta/ReservedMemory.svg" alt="Reserved memory">&lt;/p>
&lt;h2 id="current-limitations">Current limitations&lt;/h2>
&lt;p>The 1.22 release and promotion to beta brings along enhancements and fixes, but the Memory Manager still has several limitations.&lt;/p>
&lt;h3 id="single-vs-cross-numa-node-allocation">Single vs Cross NUMA node allocation&lt;/h3>
&lt;p>The NUMA node can not have both single and cross NUMA node allocations. When the container memory is pinned to two or more NUMA nodes, we can not know from which NUMA node the container will consume the memory.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-08-11-memory-manager-moves-to-beta/SingleCrossNUMAAllocation.svg" alt="Single vs Cross NUMA allocation" title="SingleCrossNUMAAllocation">&lt;/p>
&lt;ol>
&lt;li>The &lt;code>container1&lt;/code> started on the NUMA node 0 and requests &lt;em>5Gi&lt;/em> of the memory but currently is consuming only &lt;em>3Gi&lt;/em> of the memory.&lt;/li>
&lt;li>For container2 the memory request is 10Gi, and no single NUMA node can satisfy it.&lt;/li>
&lt;li>The &lt;code>container2&lt;/code> consumes &lt;em>3.5Gi&lt;/em> of the memory from the NUMA node 0, but once the &lt;code>container1&lt;/code> will require more memory, it will not have it, and the kernel will kill one of the containers with the &lt;em>OOM&lt;/em> error.&lt;/li>
&lt;/ol>
&lt;p>To prevent such issues, the Memory Manager will fail the admission of the &lt;code>container2&lt;/code> until the machine has two NUMA nodes without a single NUMA node allocation.&lt;/p>
&lt;h3 id="works-only-for-guaranteed-pods">Works only for Guaranteed pods&lt;/h3>
&lt;p>The Memory Manager can not guarantee memory allocation for Burstable pods,
also when the Burstable pod has specified equal memory limit and request.&lt;/p>
&lt;p>Let's assume you have two Burstable pods: &lt;code>pod1&lt;/code> has containers with
equal memory request and limits, and &lt;code>pod2&lt;/code> has containers only with a
memory request set. You want to guarantee memory allocation for the &lt;code>pod1&lt;/code>.
To the Linux kernel, processes in either pod have the same &lt;em>OOM score&lt;/em>,
once the kernel finds that it does not have enough memory, it can kill
processes that belong to pod &lt;code>pod1&lt;/code>.&lt;/p>
&lt;h3 id="memory-fragmentation">Memory fragmentation&lt;/h3>
&lt;p>The sequence of Pods and containers that start and stop can fragment the memory on NUMA nodes.
The alpha implementation of the Memory Manager does not have any mechanism to balance pods and defragment memory back.&lt;/p>
&lt;h2 id="future-work-for-the-memory-manager">Future work for the Memory Manager&lt;/h2>
&lt;p>We do not want to stop with the current state of the Memory Manager and are looking to
make improvements, including in the following areas.&lt;/p>
&lt;h3 id="make-the-memory-manager-allocation-algorithm-smarter">Make the Memory Manager allocation algorithm smarter&lt;/h3>
&lt;p>The current algorithm ignores distances between NUMA nodes during the
calculation of the allocation. If same-node placement isn't available, we can still
provide better performance compared to the current implementation, by changing the
Memory Manager to prefer the closest NUMA nodes for cross-node allocation.&lt;/p>
&lt;h3 id="reduce-the-number-of-admission-errors">Reduce the number of admission errors&lt;/h3>
&lt;p>The default Kubernetes scheduler is not aware of the node's NUMA topology, and it can be a reason for many admission errors during the pod start.
We're hoping to add a KEP (Kubernetes Enhancement Proposal) to cover improvements in this area.
Follow &lt;a href="https://github.com/kubernetes/enhancements/issues/2044">Topology aware scheduler plugin in kube-scheduler&lt;/a> to see how this idea progresses.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>With the promotion of the Memory Manager to beta in 1.22, we encourage everyone to give it a try and look forward to any feedback you may have. While there are still several limitations, we have a set of enhancements planned to address them and look forward to providing you with many new features in upcoming releases.
If you have ideas for additional enhancements or a desire for certain features, please let us know. The team is always open to suggestions to enhance and improve the Memory Manager.
We hope you have found this blog informative and helpful! Let us know if you have any questions or comments.&lt;/p>
&lt;p>You can contact us via:&lt;/p>
&lt;ul>
&lt;li>The Kubernetes &lt;a href="https://kubernetes.slack.com/messages/sig-node">#sig-node &lt;/a>
channel in Slack (visit &lt;a href="https://slack.k8s.io/">https://slack.k8s.io/&lt;/a> for an invitation if you need one)&lt;/li>
&lt;li>The SIG Node mailing list, &lt;a href="https://groups.google.com/g/kubernetes-sig-node">kubernetes-sig-node@googlegroups.com&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.22: CSI Windows Support (with CSI Proxy) reaches GA</title><link>https://kubernetes.io/blog/2021/08/09/csi-windows-support-with-csi-proxy-reaches-ga/</link><pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/09/csi-windows-support-with-csi-proxy-reaches-ga/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Mauricio Poppe (Google), Jing Xu (Google), and Deep Debroy (Apple)&lt;/p>
&lt;p>&lt;em>The stable version of CSI Proxy for Windows has been released alongside Kubernetes 1.22. CSI Proxy enables CSI Drivers running on Windows nodes to perform privileged storage operations.&lt;/em>&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Container Storage Interface (CSI) for Kubernetes went GA in the Kubernetes 1.13 release. CSI has become the standard for exposing block and file storage to containerized workloads on Container Orchestration systems (COs) like Kubernetes. It enables third-party storage providers to write and deploy plugins without the need to alter the core Kubernetes codebase. Legacy in-tree drivers are deprecated and new storage features are introduced in CSI, therefore it is important to get CSI Drivers to work on Windows.&lt;/p>
&lt;p>A CSI Driver in Kubernetes has two main components: a controller plugin which runs in the control plane and a node plugin which runs on every node.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The controller plugin generally does not need direct access to the host and can perform all its operations through the Kubernetes API and external control plane services.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The node plugin, however, requires direct access to the host for making block devices and/or file systems available to the Kubernetes kubelet. Due to the missing capability of running privileged operations from containers on Windows nodes &lt;a href="https://kubernetes.io/blog/2020/04/03/kubernetes-1-18-feature-windows-csi-support-alpha/">CSI Proxy was introduced as alpha in Kubernetes 1.18&lt;/a> as a way to enable containers to perform privileged storage operations. This enables containerized CSI Drivers to run on Windows nodes.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-csi-proxy-and-how-do-csi-drivers-interact-with-it">What's CSI Proxy and how do CSI drivers interact with it?&lt;/h2>
&lt;p>When a workload that uses persistent volumes is scheduled, it'll go through a sequence of steps defined in the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI Spec&lt;/a>. First, the workload will be scheduled to run on a node. Then the controller component of a CSI Driver will attach the persistent volume to the node. Finally the node component of a CSI Driver will mount the persistent volume on the node.&lt;/p>
&lt;p>The node component of a CSI Driver needs to run on Windows nodes to support Windows workloads. Various privileged operations like scanning of disk devices, mounting of file systems, etc. cannot be done from a containerized application running on Windows nodes yet (&lt;a href="https://github.com/kubernetes/enhancements/issues/1981">Windows HostProcess containers&lt;/a> introduced in Kubernetes 1.22 as alpha enable functionalities that require host access like the operations mentioned before). However, we can perform these operations through a binary (CSI Proxy) that's pre-installed on the Window nodes. CSI Proxy has a client-server architecture and allows CSI drivers to issue privileged storage operations through a gRPC interface exposed over named pipes created during the startup of CSI Proxy.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-08-09-csi-windows-support-with-csi-proxy-reaches-ga/csi-proxy.png" alt="CSI Proxy Architecture">&lt;/p>
&lt;h2 id="csi-proxy-reaches-ga">CSI Proxy reaches GA&lt;/h2>
&lt;p>The CSI Proxy development team has worked closely with storage vendors, many of whom started integrating CSI Proxy into their CSI Drivers and provided feedback as early as CSI Proxy design proposal. This cooperation uncovered use cases where additional APIs were needed, found bugs, and identified areas for documentation improvement.&lt;/p>
&lt;p>The CSI Proxy design &lt;a href="https://github.com/kubernetes/enhancements/pull/2737">KEP&lt;/a> has been updated to reflect the current CSI Proxy architecture. Additional &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/DEVELOPMENT.md">development documentation&lt;/a> is included for contributors interested in helping with new features or bug fixes.&lt;/p>
&lt;p>Before we reached GA we wanted to make sure that our API is simple and consistent. We went through an extensive API review of the v1beta API groups where we made sure that the CSI Proxy API methods and messages are consistent with the naming conventions defined in the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI Spec&lt;/a>. As part of this effort we're graduating the &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/apis/disk_v1.md">Disk&lt;/a>, &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/apis/filesystem_v1.md">Filesystem&lt;/a>, &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/apis/smb_v1.md">SMB&lt;/a> and &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/docs/apis/volume_v1.md">Volume&lt;/a> API groups to v1.&lt;/p>
&lt;p>Additional Windows system APIs to get information from the Windows nodes and support to mount iSCSI targets in Windows nodes, are available as alpha APIs in the &lt;a href="https://github.com/kubernetes-csi/csi-proxy/tree/v1.0.0/client/api/system/v1alpha1">System API&lt;/a> and the &lt;a href="https://github.com/kubernetes-csi/csi-proxy/tree/v1.0.0/client/api/iscsi/v1alpha2">iSCSI API&lt;/a>. These APIs will continue to be improved before we graduate them to v1.&lt;/p>
&lt;p>CSI Proxy v1 is compatible with all the previous v1betaX releases. The GA &lt;code>csi-proxy.exe&lt;/code> binary can handle requests from v1betaX clients thanks to the autogenerated conversion layer that transforms any versioned client request to a version-agnostic request that the server can process. Several &lt;a href="https://github.com/kubernetes-csi/csi-proxy/tree/v1.0.0/integrationtests">integration tests&lt;/a> were added for all the API versions of the API groups that are graduating to v1 to ensure that CSI Proxy is backwards compatible.&lt;/p>
&lt;p>Version drift between CSI Proxy and the CSI Drivers that interact with it was also carefully considered. A &lt;a href="https://github.com/kubernetes-csi/csi-proxy/pull/124">connection fallback mechanism&lt;/a> has been provided for CSI Drivers to handle multiple versions of CSI Proxy for a smooth upgrade to v1. This allows CSI Drivers, like the GCE PD CSI Driver, &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/pull/738">to recognize which version of the CSI Proxy binary is running&lt;/a> and handle multiple versions of the CSI Proxy binary deployed on the node.&lt;/p>
&lt;p>CSI Proxy v1 is already being used by many CSI Drivers, including the &lt;a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver/pull/966">AWS EBS CSI Driver&lt;/a>, &lt;a href="https://github.com/kubernetes-sigs/azuredisk-csi-driver/pull/919">Azure Disk CSI Driver&lt;/a>, &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/pull/738">GCE PD CSI Driver&lt;/a>, and &lt;a href="https://github.com/kubernetes-csi/csi-driver-smb/pull/319">SMB CSI Driver&lt;/a>.&lt;/p>
&lt;h2 id="future-plans">Future plans&lt;/h2>
&lt;p>We're very excited for the future of CSI Proxy. With the upcoming &lt;a href="https://github.com/kubernetes/enhancements/issues/1981">Windows HostProcess containers&lt;/a>, we are considering converting the CSI Proxy in to a library consumed by CSI Drivers in addition to the current client/server design. This will allow us to iterate faster on new features because the &lt;code>csi-proxy.exe&lt;/code> binary will no longer be needed.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved?&lt;/h2>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. Those interested in getting involved with the design and development of CSI Proxy, or any part of the Kubernetes Storage system, may join the Kubernetes Storage Special Interest Group (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p>
&lt;p>For those interested in more details about CSI support in Windows please reach out in the &lt;a href="https://kubernetes.slack.com/messages/csi-windows">#csi-windows&lt;/a> Kubernetes slack channel.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>CSI-Proxy received many contributions from members of the Kubernetes community. We thank all of the people that contributed to CSI Proxy with design reviews, bug reports, bug fixes, and for their continuous support in reaching this milestone:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/andyzhangx">Andy Zhang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/jmpfar">Dan Ilan&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ddebroy">Deep Debroy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/humblec">Humble Devassy Chirammal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/jingxu97">Jing Xu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/wk8">Jean Rougé&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/liggitt">Jordan Liggitt&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ksubrmnn">Kalya Subramanian&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kkmsft">Krishnakumar R&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/manueltellez">Manuel Tellez&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/marosset">Mark Rossetti&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/mauriciopoppe">Mauricio Poppe&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/wongma7">Matthew Wong&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/msau42">Michelle Au&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/PatrickLang">Patrick Lang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/saad-ali">Saad Ali&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/yujuhong">Yuju Hong&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: New in Kubernetes v1.22: alpha support for using swap memory</title><link>https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha/</link><pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Elana Hashman (Red Hat)&lt;/p>
&lt;p>The 1.22 release introduced alpha support for configuring swap memory usage for
Kubernetes workloads on a per-node basis.&lt;/p>
&lt;p>In prior releases, Kubernetes did not support the use of swap memory on Linux,
as it is difficult to provide guarantees and account for pod memory utilization
when swap is involved. As part of Kubernetes' earlier design, swap support was
considered out of scope, and a kubelet would by default fail to start if swap
was detected on a node.&lt;/p>
&lt;p>However, there are a number of &lt;a href="https://github.com/kubernetes/enhancements/blob/9d127347773ad19894ca488ee04f1cd3af5774fc/keps/sig-node/2400-node-swap/README.md#user-stories">use cases&lt;/a>
that would benefit from Kubernetes nodes supporting swap, including improved
node stability, better support for applications with high memory overhead but
smaller working sets, the use of memory-constrained devices, and memory
flexibility.&lt;/p>
&lt;p>Hence, over the past two releases, &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#readme">SIG Node&lt;/a> has
been working to gather appropriate use cases and feedback, and propose a design
for adding swap support to nodes in a controlled, predictable manner so that
Kubernetes users can perform testing and provide data to continue building
cluster capabilities on top of swap. The alpha graduation of swap memory
support for nodes is our first milestone towards this goal!&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>There are a number of possible ways that one could envision swap use on a node.
To keep the scope manageable for this initial implementation, when swap is
already provisioned and available on a node, &lt;a href="https://github.com/kubernetes/enhancements/blob/9d127347773ad19894ca488ee04f1cd3af5774fc/keps/sig-node/2400-node-swap/README.md#proposal">we have proposed&lt;/a>
the kubelet should be able to be configured such that:&lt;/p>
&lt;ul>
&lt;li>It can start with swap on.&lt;/li>
&lt;li>It will direct the Container Runtime Interface to allocate zero swap memory
to Kubernetes workloads by default.&lt;/li>
&lt;li>You can configure the kubelet to specify swap utilization for the entire
node.&lt;/li>
&lt;/ul>
&lt;p>Swap configuration on a node is exposed to a cluster admin via the
&lt;a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/">&lt;code>memorySwap&lt;/code> in the KubeletConfiguration&lt;/a>.
As a cluster administrator, you can specify the node's behaviour in the
presence of swap memory by setting &lt;code>memorySwap.swapBehavior&lt;/code>.&lt;/p>
&lt;p>This is possible through the addition of a &lt;code>memory_swap_limit_in_bytes&lt;/code> field
to the container runtime interface (CRI). The kubelet's config will control how
much swap memory the kubelet instructs the container runtime to allocate to
each container via the CRI. The container runtime will then write the swap
settings to the container level cgroup.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>On a node where swap memory is already provisioned, Kubernetes use of swap on a
node can be enabled by enabling the &lt;code>NodeSwap&lt;/code> feature gate on the kubelet, and
disabling the &lt;code>failSwapOn&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration">configuration setting&lt;/a>
or the &lt;code>--fail-swap-on&lt;/code> command line flag.&lt;/p>
&lt;p>You can also optionally configure &lt;code>memorySwap.swapBehavior&lt;/code> in order to
specify how a node will use swap memory. For example,&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">memorySwap&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">swapBehavior&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>LimitedSwap&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The available configuration options for &lt;code>swapBehavior&lt;/code> are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>LimitedSwap&lt;/code> (default): Kubernetes workloads are limited in how much swap
they can use. Workloads on the node not managed by Kubernetes can still swap.&lt;/li>
&lt;li>&lt;code>UnlimitedSwap&lt;/code>: Kubernetes workloads can use as much swap memory as they
request, up to the system limit.&lt;/li>
&lt;/ul>
&lt;p>If configuration for &lt;code>memorySwap&lt;/code> is not specified and the feature gate is
enabled, by default the kubelet will apply the same behaviour as the
&lt;code>LimitedSwap&lt;/code> setting.&lt;/p>
&lt;p>The behaviour of the &lt;code>LimitedSwap&lt;/code> setting depends if the node is running with
v1 or v2 of control groups (also known as &amp;quot;cgroups&amp;quot;):&lt;/p>
&lt;ul>
&lt;li>&lt;strong>cgroups v1:&lt;/strong> Kubernetes workloads can use any combination of memory and
swap, up to the pod's memory limit, if set.&lt;/li>
&lt;li>&lt;strong>cgroups v2:&lt;/strong> Kubernetes workloads cannot use swap memory.&lt;/li>
&lt;/ul>
&lt;h3 id="caveats">Caveats&lt;/h3>
&lt;p>Having swap available on a system reduces predictability. Swap's performance is
worse than regular memory, sometimes by many orders of magnitude, which can
cause unexpected performance regressions. Furthermore, swap changes a system's
behaviour under memory pressure, and applications cannot directly control what
portions of their memory usage are swapped out. Since enabling swap permits
greater memory usage for workloads in Kubernetes that cannot be predictably
accounted for, it also increases the risk of noisy neighbours and unexpected
packing configurations, as the scheduler cannot account for swap memory usage.&lt;/p>
&lt;p>The performance of a node with swap memory enabled depends on the underlying
physical storage. When swap memory is in use, performance will be significantly
worse in an I/O operations per second (IOPS) constrained environment, such as a
cloud VM with I/O throttling, when compared to faster storage mediums like
solid-state drives or NVMe.&lt;/p>
&lt;p>Hence, we do not recommend the use of swap for certain performance-constrained
workloads or environments. Cluster administrators and developers should
benchmark their nodes and applications before using swap in production
scenarios, and &lt;a href="#how-do-i-get-involved">we need your help&lt;/a> with that!&lt;/p>
&lt;h2 id="looking-ahead">Looking ahead&lt;/h2>
&lt;p>The Kubernetes 1.22 release introduces alpha support for swap memory on nodes,
and we will continue to work towards beta graduation in the 1.23 release. This
will include:&lt;/p>
&lt;ul>
&lt;li>Adding support for controlling swap consumption at the Pod level via cgroups.
&lt;ul>
&lt;li>This will include the ability to set a system-reserved quantity of swap
from what kubelet detects on the host.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Determining a set of metrics for node QoS in order to evaluate the
performance and stability of nodes with and without swap enabled.&lt;/li>
&lt;li>Collecting feedback from test user cases.
&lt;ul>
&lt;li>We will consider introducing new configuration modes for swap, such as a
node-wide swap limit for workloads.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>You can review the current &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory">documentation&lt;/a>
on the Kubernetes website.&lt;/p>
&lt;p>For more information, and to assist with testing and provide feedback, please
see &lt;a href="https://github.com/kubernetes/enhancements/issues/2400">KEP-2400&lt;/a> and its
&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md">design proposal&lt;/a>.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>Your feedback is always welcome! SIG Node &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#meetings">meets regularly&lt;/a>
and &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#contact">can be reached&lt;/a>
via &lt;a href="https://slack.k8s.io/">Slack&lt;/a> (channel &lt;strong>#sig-node&lt;/strong>), or the SIG's
&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-node">mailing list&lt;/a>.
Feel free to reach out to me, Elana Hashman (&lt;strong>@ehashman&lt;/strong> on Slack and GitHub)
if you'd like to help.&lt;/p></description></item><item><title>Blog: Kubernetes 1.22: Server Side Apply moves to GA</title><link>https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/</link><pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Jeffrey Ying, Google &amp;amp; Joe Betz, Google&lt;/p>
&lt;p>Server-side Apply (SSA) has been promoted to GA in the Kubernetes v1.22 release. The GA milestone means you can depend on the feature and its API, without fear of future backwards-incompatible changes. GA features are protected by the Kubernetes &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy&lt;/a>.&lt;/p>
&lt;h2 id="what-is-server-side-apply">What is Server-side Apply?&lt;/h2>
&lt;p>Server-side Apply helps users and controllers manage their resources through declarative configurations. Server-side Apply replaces the client side apply feature implemented by “kubectl apply” with a server-side implementation, permitting use by tools/clients other than kubectl. Server-side Apply is a new merging algorithm, as well as tracking of field ownership, running on the Kubernetes api-server. Server-side Apply enables new features like conflict detection, so the system knows when two actors are trying to edit the same field. Refer to the &lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Server-side Apply Documentation&lt;/a> and &lt;a href="https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/">Beta 2 release announcement&lt;/a> for more information.&lt;/p>
&lt;h2 id="what-s-new-since-beta">What’s new since Beta?&lt;/h2>
&lt;p>Since the &lt;a href="https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/">Beta 2 release&lt;/a> subresources support has been added, and both client-go and Kubebuilder have added comprehensive support for Server-side Apply. This completes the Server-side Apply functionality required to make controller development practical.&lt;/p>
&lt;h3 id="support-for-subresources">Support for subresources&lt;/h3>
&lt;p>Server-side Apply now fully supports subresources like &lt;code>status&lt;/code> and &lt;code>scale&lt;/code>. This is particularly important for &lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/">controllers&lt;/a>, which are often responsible for writing to subresources.&lt;/p>
&lt;h2 id="server-side-apply-support-in-client-go">Server-side Apply support in client-go&lt;/h2>
&lt;p>Previously, Server-side Apply could only be called from the client-go typed client using the &lt;code>Patch&lt;/code> function, with &lt;code>PatchType&lt;/code> set to &lt;code>ApplyPatchType&lt;/code>. Now, &lt;code>Apply&lt;/code> functions are included in the client to allow for a more direct and typesafe way of calling Server-side Apply. Each &lt;code>Apply&lt;/code> function takes an &amp;quot;apply configuration&amp;quot; type as an argument, which is a structured representation of an Apply request. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#666">...&lt;/span>
v1ac &lt;span style="color:#b44">&amp;#34;k8s.io/client-go/applyconfigurations/autoscaling/v1&amp;#34;&lt;/span>
)
hpaApplyConfig &lt;span style="color:#666">:=&lt;/span> v1ac.&lt;span style="color:#00a000">HorizontalPodAutoscaler&lt;/span>(autoscalerName, ns).
&lt;span style="color:#00a000">WithSpec&lt;/span>(v1ac.&lt;span style="color:#00a000">HorizontalPodAutoscalerSpec&lt;/span>().
&lt;span style="color:#00a000">WithMinReplicas&lt;/span>(&lt;span style="color:#666">0&lt;/span>)
)
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> hpav1client.&lt;span style="color:#00a000">Apply&lt;/span>(ctx, hpaApplyConfig, metav1.ApplyOptions{FieldManager: &lt;span style="color:#b44">&amp;#34;mycontroller&amp;#34;&lt;/span>, Force: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>})
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note in this example that &lt;code>HorizontalPodAutoscaler&lt;/code> is imported from an &amp;quot;applyconfigurations&amp;quot; package. Each &amp;quot;apply configuration&amp;quot; type represents the same Kubernetes object kind as the corresponding go struct, but where all fields are pointers to make them optional, allowing apply requests to be accurately represented. For example, when the apply configuration in the above example is marshalled to YAML, it produces:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>autoscaling/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HorizontalPodAutoscaler&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myHPA&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myNamespace&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">minReplicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To understand why this is needed, the above YAML cannot be produced by the v1.HorizontalPodAutoscaler go struct. Take for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">hpa &lt;span style="color:#666">:=&lt;/span> v1.HorizontalPodAutoscaler{
TypeMeta: metav1.TypeMeta{
APIVersion: &lt;span style="color:#b44">&amp;#34;autoscaling/v1&amp;#34;&lt;/span>,
Kind: &lt;span style="color:#b44">&amp;#34;HorizontalPodAutoscaler&amp;#34;&lt;/span>,
},
ObjectMeta: ObjectMeta{
Namespace: ns,
Name: autoscalerName,
},
Spec: v1.HorizontalPodAutoscalerSpec{
MinReplicas: pointer.&lt;span style="color:#00a000">Int32Ptr&lt;/span>(&lt;span style="color:#666">0&lt;/span>),
},
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The above code attempts to declare the same apply configuration as shown in the previous examples, but when marshalled to YAML, produces:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HorizontalPodAutoscaler&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>autoscaling/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>metadata&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myHPA&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myNamespace&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">creationTimestamp&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">null&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">scaleTargetRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">minReplicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxReplicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Which, among other things, contains &lt;code>spec.maxReplicas&lt;/code> set to &lt;code>0&lt;/code>. This is almost certainly not what the caller intended (the intended apply configuration says nothing about the &lt;code>maxReplicas&lt;/code> field), and could have serious consequences on a production system: it directs the autoscaler to downscale to zero pods. The problem here originates from the fact that the go structs contain required fields that are zero valued if not set explicitly. The go structs work as intended for create and update operations, but are fundamentally incompatible with apply, which is why we have introduced the generated &amp;quot;apply configuration&amp;quot; types.&lt;/p>
&lt;p>The &amp;quot;apply configurations&amp;quot; also have convenience &lt;code>With&amp;lt;FieldName&amp;gt;&lt;/code> functions that make it easier to build apply requests. This allows developers to set fields without having to deal with the fact that all the fields in the &amp;quot;apply configuration&amp;quot; types are pointers, and are inconvenient to set using go. For example &lt;code>MinReplicas: &amp;amp;0&lt;/code> is not legal go code, so without the &lt;code>With&lt;/code> functions, developers would work around this problem by using a library, e.g. &lt;code>MinReplicas: pointer.Int32Ptr(0)&lt;/code>, but string enumerations like &lt;code>corev1.Protocol&lt;/code> are still a problem since they cannot be supported by a general purpose library. In addition to the convenience, the &lt;code>With&lt;/code> functions also isolate developers from the underlying representation, which makes it safer for the underlying representation to be changed to support additional features in the future.&lt;/p>
&lt;h2 id="using-server-side-apply-in-a-controller">Using Server-side Apply in a controller&lt;/h2>
&lt;p>You can use the new support for Server-side Apply no matter how you implemented your controller. However, the new client-go support makes it easier to use Server-side Apply in controllers.&lt;/p>
&lt;p>When authoring new controllers to use Server-side Apply, a good approach is to have the controller recreate the apply configuration for an object each time it reconciles that object. This ensures that the controller fully reconciles all the fields that it is responsible for. Controllers typically should unconditionally set all the fields they own by setting &lt;code>Force: true&lt;/code> in the &lt;code>ApplyOptions&lt;/code>. Controllers must also provide a &lt;code>FieldManager&lt;/code> name that is unique to the reconciliation loop that apply is called from.&lt;/p>
&lt;p>When upgrading existing controllers to use Server-side Apply the same approach often works well--migrate the controllers to recreate the apply configuration each time it reconciles any object. Unfortunately, the controller might have multiple code paths that update different parts of an object depending on various conditions. Migrating a controller like this to Server-side Apply can be risky because if the controller forgets to include any fields in an apply configuration that is included in a previous apply request, a field can be accidently deleted. To ease this type of migration, client-go apply support provides a way to replace any controller reconciliation code that performs a &amp;quot;read/modify-in-place/update&amp;quot; (or patch) workflow with a &amp;quot;extract/modify-in-place/apply&amp;quot; workflow. Here's an example of the new workflow:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">fieldMgr &lt;span style="color:#666">:=&lt;/span> &lt;span style="color:#b44">&amp;#34;my-field-manager&amp;#34;&lt;/span>
deploymentClient &lt;span style="color:#666">:=&lt;/span> clientset.&lt;span style="color:#00a000">AppsV1&lt;/span>().&lt;span style="color:#00a000">Deployments&lt;/span>(&lt;span style="color:#b44">&amp;#34;default&amp;#34;&lt;/span>)
&lt;span style="color:#080;font-style:italic">// read, could also be read from a shared informer
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>deployment, err &lt;span style="color:#666">:=&lt;/span> deploymentClient.&lt;span style="color:#00a000">Get&lt;/span>(ctx, &lt;span style="color:#b44">&amp;#34;example-deployment&amp;#34;&lt;/span>, metav1.GetOptions{})
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#080;font-style:italic">// handle error
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>}
&lt;span style="color:#080;font-style:italic">// extract
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>deploymentApplyConfig, err &lt;span style="color:#666">:=&lt;/span> appsv1ac.&lt;span style="color:#00a000">ExtractDeployment&lt;/span>(deployment, fieldMgr)
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#080;font-style:italic">// handle error
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>}
&lt;span style="color:#080;font-style:italic">// modify-in-place
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>deploymentApplyConfig.Spec.Template.Spec.&lt;span style="color:#00a000">WithContainers&lt;/span>(corev1ac.&lt;span style="color:#00a000">Container&lt;/span>().
&lt;span style="color:#00a000">WithName&lt;/span>(&lt;span style="color:#b44">&amp;#34;modify-slice&amp;#34;&lt;/span>).
&lt;span style="color:#00a000">WithImage&lt;/span>(&lt;span style="color:#b44">&amp;#34;nginx:1.14.2&amp;#34;&lt;/span>),
)
&lt;span style="color:#080;font-style:italic">// apply
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>applied, err &lt;span style="color:#666">:=&lt;/span> deploymentClient.&lt;span style="color:#00a000">Apply&lt;/span>(ctx, deploymentApplyConfig, metav1.ApplyOptions{FieldManager: fieldMgr})
&lt;/code>&lt;/pre>&lt;/div>&lt;p>For developers using Custom Resource Definitions (CRDs), the Kubebuilder apply support will provide the same capabilities. Documentation will be included in the Kubebuilder book when available.&lt;/p>
&lt;h2 id="server-side-apply-and-customresourcedefinitions">Server-side Apply and CustomResourceDefinitions&lt;/h2>
&lt;p>It is strongly recommended that all &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definitions&lt;/a> (CRDs) have a schema. CRDs without a schema are treated as unstructured data by Server-side Apply. Keys are treated as fields in a struct and lists are assumed to be atomic.&lt;/p>
&lt;p>CRDs that specify a schema are able to specify additional annotations in the schema. Please refer to the documentation on the full list of available annotations.&lt;/p>
&lt;p>New annotations since beta:&lt;/p>
&lt;p>&lt;strong>Defaulting:&lt;/strong> Values for fields that appliers do not express explicit interest in should be defaulted. This prevents an applier from unintentionally owning a defaulted field that might cause conflicts with other appliers. If unspecified, the default value is nil or the nil equivalent for the corresponding type.&lt;/p>
&lt;ul>
&lt;li>Usage: see the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting">CRD Defaulting&lt;/a> documentation for more details.&lt;/li>
&lt;li>Golang: &lt;code>+default=&amp;lt;value&amp;gt;&lt;/code>&lt;/li>
&lt;li>OpenAPI extension: &lt;code>default: &amp;lt;value&amp;gt;&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Atomic for maps and structs:&lt;/p>
&lt;p>&lt;strong>Maps:&lt;/strong> By default maps are granular. A different manager is able to manage each map entry. They can also be configured to be atomic such that a single manager owns the entire map.&lt;/p>
&lt;ul>
&lt;li>Usage: Refer to &lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/#merge-strategy">Merge Strategy&lt;/a> for a more detailed overview&lt;/li>
&lt;li>Golang: &lt;code>+mapType=granular/atomic&lt;/code>&lt;/li>
&lt;li>OpenAPI extension: &lt;code>x-kubernetes-map-type: granular/atomic&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Structs:&lt;/strong> By default structs are granular and a separate applier may own each field. For certain kinds of structs, atomicity may be desired. This is most commonly seen in small coordinate-like structs such as Field/Object/Namespace Selectors, Object References, RGB values, Endpoints (Protocol/Port pairs), etc.&lt;/p>
&lt;ul>
&lt;li>Usage: Refer to &lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/#merge-strategy">Merge Strategy&lt;/a> for a more detailed overview&lt;/li>
&lt;li>Golang: &lt;code>+structType=granular/atomic&lt;/code>&lt;/li>
&lt;li>OpenAPI extension: &lt;code>x-kubernetes-map-type:atomic/granular&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-next">What's Next?&lt;/h2>
&lt;p>After Server Side Apply, the next focus for the API Expression working-group is around improving the expressiveness and size of the published Kubernetes API schema. To see the full list of items we are working on, please join our working group and refer to the work items document.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved?&lt;/h2>
&lt;p>The working-group for apply is &lt;a href="https://github.com/kubernetes/community/tree/master/wg-api-expression">wg-api-expression&lt;/a>. It is available on slack &lt;a href="https://kubernetes.slack.com/archives/C0123CNN8F3">#wg-api-expression&lt;/a>, through the &lt;a href="https://groups.google.com/g/kubernetes-wg-api-expression">mailing list&lt;/a> and we also meet every other Tuesday at 9.30 PT on Zoom.&lt;/p>
&lt;p>We would also like to use the opportunity to thank the hard work of all the contributors involved in making this promotion to GA possible:&lt;/p>
&lt;ul>
&lt;li>Andrea Nodari&lt;/li>
&lt;li>Antoine Pelisse&lt;/li>
&lt;li>Daniel Smith&lt;/li>
&lt;li>Jeffrey Ying&lt;/li>
&lt;li>Jenny Buckley&lt;/li>
&lt;li>Joe Betz&lt;/li>
&lt;li>Julian Modesto&lt;/li>
&lt;li>Kevin Delgado&lt;/li>
&lt;li>Kevin Wiesmüller&lt;/li>
&lt;li>Maria Ntalla&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.22: Reaching New Peaks</title><link>https://kubernetes.io/blog/2021/08/04/kubernetes-1-22-release-announcement/</link><pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/08/04/kubernetes-1-22-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.22/release-team.md">Kubernetes 1.22 Release Team&lt;/a>&lt;/p>
&lt;p>We’re pleased to announce the release of Kubernetes 1.22, the second release of 2021!&lt;/p>
&lt;p>This release consists of 53 enhancements: 13 enhancements have graduated to stable, 24 enhancements are moving to beta, and 16 enhancements are entering alpha. Also, three features have been deprecated.&lt;/p>
&lt;p>In April of this year, the Kubernetes release cadence was officially changed from four to three releases yearly. This is the first longer-cycle release related to that change. As the Kubernetes project matures, the number of enhancements per cycle grows. This means more work, from version to version, for the contributor community and Release Engineering team, and it can put pressure on the end-user community to stay up-to-date with releases containing increasingly more features.&lt;/p>
&lt;p>Changing the release cadence from four to three releases yearly balances many aspects of the project, both in how contributions and releases are managed, and also in the community's ability to plan for upgrades and stay up to date.&lt;/p>
&lt;p>You can read more in the official blog post &lt;a href="https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/">Kubernetes Release Cadence Change: Here’s What You Need To Know&lt;/a>.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="server-side-apply-graduates-to-ga">Server-side Apply graduates to GA&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Server-side Apply&lt;/a> is a new field ownership and object merge algorithm running on the Kubernetes API server. Server-side Apply helps users and controllers manage their resources via declarative configurations. It allows them to create and/or modify their objects declaratively, simply by sending their fully specified intent. After being in beta for a couple releases, Server-side Apply is now generally available.&lt;/p>
&lt;h3 id="external-credential-providers-now-stable">External credential providers now stable&lt;/h3>
&lt;p>Support for Kubernetes client &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins">credential plugins&lt;/a> has been in beta since 1.11, and with the release of Kubernetes 1.22 now graduates to stable. The GA feature set includes improved support for plugins that provide interactive login flows, as well as a number of bug fixes. Aspiring plugin authors can look at &lt;a href="https://github.com/ankeesler/sample-exec-plugin">sample-exec-plugin&lt;/a> to get started.&lt;/p>
&lt;h3 id="etcd-moves-to-3-5-0">etcd moves to 3.5.0&lt;/h3>
&lt;p>Kubernetes' default backend storage, etcd, has a new release: 3.5.0. The new release comes with improvements to the security, performance, monitoring, and developer experience. There are numerous bug fixes and some critical new features like the migration to structured logging and built-in log rotation. The release comes with a detailed future roadmap to implement a solution to traffic overload. You can read a full and detailed list of changes in the &lt;a href="https://etcd.io/blog/2021/announcing-etcd-3.5/">3.5.0 release announcement&lt;/a>.&lt;/p>
&lt;h3 id="quality-of-service-for-memory-resources">Quality of Service for memory resources&lt;/h3>
&lt;p>Originally, Kubernetes used the v1 cgroups API. With that design, the QoS class for a &lt;code>Pod&lt;/code> only applied to CPU resources (such as &lt;code>cpu_shares&lt;/code>). As an alpha feature, Kubernetes v1.22 can now use the cgroups v2 API to control memory allocation and isolation. This feature is designed to improve workload and node availability when there is contention for memory resources, and to improve the predictability of container lifecycle.&lt;/p>
&lt;h3 id="node-system-swap-support">Node system swap support&lt;/h3>
&lt;p>Every system administrator or Kubernetes user has been in the same boat regarding setting up and using Kubernetes: disable swap space. With the release of Kubernetes 1.22, alpha support is available to run nodes with swap memory. This change lets administrators opt in to configuring swap on Linux nodes, treating a portion of block storage as additional virtual memory.&lt;/p>
&lt;h3 id="windows-enhancements-and-capabilities">Windows enhancements and capabilities&lt;/h3>
&lt;p>Continuing to support the growing developer community, SIG Windows has released their &lt;a href="https://github.com/kubernetes-sigs/sig-windows-dev-tools/">Development Environment&lt;/a>. These new tools support multiple CNI providers and can run on multiple platforms. There is also a new way to run bleeding-edge Windows features from scratch by compiling the Windows kubelet and kube-proxy, then using them along with daily builds of other Kubernetes components.&lt;/p>
&lt;p>CSI support for Windows nodes moves to GA in the 1.22 release. In Kubernetes v1.22, Windows privileged containers are an alpha feature. To allow using CSI storage on Windows nodes, &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a> enables CSI node plugins to be deployed as unprivileged pods, using the proxy to perform privileged storage operations on the node.&lt;/p>
&lt;h3 id="default-profiles-for-seccomp">Default profiles for seccomp&lt;/h3>
&lt;p>An alpha feature for default seccomp profiles has been added to the kubelet, along with a new command line flag and configuration. When in use, this new feature provides cluster-wide seccomp defaults, using the &lt;code>RuntimeDefault&lt;/code> seccomp profile rather than &lt;code>Unconfined&lt;/code> by default. This enhances the default security of the Kubernetes Deployment. Security administrators will now sleep better knowing that workloads are more secure by default. To learn more about the feature, please refer to the official &lt;a href="https://kubernetes.io/docs/tutorials/clusters/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads">seccomp tutorial&lt;/a>.&lt;/p>
&lt;h3 id="more-secure-control-plane-with-kubeadm">More secure control plane with kubeadm&lt;/h3>
&lt;p>A new alpha feature allows running the &lt;code>kubeadm&lt;/code> control plane components as non-root users. This is a long requested security measure in &lt;code>kubeadm&lt;/code>. To try it you must enable the &lt;code>kubeadm&lt;/code> specific RootlessControlPlane feature gate. When you deploy a cluster using this alpha feature, your control plane runs with lower privileges.&lt;/p>
&lt;p>For &lt;code>kubeadm&lt;/code>, Kubernetes 1.22 also brings a new &lt;a href="https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta3/">v1beta3 configuration API&lt;/a>. This iteration adds some long requested features and deprecates some existing ones. The v1beta3 version is now the preferred API version; the v1beta2 API also remains available and is not yet deprecated.&lt;/p>
&lt;h2 id="major-changes">Major Changes&lt;/h2>
&lt;h3 id="removal-of-several-deprecated-beta-apis">Removal of several deprecated beta APIs&lt;/h3>
&lt;p>A number of deprecated beta APIs have been removed in 1.22 in favor of the GA version of those same APIs. All existing objects can be interacted with via stable APIs. This removal includes beta versions of the &lt;code>Ingress&lt;/code>, &lt;code>IngressClass&lt;/code>, &lt;code>Lease&lt;/code>, &lt;code>APIService&lt;/code>, &lt;code>ValidatingWebhookConfiguration&lt;/code>, &lt;code>MutatingWebhookConfiguration&lt;/code>, &lt;code>CustomResourceDefinition&lt;/code>, &lt;code>TokenReview&lt;/code>, &lt;code>SubjectAccessReview&lt;/code>, and &lt;code>CertificateSigningRequest&lt;/code> APIs.&lt;/p>
&lt;p>For the full list, check out the &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22">Deprecated API Migration Guide&lt;/a> as well as the blog post &lt;a href="https://blog.k8s.io/2021/07/14/upcoming-changes-in-kubernetes-1-22/">Kubernetes API and Feature Removals In 1.22: Here’s What You Need To Know&lt;/a>.&lt;/p>
&lt;h3 id="api-changes-and-improvements-for-ephemeral-containers">API changes and improvements for ephemeral containers&lt;/h3>
&lt;p>The API used to create &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">Ephemeral Containers&lt;/a> changes in 1.22. The Ephemeral Containers feature is alpha and disabled by default, and the new API does not work with clients that attempt to use the old API.&lt;/p>
&lt;p>For stable features, the kubectl tool follows the Kubernetes &lt;a href="https://kubernetes.io/releases/version-skew-policy/">version skew policy&lt;/a>; however, kubectl v1.21 and older do not support the new API for ephemeral containers. If you plan to use &lt;code>kubectl debug&lt;/code> to create ephemeral containers, and your cluster is running Kubernetes v1.22, you cannot do so with kubectl v1.21 or earlier. Please update kubectl to 1.22 if you wish to use &lt;code>kubectl debug&lt;/code> with a mix of cluster versions.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/542">Bound Service Account Token Volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2047">CSI Service Account Token&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1122">Windows Support for CSI Plugins&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1693">Warning mechanism for deprecated API use&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/85">PodDisruptionBudget Eviction&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="notable-feature-updates">Notable Feature Updates&lt;/h3>
&lt;ul>
&lt;li>A new &lt;a href="https://github.com/kubernetes/enhancements/issues/2579">PodSecurity admission&lt;/a> alpha feature is introduced, intended as a replacement for PodSecurityPolicy&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1769">The Memory Manager&lt;/a> moves to beta&lt;/li>
&lt;li>A new alpha feature to enable &lt;a href="https://github.com/kubernetes/enhancements/issues/647">API Server Tracing&lt;/a>&lt;/li>
&lt;li>A new v1beta3 version of the &lt;a href="https://github.com/kubernetes/enhancements/issues/970">kubeadm configuration&lt;/a> format&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1495">Generic data populators&lt;/a> for PersistentVolumes are now available in alpha&lt;/li>
&lt;li>The Kubernetes control plane will now always use the &lt;a href="https://github.com/kubernetes/enhancements/issues/19">CronJobs v2 controller&lt;/a>&lt;/li>
&lt;li>As an alpha feature, all Kubernetes node components (including the kubelet, kube-proxy, and container runtime) can be &lt;a href="https://github.com/kubernetes/enhancements/issues/2033">run as a non-root user&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="release-notes">Release notes&lt;/h1>
&lt;p>You can check out the full details of the 1.22 release in the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md">release notes&lt;/a>.&lt;/p>
&lt;h1 id="availability-of-release">Availability of release&lt;/h1>
&lt;p>Kubernetes 1.22 is &lt;a href="https://kubernetes.io/releases/download/">available for download&lt;/a> and also &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.22.0">on the GitHub project&lt;/a>.&lt;/p>
&lt;p>There are some great resources out there for getting started with Kubernetes. You can check out some &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> on the main Kubernetes site, or run a local cluster on your machine using Docker containers with &lt;a href="https://kind.sigs.k8s.io">kind&lt;/a>. If you’d like to try building a cluster from scratch, check out the &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes the Hard Way&lt;/a> tutorial by Kelsey Hightower.&lt;/p>
&lt;h1 id="release-team">Release Team&lt;/h1>
&lt;p>This release was made possible by a very dedicated group of individuals, who came together as a team to deliver technical content, documentation, code, and a host of other components that go into every Kubernetes release.&lt;/p>
&lt;p>A huge thank you to the release lead Savitha Raghunathan for leading us through a successful release cycle, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.22 release for the community.&lt;/p>
&lt;p>We would also like to take this opportunity to remember Peeyush Gupta, a member of our team that we lost earlier this year. Peeyush was actively involved in SIG ContribEx and the Kubernetes Release Team, most recently serving as the 1.22 Communications lead. His contributions and efforts will continue to reflect in the community he helped build. A &lt;a href="https://github.com/cncf/memorials/blob/main/peeyush-gupta.md">CNCF memorial&lt;/a> page has been created where thoughts and memories can be shared by the community.&lt;/p>
&lt;h1 id="release-logo">Release Logo&lt;/h1>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-08-04-kubernetes-release-1.22/kubernetes-1.22.png" alt="Kubernetes 1.22 Release Logo">&lt;/p>
&lt;p>Amidst the ongoing pandemic, natural disasters, and ever-present shadow of burnout, the 1.22 release of Kubernetes includes 53 enhancements. This makes it the largest release to date. This accomplishment was only made possible due to the hard-working and passionate Release Team members and the amazing contributors of the Kubernetes ecosystem. The release logo is our reminder to keep reaching for new milestones and setting new records. And it is dedicated to all the Release Team members, hikers, and stargazers!&lt;/p>
&lt;p>The logo is designed by &lt;a href="https://www.instagram.com/boris.z.man/">Boris Zotkin&lt;/a>. Boris is a Mac/Linux Administrator at the MathWorks. He enjoys simple things in life and loves spending time with his family. This tech-savvy individual is always up for a challenge and happy to help a friend!&lt;/p>
&lt;h1 id="user-highlights">User Highlights&lt;/h1>
&lt;ul>
&lt;li>In May, the CNCF welcomed 27 new organizations across the globe as members of the diverse cloud native ecosystem. These new &lt;a href="https://www.cncf.io/announcements/2021/05/05/27-new-members-join-the-cloud-native-computing-foundation/">members&lt;/a> will participate in CNCF events, including the upcoming &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/">KubeCon + CloudNativeCon NA in Los Angeles&lt;/a> from October 12 – 15, 2021.&lt;/li>
&lt;li>The CNCF granted Spotify the &lt;a href="https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/">Top End User Award&lt;/a> during &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">KubeCon + CloudNativeCon EU – Virtual 2021&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h1 id="project-velocity">Project Velocity&lt;/h1>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/">CNCF K8s DevStats project&lt;/a> aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p>
&lt;p>In the v1.22 release cycle, which ran for 15 weeks (April 26 to August 4), we saw contributions from &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.21.0%20-%20now&amp;amp;var-metric=contributions">1063 companies&lt;/a> and &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.21.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All">2054 individuals&lt;/a>.&lt;/p>
&lt;h1 id="ecosystem-updates">Ecosystem Updates&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">KubeCon + CloudNativeCon Europe 2021&lt;/a> was held in May, the third such event to be virtual. All talks are &lt;a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2MqBm19mRz9SYLsw4kfQBrC">now available on-demand&lt;/a> for anyone that would like to catch up!&lt;/li>
&lt;li>&lt;a href="https://www.cncf.io/blog/2021/07/13/spring-term-lfx-program-largest-graduating-class-with-28-successful-cncf-interns">Spring Term LFX Program&lt;/a> had the largest graduating class with 28 successful CNCF interns!&lt;/li>
&lt;li>CNCF launched &lt;a href="https://www.cncf.io/blog/2021/06/03/cloud-native-community-goes-live-with-10-shows-on-twitch/">livestreaming on Twitch&lt;/a> at the beginning of the year targeting definitive interactive media experience for anyone wanting to learn, grow, and collaborate with others in the Cloud Native community from anywhere in the world.&lt;/li>
&lt;/ul>
&lt;h1 id="event-updates">Event Updates&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/">KubeCon + CloudNativeCon North America 2021&lt;/a> will take place in Los Angeles, October 12 – 15, 2021! You can find more information about the conference and registration on the event site.&lt;/li>
&lt;li>&lt;a href="https://community.cncf.io/kubernetes-community-days/about-kcd/">Kubernetes Community Days&lt;/a> has upcoming events scheduled in Italy, the UK, and in Washington DC.&lt;/li>
&lt;/ul>
&lt;h1 id="upcoming-release-webinar">Upcoming release webinar&lt;/h1>
&lt;p>Join members of the Kubernetes 1.22 release team on October 5, 2021 to learn about the major features of this release, as well as deprecations and removals to help plan for upgrades. For more information and registration, visit the &lt;a href="https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-122-release/">event page&lt;/a> on the CNCF Online Programs site.&lt;/p>
&lt;h1 id="get-involved">Get Involved&lt;/h1>
&lt;p>If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels:&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributors&lt;/a> website.&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Roorkee robots, releases and racing: the Kubernetes 1.21 release interview</title><link>https://kubernetes.io/blog/2021/07/29/roorkee-robots-releases-and-racing-the-kubernetes-1.21-release-interview/</link><pubDate>Thu, 29 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/29/roorkee-robots-releases-and-racing-the-kubernetes-1.21-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>With Kubernetes 1.22 due out next week, now is a great time to look back on 1.21. The release team for that version was led by &lt;a href="https://twitter.com/theonlynabarun">Nabarun Pal&lt;/a> from VMware.&lt;/p>
&lt;p>Back in April I &lt;a href="https://kubernetespodcast.com/episode/146-kubernetes-1.21/">interviewed Nabarun&lt;/a> on the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a>; the latest in a series of release lead conversations that started back with 1.11, not long after the show started back in 2018.&lt;/p>
&lt;p>In these interviews we learn a little about the release, but also about the process behind it, and the story behind the person chosen to lead it. Getting to know a community member is my favourite part of the show each week, and so I encourage you to &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe wherever you get your podcasts&lt;/a>. With a release coming next week, you can probably guess what our next topic will be!&lt;/p>
&lt;p>&lt;em>This transcript has been edited and condensed for clarity.&lt;/em>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>CRAIG BOX: You have a Bachelor of Technology in Metallurgical and Materials Engineering. How are we doing at turning lead into gold?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Well, last I checked, we have yet to find the philosopher's stone!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One of the more important parts of the process?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: We're not doing that well in terms of getting alchemists up and running. There is some improvement in nuclear technology, where you can turn lead into gold, but I would guess buying gold would be much more efficient.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Or Bitcoin? It depends what you want to do with the gold.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yeah, seeing the increasing prices of Bitcoin, you'd probably prefer to bet on that. But, don't take this as a suggestion. I'm not a registered investment advisor, and I don't give investment advice!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: But you are, of course, a trained materials engineer. How did you get into that line of education?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: We had a graded and equated exam structure, where you sit a single exam, and then based on your performance in that exam, you can try any of the universities which take those scores into account. I went to the Indian Institute of Technology, Roorkee.&lt;/p>
&lt;p>Materials engineering interested me a lot. I had a passion for computer science since childhood, but I also liked material science, so I wanted to explore that field. I did a lot of exploration around material science and metallurgy in my freshman and sophomore years, but then computing, since it was a passion, crept into the picture.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's dig in there a little bit. What did computing look like during your childhood?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It was a very interesting journey. I started exploring computers back when I was seven or eight. For my first programming language, if you call it a programming language, I explored LOGO.&lt;/p>
&lt;p>You have a turtle on the screen, and you issue commands to it, like move forward or rotate or pen up or pen down. You basically draw geometric figures. I could visually see how I could draw a square and how I could draw a triangle. It was an interesting journey after that. I learned BASIC, then went to some amount of HTML, JavaScript.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's interesting to me because Logo and BASIC were probably my first two programming languages, but I think there was probably quite a gap in terms of when HTML became a thing after those two! Did your love of computing always lead you down the path towards programming, or were you interested as a child in using computers for games or application software? What led you specifically into programming?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Programming came in late. Not just in computing, but in life, I'm curious with things. When my parents got me my first computer, I was curious. I was like, &amp;quot;how does this operating system work?&amp;quot; What even is running it? Using a television and using a computer is a different experience, but usability is kind of the same thing. The HCI device for a television is a remote, whereas with a computer, I had a keyboard and a mouse. I used to tinker with the box and reinstall operating systems.&lt;/p>
&lt;p>We used to get magazines back then. They used to bundle OpenSuse or Debian, and I used to install them. It was an interesting experience, 15 years back, how Linux used to be. I have been a tinkerer all around, and that's what eventually led me to programming.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: With an interest in both the physical and ethereal aspects of technology, you did a lot of robotics challenges during university. That's something that I am not surprised to hear from someone who has a background in Logo, to be honest. There's Mindstorms, and a lot of other technology that is based around robotics that a lot of LOGO people got into. How was that something that came about for you?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: When I joined my university, apart from studying materials, one of the things they used to really encourage was to get involved in a lot of extracurricular activities. One which interested me was robotics. I joined &lt;a href="https://github.com/marsiitr">my college robotics team&lt;/a> and participated in a lot of challenges.&lt;/p>
&lt;p>Predominantly, we used to participate in this competition called &lt;a href="https://en.wikipedia.org/wiki/ABU_Robocon">ABU Robocon&lt;/a>, which is an event conducted by the Asia-Pacific Broadcasting Union. What they used to do was, every year, one of the participating countries in the contest would provide a problem statement. For example, one year, they asked us to build a badminton-playing robot. They asked us to build a rugby playing robot or a Frisbee thrower, and there are some interesting problem statements around the challenge: you can't do this. You can't do that. Weight has to be like this. Dimensions have to be like that.&lt;/p>
&lt;p>I got involved in that, and most of my time at university, I used to spend there. Material science became kind of a backburner for me, and my hobby became my full time thing.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And you were not only involved there in terms of the project and contributions to it, but you got involved as a secretary of the team, effectively, doing a lot of the organization, which is a thread that will come up as we speak about Kubernetes.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Over the course of time, when I gained more knowledge into how the team works, it became very natural that I graduated up the ladder and then managed juniors. I became the joint secretary of the robotics club in our college. This was more of a broad, engaging role in evangelizing robotics at the university, to promote events, to help students to see the value in learning robotics - what you gain out of that mechanically or electronically, or how do you develop your logic by programming robots.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Your first job after graduation was working at a company called Algoshelf, but you were also an intern there while you were at school?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Algoshelf was known as Rorodata when I joined them as an intern. This was also an interesting opportunity for me in the sense that I was always interested in writing programs which people would use. One of the things that I did there was build an open source Function as a Service framework, if I may call it that - it was mostly turning Python functions into web servers without even writing any code. The interesting bit there was that it was targeted toward data scientists, and not towards programmers. We had to understand the pain of data scientists, that they had to learn a lot of programming in order to even deploy their machine learning models, and we wanted to solve that problem.&lt;/p>
&lt;p>They offered me a job after my internship, and I kept on working for them after I graduated from university. There, I got introduced to Kubernetes, so we pivoted into a product structure where the very same thing I told you, the Functions as a Service thing, could be deployed in Kubernetes. I was exploring Kubernetes to use it as a scalable platform. Instead of managing pets, we wanted to manage cattle, as in, we wanted to have a very highly distributed architecture.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Not actual cattle. I've been to India. There are a lot of cows around.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yeah, not actual cattle. That is a bit tough.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: When Algoshelf we're looking at picking up Kubernetes, what was the evaluation process like? Were you looking at other tools at the time? Or had enough time passed that Kubernetes was clearly the platform that everyone was going to use?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Algoshelf was a natural evolution. Before Kubernetes, we used to deploy everything on a single big AWS server, using systemd. Everything was a systemd service, and everything was deployed using Fabric. Fabric is a Python package which essentially is like Ansible, but much leaner, as it does not have all the shims and things that Ansible has.&lt;/p>
&lt;p>Then we asked &amp;quot;what if we need to scale out to different machines?&amp;quot; Kubernetes was in the hype. We hopped onto the hype train to see whether Kubernetes was worth it for us. And that's where my journey started, exploring the ecosystem, exploring the community. How can we improve the community in essence?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: A couple of times now you've mentioned as you've grown in a role, becoming part of the organization and the arranging of the group. You've talked about working in Python. You had submitted some talks to Pycon India. And I understand you're now a tech lead for that conference. What does the tech community look like in India and how do you describe your involvement in it?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: My involvement with the community began when I was at university. When I was working as an intern at Algoshelf, I was introduced to this-- I never knew about PyCon India, or tech conferences in general.&lt;/p>
&lt;p>The person that I was working with just asked me, like hey, did you submit a talk to PyCon India? It's very useful, the library that we were making. So I &lt;a href="https://www.nabarun.in/talk/2017/pyconindia/#1">submitted a talk&lt;/a> to PyCon India in 2017. Eventually the talk got selected. That was not my first speaking opportunity, it was my second. I also spoke at PyData Delhi on a similar thing that I worked on in my internship.&lt;/p>
&lt;p>It has been a journey since then. I talked about the same thing at FOSSASIA Summit in Singapore, and got really involved with the Python community because it was what I used to work on back then.&lt;/p>
&lt;p>After giving all those talks at conferences, I got also introduced to this amazing group called &lt;a href="https://dgplug.org/">dgplug&lt;/a>, which is an acronym for the Durgapur Linux Users Group. It is a group started in-- I don't remember the exact year, but it was around 12 to 13 years back, by someone called Kushal Das, with the ideology of &lt;a href="https://foss.training/">training students into being better open source contributors&lt;/a>.&lt;/p>
&lt;p>I liked the idea and got involved with in teaching last year. It is not limited to students. Professionals can also join in. It's about making anyone better at upstream contributions, making things sustainable. I started training people on Vim, on how to use text editors. so they are more efficient and productive. In general life, text editors are a really good tool.&lt;/p>
&lt;p>The other thing was the shell. How do you navigate around the Linux shell and command line? That has been a fun experience.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's very interesting to think about that, because my own involvement with a Linux User Group was probably around the year 2000. And back then we were teaching people how to install things-- Linux on CD was kinda new at that point in time. There was a lot more of, what is this new thing and how do we get involved? When the internet took off around that time, all of that stuff moved online - you no longer needed to go meet a group of people in a room to talk about Linux. And I haven't really given much thought to the concept of a LUG since then, but it's great to see it having turned into something that's now about contributing, rather than just about how you get things going for yourself.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Exactly. So as I mentioned earlier, my journey into Linux was installing SUSE from DVDs that came bundled with magazines. Back then it was a pain installing things because you did not get any instructions. There has certainly been a paradigm shift now. People are more open to reading instructions online, downloading ISOs, and then just installing them. So we really don't need to do that as part of LUGs.&lt;/p>
&lt;p>We have shifted more towards enabling people to contribute to whichever project that they use. For example, if you're using Fedora, contribute to Fedora; make things better. It's just about giving back to the community in any way possible.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You're also involved in the &lt;a href="https://www.meetup.com/Bangalore-Kubernetes-Meetup/">Kubernetes Bangalore meetup group&lt;/a>. Does that group have a similar mentality?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: The Kubernetes Bangalore meetup group is essentially focused towards spreading the knowledge of Kubernetes and the aligned products in the ecosystem, whatever there is in the Cloud Native Landscape, in various ways. For example, to evangelize about using them in your company or how people use them in existing ways.&lt;/p>
&lt;p>So a few months back in February, we did something like a &lt;a href="https://www.youtube.com/watch?v=FgsXbHBRYIc">Kubernetes contributor workshop&lt;/a>. It was one of its kind in India. It was the first one if I recall correctly. We got a lot of traction and community members interested in contributing to Kubernetes and a lot of other projects. And this is becoming a really valuable thing.&lt;/p>
&lt;p>I'm not much involved in the organization of the group. There are really great people already organizing it. I keep on being around and attending the meetups and trying to answer any questions if people have any.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One way that it is possible to contribute to the Kubernetes ecosystem is through the release process. You've &lt;a href="https://blog.naba.run/posts/release-enhancements-journey/">written a blog&lt;/a> which talks about your journey through that. It started in Kubernetes 1.17, where you took a shadow role for that release. Tell me about what it was like to first take that plunge.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Taking the plunge was a big step, I would say. It should not have been that way. After getting into the team, I saw that it is really encouraged that you should just apply to the team - but then write truthfully about yourself. What do you want? Write your passionate goal, why you want to be in the team.&lt;/p>
&lt;p>So even right now the shadow applications are open for the next release. I wanted to give that a small shoutout. If you want to contribute to the Kubernetes release team, please do apply. The form is pretty simple. You just need to say why do you want to contribute to the release team.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What was your answer to that question?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It was a bit tricky. I have this philosophy of contributing to projects that I use in my day-to-day life. I use a lot of open source projects daily, and I started contributing to Kubernetes primarily because I was using the Kubernetes Python client. That was one of my first contributions.&lt;/p>
&lt;p>When I was contributing to that, I explored the release team and it interested me a lot, particularly how interesting and varied the mechanics of releasing Kubernetes are. For most software projects, it's usually whenever you decide that you have made meaningful progress in terms of features, you release it. But Kubernetes is not like that. We follow a regular release cadence. And all those aspects really interested me. I actually applied for the first time in Kubernetes 1.16, but got rejected.&lt;/p>
&lt;p>But I still applied to Kubernetes 1.17, and I got into the enhancements team. That team was led by &lt;a href="https://kubernetespodcast.com/episode/126-research-steering-honking/">MrBobbyTables, Bob Killen&lt;/a>, back then, and &lt;a href="https://kubernetespodcast.com/episode/131-kubernetes-1.20/">Jeremy Rickard&lt;/a> was one of my co-shadows in the team. I shadowed enhancements again. Then I lead enhancements in 1.19. I then shadowed the lead in 1.20 and eventually led the 1.21 team. That's what my journey has been.&lt;/p>
&lt;p>My suggestion to people is don't be afraid of failure. Even if you don't get selected, it's perfectly fine. You can still contribute to the release team. Just hop on the release calls, raise your hand, and introduce yourself.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Between the 1.20 and 1.21 releases, you moved to work on the upstream contribution team at VMware. I've noticed that VMware is hiring a lot of great upstream contributors at the moment. Is this something that &lt;a href="https://kubernetespodcast.com/episode/130-kubecon-na-2020/">Stephen Augustus&lt;/a> had his fingerprints all over? Is there something in the water?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: A lot of people have fingerprints on this process. Stephen certainly had his fingerprints on it, I would say. We are expanding the team of upstream contributors primarily because the product that we are working for is based on Kubernetes. It helps us a lot in driving processes upstream and helping out the community as a whole, because everyone then gets enabled and benefits from what we contribute to the community.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I understand that the Tanzu team is being built out in India at the moment, but I guess you probably haven't been able to meet them in person yet?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yes and no. I did not meet any of them after joining VMware, but I met a lot of my teammates, before I joined VMware, at KubeCons. For example, I met Nikhita, I met Dims, I met Stephen at KubeCon. I am yet to meet other members of the team and I'm really excited to catch up with them once everything comes out of lockdown and we go back to our normal lives.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yes, everyone that I speak to who has changed jobs in the pandemic says it's a very odd experience, just nothing really being different. And the same perhaps for people who are working on open source moving companies as well. They're doing the same thing, perhaps just for a different employer.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: As we say in the community, see you in another Slack in some time.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We now turn to the recent release of Kubernetes 1.21. First of all, congratulations on that.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Thank you.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: &lt;a href="https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/">The announcement&lt;/a> says the release consists of 51 enhancements, 13 graduating to stable, 16 moving to beta, 20 entering alpha, and then two features that have been deprecated. How would you summarize this release?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: One of the big points for this release is that it is the largest release of all time.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Really?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yep. 1.20 was the largest release back then, but 1.21 got more enhancements, primarily due to a lot of changes that we did to the process.&lt;/p>
&lt;p>In the 1.21 release cycle, we did a few things differently compared to other release cycles-- for example, in the enhancement process. An enhancement, in the Kubernetes context, is basically a feature proposal. You will hear the terminology &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/README.md">Kubernetes Enhancement Proposals&lt;/a>, or KEP, a lot in the community. An enhancement is a broad thing encapsulated in a specific document.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I like to think of it as a thing that's worth having a heading in the release notes.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Indeed. Until the 1.20 release cycle, what we used to do was-- the release team has a vertical called enhancements. The enhancements team members used to ping each of the enhancement issues and ask whether they want to be part of the release cycle or not. The authors would decide, or talk to their SIG, and then come back with the answer, as to whether they wanted to be part of the cycle.&lt;/p>
&lt;p>In this release, what we did was we eliminated that process and asked the SIGs proactively to discuss amongst themselves, what they wanted to pitch in for this release cycle. What set of features did they want to graduate this release? They may introduce things in alpha, graduate things to beta or stable, or they may also deprecate features.&lt;/p>
&lt;p>What this did was promote a lot of async processes, and at the same time, give power back to the community. The community decides what they want in the release and then comes back collectively. It also reduces a lot of stress on the release team who previously had to ask people consistently what they wanted to pitch in for the release. You now have a deadline. You discuss amongst your SIG what your roadmap is and what it looks like for the near future. Maybe this release, and the next two. And you put all of those answers into a Google spreadsheet. Spreadsheets are still a thing.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The Kubernetes ecosystem runs entirely on Google Spreadsheets.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It does, and a lot of Google Docs for meeting notes! We did a lot of process improvements, which essentially led to a better release. This release cycle we had 13 enhancements graduating to stable, 16 which moved to beta, and 20 enhancements which were net new features into the ecosystem, and came in as alpha.&lt;/p>
&lt;p>Along with that are features set for deprecation. One of them was PodSecurityPolicy. That has been a point of discussion in the Kubernetes user base and we also published &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">a blog post about it&lt;/a>. All credit to SIG Security who have been on top of things as to find a replacement for PodSecurityPolicy even before this release cycle ended, so that they could at least have a proposal of what will happen next.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's talk about some old things and some new things. You mentioned PodSecurityPolicy there. That's a thing that's been around a long time and is being deprecated. Two things that have been around a long time and that are now being promoted to stable are CronJobs and PodDisruptionBudgets, both of which were introduced in Kubernetes 1.4, which came out in 2016. Why do you think it took so long for them both to go stable?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: I might not have a definitive answer to your question. One of the things that I feel is they might be already so good that nobody saw that they were beta features, and just kept on using them.&lt;/p>
&lt;p>One of the things that I noticed when reading for the CronJobs graduation from beta to stable was the new controller. Users might not see this, but there has been a drastic change in the CronJob controller v2. What it essentially does is goes from a poll-based method of checking what users have defined as CronJobs to a queue architecture, which is the modern method of defining controllers. That has been one of the really good improvements in the case of CronJobs. Instead of the controller working in O(N) time, you now have constant time complexity.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: A lot of these features that have been in beta for a long time, like you say, people have an expectation that they are complete. With PodSecurityPolicy, it's being deprecated, which is allowed because it's a feature that never made it out of beta. But how do you think people will react to it going away? And does that say something about the need for the process to make sure that features don't just languish in beta forever, which has been introduced recently?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: That's true. One of the driving factors, when contributors are thinking of graduating beta features has been the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-architecture/1635-prevent-permabeta/README.md">&amp;quot;prevention of perma-beta&amp;quot; KEP&lt;/a>. Back in 1.19 we &lt;a href="https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/">introduced this process&lt;/a> where each of the beta resources were marked for deprecation and removal in a certain time frame-- three releases for deprecation and another release for removal. That's also a motivating factor for eventually rethinking as to how beta resources work for us in the community. That is also very effective, I would say.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do remember that Gmail was in beta for eight years.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: I did not know that!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Nothing in Kubernetes is quite that old yet, but we'll get there. Of the 20 new enhancements, do you have a favorite or any that you'd like to call out?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: There are two specific features in 1.21 that I'm really interested in, and are coming as net new features. One of them is the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor">persistent volume health monitor&lt;/a>, which gives the users the capability to actually see whether the backing volumes, which power persistent volumes in Kubernetes, are deleted or not. For example, the volumes may get deleted due to an inadvertent event, or they may get corrupted. That information is basically surfaced out as a field so that the user can leverage it in any way.&lt;/p>
&lt;p>The other feature is the proposal for &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cli/859-kubectl-headers">adding headers with the command name to kubectl requests&lt;/a>. We have always set the user-agent information when doing those kind of requests, but the proposal is to add what command the user put in so that we can enable more telemetry, and cluster administrators can determine the usage patterns of how people are using the cluster. I'm really excited about these kind of features coming into play.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You're the first release lead from the Asia-Pacific region, or more accurately, outside of the US and Europe. Most meetings in the Kubernetes ecosystem are traditionally in the window of overlap between the US and Europe, in the morning in California and the evening here in the UK. What's it been like to work outside of the time zones that the community had previously been operating in?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It has been a fun and a challenging proposition, I would say. In the last two-ish years that I have been contributing to Kubernetes, the community has also transformed from a lot of early morning Pacific calls to more towards async processes. For example, we in the release team have transformed our processes so we don't do updates in the calls anymore. What we do is ask for updates ahead of time, and then in the call, we just discuss things which need to be discussed synchronously in the team.&lt;/p>
&lt;p>We leverage the meetings right now more for discussions. But we also don't come to decisions in those discussions, because if any stakeholder is not present on the call, it puts them at a disadvantage. We are trying to talk more on Slack, publicly, or talk on mailing lists. That's where most of the discussion should happen, and also to gain lazy consensus. What I mean by lazy consensus is come up with a pre-decision kind of thing, but then also invite feedback from the broader community about what people would like them to see about that specific thing being discussed. This is where we as a community are also transforming a lot, but there is a lot more headroom to grow.&lt;/p>
&lt;p>The release team also started to have EU/APAC burndown meetings. In addition to having one meeting focused towards the US and European time zones, we also do a meeting which is more suited towards European and Asia-Pacific time zones. One of the driving factors for those decisions was that the release team is seeing a lot of participation from a variety of time zones. To give you one metric, we had release team members this cycle from UTC+8 all through UTC-8 - 16 hours of span. It's really difficult to accommodate all of those zones in a single meeting. And it's not just those 16 hours of span - what about the other eight hours?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yeah, you're missing New Zealand. You could add another 5 hours of span right there.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Exactly. So we will always miss people in meetings, and that's why we should also innovate more, have different kinds of meetings. But that also may not be very sustainable in the future. Will people attend duplicate meetings? Will people follow both of the meetings? More meetings is one of the solutions.&lt;/p>
&lt;p>The other solution is you have threaded discussions on some medium, be it Slack or be it a mailing list. Then, people can just pitch in whenever it is work time for them. Then, at the end of the day, a 24-hour rolling period, you digest it, and then push it out as meeting notes. That's what the Contributor Experience Special Interest Group is doing - shout-out to them for moving to that process. I may be wrong here, but I think once every two weeks, they do async updates on Slack. And that is a really nice thing to have, improving variety of geographies that people can contribute from.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Once you've put everything together that you hope to be in your release, you create a release candidate build. How do you motivate people to test those?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: That's a very interesting question. It is difficult for us to motivate people into trying out these candidates. It's mostly people who are passionate about Kubernetes who try out the release candidates and see for themselves what the bugs are. I remember &lt;a href="https://twitter.com/dims/status/1377272238420934656">Dims tweeting out a call&lt;/a> that if somebody tries out the release candidate and finds a good bug or caveat, they could get a callout in the KubeCon keynote. That's one of the incentives - if you want to be called out in a KubeCon keynote, please try our release candidates.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Or get a new pair of Kubernetes socks?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: We would love to give out goodies to people who try out our release candidates and find bugs. For example, if you want the brand new release team logo as a sticker, just hit me up. If you find a bug in a 1.22 release candidate, I would love to be able to send you some coupon codes for the store. Don't quote me on this, but do reach out.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now the release is out, is it time for you to put your feet up? What more things do you have to do, and how do you feel about the path ahead for yourself?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: I was discussing this with the team yesterday. Even after the release, we had kind of a water-cooler conversation. I just pasted in a Zoom link to all the release team members and said, hey, do you want to chat? One of the things that I realized that I'm really missing is the daily burndowns right now. I will be around in the release team and the SIG Release meetings, helping out the new lead in transitioning. And even my job, right now, is not over. I'm working with Taylor, who is the emeritus advisor for 1.21, on figuring out some of the mechanics for the next release cycle. I'm also documenting what all we did as part of the process and as part of the process changes, and making sure the next release cycle is up and running.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We've done a lot of these release lead interviews now, and there's a question which we always like to ask, which is, what will you write down in the transition envelope? Savitha Raghunathan is the release lead for 1.22. What is the advice that you will pass on to her?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Three words-- &lt;strong>Do, Delegate, and Defer&lt;/strong>. Categorize things into those three buckets as to what you should do right away, what you need to defer, and things that you can delegate to your shadows or other release team members. That's one of the mantras that works really well when leading a team. It is not just in the context of the release team, but it's in the context of managing any team.&lt;/p>
&lt;p>The other bit is &lt;strong>over-communicate&lt;/strong>. No amount of communication is enough. What I've realized is the community is always willing to help you. One of the big examples that I can give is the day before release was supposed to happen, we were seeing a lot of test failures, and then one of the community members had an idea-- why don't you just send an email? I was like, &amp;quot;that sounds good. We can send an email mentioning all the flakes and call out for help to the broader Kubernetes developer community.&amp;quot; And eventually, once we sent out the email, lots of people came in to help us in de-flaking the tests and trying to find out the root cause as to why those tests were failing so often. Big shout out to Antonio and all the SIG Network folks who came to pitch in.&lt;/p>
&lt;p>No matter how many names I mention, it will never be enough. A lot of people, even outside the release team, have helped us a lot with this release. And that's where the release theme comes in - &lt;strong>Power to the Community&lt;/strong>. I'm really stoked by how this community behaves and how people are willing to help you all the time. It's not about what they're telling you to do, but it's what they're also interested in, they're passionate about.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One of the things you're passionate about is Formula One. Do you think Lewis Hamilton is going to take it away this year?&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: It's a fair probability that Lewis will win the title this year as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Which would take him to eight all time career wins. And thus-- &lt;a href="https://www.nytimes.com/2020/11/15/sports/autoracing/lewis-hamilton-schumacher-formula-one-record.html">he's currently tied with Michael Schumacher&lt;/a>-- would pull him ahead.&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: Yes. Michael Schumacher was my first favorite F1 driver, I would say. It feels a bit heartbreaking to see someone break Michael's record.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: How do you feel about &lt;a href="https://www.formula1.com/en/latest/article.breaking-mick-schumacher-to-race-for-haas-in-2021-as-famous-surname-returns.66XTVfSt80GrZe91lvWVwJ.html">Michael Schumacher's son joining the contest?&lt;/a>&lt;/strong>&lt;/p>
&lt;p>NABARUN PAL: I feel good. Mick Schumacher is in the fray right now. And I wish we could see him, in a few years, in a Ferrari. The Schumacher family back to Ferrari would be really great to see. But then, my fan favorite has always been McLaren, partly because I like the chemistry of Lando and Carlos over the last two years. It was heartbreaking to see Carlos go to Ferrari. But then we have Lando and Daniel Ricciardo in the team. They're also fun people.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/theonlynabarun">Nabarun Pal&lt;/a> is on the Tanzu team at VMware and served as the Kubernetes 1.21 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Updating NGINX-Ingress to use the stable Ingress API</title><link>https://kubernetes.io/blog/2021/07/26/update-with-ingress-nginx/</link><pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/26/update-with-ingress-nginx/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> James Strong, Ricardo Katz&lt;/p>
&lt;p>With all Kubernetes APIs, there is a process to creating, maintaining, and
ultimately deprecating them once they become GA. The networking.k8s.io API group is no
different. The upcoming Kubernetes 1.22 release will remove several deprecated APIs
that are relevant to networking:&lt;/p>
&lt;ul>
&lt;li>the &lt;code>networking.k8s.io/v1beta1&lt;/code> API version of &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-class">IngressClass&lt;/a>&lt;/li>
&lt;li>all beta versions of &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a>: &lt;code>extensions/v1beta1&lt;/code> and &lt;code>networking.k8s.io/v1beta1&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>On a v1.22 Kubernetes cluster, you'll be able to access Ingress and IngressClass
objects through the stable (v1) APIs, but access via their beta APIs won't be possible.
This change has been in
in discussion since
&lt;a href="https://github.com/kubernetes/kubernetes/issues/43214">2017&lt;/a>,
&lt;a href="https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/">2019&lt;/a> with
1.16 Kubernetes API deprecations, and most recently in
KEP-1453:
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/1453-ingress-api#122">Graduate Ingress API to GA&lt;/a>.&lt;/p>
&lt;p>During community meetings, the networking Special Interest Group has decided to continue
supporting Kubernetes versions older than 1.22 with Ingress-NGINX version 0.47.0.
Support for Ingress-NGINX will continue for six months after Kubernetes 1.22
is released. Any additional bug fixes and CVEs for Ingress-NGINX will be
addressed on a need-by-need basis.&lt;/p>
&lt;p>Ingress-NGINX will have separate branches and releases of Ingress-NGINX to
support this model, mirroring the Kubernetes project process. Future
releases of the Ingress-NGINX project will track and support the latest
versions of Kubernetes.&lt;/p>
&lt;table>&lt;caption style="display: none;">Ingress NGINX supported version with Kubernetes Versions&lt;/caption>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Kubernetes version&lt;/th>
&lt;th style="text-align:left">Ingress-NGINX version&lt;/th>
&lt;th style="text-align:left">Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">v1.22&lt;/td>
&lt;td style="text-align:left">v1.0.0-alpha.2&lt;/td>
&lt;td style="text-align:left">New features, plus bug fixes.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.21&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">Bugfixes only, and just for security issues or crashes. No end-of-support date announced.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.20&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">Bugfixes only, and just for security issues or crashes. No end-of-support date announced.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.19&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">Bugfixes only, and just for security issues or crashes. Fixes only provided until 6 months after Kubernetes v1.22.0 is released.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Because of the updates in Kubernetes 1.22, &lt;strong>v0.47.0&lt;/strong> will not work with
Kubernetes 1.22.&lt;/p>
&lt;h1 id="what-you-need-to-do">What you need to do&lt;/h1>
&lt;p>The team is currently in the process of upgrading ingress-nginx to support
the v1 migration, you can track the progress
&lt;a href="https://github.com/kubernetes/ingress-nginx/pull/7156">here&lt;/a>.&lt;br>
We're not making feature improvements to &lt;code>ingress-nginx&lt;/code> until after the support for
Ingress v1 is complete.&lt;/p>
&lt;p>In the meantime to ensure no compatibility issues:&lt;/p>
&lt;ul>
&lt;li>Update to the latest version of Ingress-NGINX; currently
&lt;a href="https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v0.47.0">v0.47.0&lt;/a>&lt;/li>
&lt;li>After Kubernetes 1.22 is released, ensure you are using the latest version of
Ingress-NGINX that supports the stable APIs for Ingress and IngressClass.&lt;/li>
&lt;li>Test Ingress-NGINX version v1.0.0-alpha.2 with Cluster versions &amp;gt;= 1.19
and report any issues to the projects Github page.&lt;/li>
&lt;/ul>
&lt;p>The community’s feedback and support in this effort is welcome. The
Ingress-NGINX Sub-project regularly holds community meetings where we discuss
this and other issues facing the project. For more information on the sub-project,
please see &lt;a href="https://github.com/kubernetes/community/tree/master/sig-network">SIG Network&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes Release Cadence Change: Here’s What You Need To Know</title><link>https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/</link><pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/20/new-kubernetes-release-cadence/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Celeste Horgan, Adolfo García Veytia, James Laverack, Jeremy Rickard&lt;/p>
&lt;p>On April 23, 2021, the Release Team merged a Kubernetes Enhancement Proposal (KEP) changing the Kubernetes release cycle from four releases a year (once a quarter) to three releases a year.&lt;/p>
&lt;p>This blog post provides a high level overview about what this means for the Kubernetes community's contributors and maintainers.&lt;/p>
&lt;h2 id="what-s-changing-and-when">What's changing and when&lt;/h2>
&lt;p>Starting with the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.22">Kubernetes 1.22 release&lt;/a>, a lightweight policy will drive the creation of each release schedule. This policy states:&lt;/p>
&lt;ul>
&lt;li>The first Kubernetes release of a calendar year should start at the second or third
week of January to provide people more time for contributors coming back from the
end of year holidays.&lt;/li>
&lt;li>The last Kubernetes release of a calendar year should be finished by the middle of
December.&lt;/li>
&lt;li>A Kubernetes release cycle has a length of approximately 15 weeks.&lt;/li>
&lt;li>The week of KubeCon + CloudNativeCon is not considered a 'working week' for SIG Release. The Release Team will not hold meetings or make decisions in this period.&lt;/li>
&lt;li>An explicit SIG Release break of at least two weeks between each cycle will
be enforced.&lt;/li>
&lt;/ul>
&lt;p>As a result, Kubernetes will follow a three releases per year cadence. Kubernetes 1.23 will be the final release of the 2021 calendar year. This new policy results in a very predictable release schedule, allowing us to forecast upcoming release dates:&lt;/p>
&lt;p>&lt;em>Proposed Kubernetes Release Schedule for the remainder of 2021&lt;/em>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Week Number in Year&lt;/th>
&lt;th>Release Number&lt;/th>
&lt;th>Release Week&lt;/th>
&lt;th>Note&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>35&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1 (August 23)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>50&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>16 (December 07)&lt;/td>
&lt;td>KubeCon + CloudNativeCon NA Break (Oct 11-15)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;em>Proposed Kubernetes Release Schedule for 2022&lt;/em>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Week Number in Year&lt;/th>
&lt;th>Release Number&lt;/th>
&lt;th>Release Week&lt;/th>
&lt;th>Note&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>1.24&lt;/td>
&lt;td>1 (January 03)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>1.24&lt;/td>
&lt;td>15 (April 12)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1 (April 26)&lt;/td>
&lt;td>KubeCon + CloudNativeCon EU likely to occur&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>15 (August 09)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>34&lt;/td>
&lt;td>1.26&lt;/td>
&lt;td>1 (August 22&lt;/td>
&lt;td>KubeCon + CloudNativeCon NA likely to occur&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>49&lt;/td>
&lt;td>1.26&lt;/td>
&lt;td>14 (December 06)&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>These proposed dates reflect only the start and end dates, and they are subject to change. The Release Team will select dates for enhancement freeze, code freeze, and other milestones at the start of each release. For more information on these milestones, please refer to the &lt;a href="https://www.k8s.dev/resources/release/#phases">release phases&lt;/a> documentation. Feedback from prior releases will feed into this process.&lt;/p>
&lt;h2 id="what-this-means-for-end-users">What this means for end users&lt;/h2>
&lt;p>The major change end users will experience is a slower release cadence and a slower rate of enhancement graduation. Kubernetes release artifacts, release notes, and all other aspects of any given release will stay the same.&lt;/p>
&lt;p>Prior to this change an enhancement could graduate from alpha to stable in 9 months. With the change in cadence, this will stretch to 12 months. Additionally, graduation of features over the last few releases has in some part been driven by release team activities.&lt;/p>
&lt;p>With fewer releases, users can expect to see the rate of feature graduation slow. Users can also expect releases to contain a larger number of enhancements that they need to be aware of during upgrades. However, with fewer releases to consume per year, it's intended that end user organizations will spend less time on upgrades and gain more time on supporting their Kubernetes clusters. It also means that Kubernetes releases are in support for a slightly longer period of time, so bug fixes and security patches will be available for releases for a longer period of time.&lt;/p>
&lt;h2 id="what-this-means-for-kubernetes-contributors">What this means for Kubernetes contributors&lt;/h2>
&lt;p>With a lower release cadence, contributors have more time for project enhancements, feature development, planning, and testing. A slower release cadence also provides more room for maintaining their mental health, preparing for events like KubeCon + CloudNativeCon or work on downstream integrations.&lt;/p>
&lt;h2 id="why-we-decided-to-change-the-release-cadence">Why we decided to change the release cadence&lt;/h2>
&lt;p>The Kubernetes 1.19 cycle was far longer than usual. SIG Release extended it to lessen the burden on both Kubernetes contributors and end users due the COVID-19 pandemic. Following this extended release, the Kubernetes 1.20 release became the third, and final, release for 2020.&lt;/p>
&lt;p>As the Kubernetes project matures, the number of enhancements per cycle grows, along with the burden on contributors, the Release Engineering team. Downstream consumers and integrators also face increased challenges keeping up with &lt;a href="https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/">ever more feature-packed releases&lt;/a>. A wider project adoption means the complexity of supporting a rapidly evolving platform affects a bigger downstream chain of consumers.&lt;/p>
&lt;p>Changing the release cadence from four to three releases per year balances a variety of factors for stakeholders: while it's not strictly an LTS policy, consumers and integrators will get longer support terms for each minor version as the extended release cycles lead to the &lt;a href="https://kubernetes.io/blog/2020/08/31/kubernetes-1-19-feature-one-year-support/">previous three releases being supported&lt;/a> for a longer period. Contributors get more time to &lt;a href="https://www.cncf.io/blog/2021/04/12/enhancing-the-kubernetes-enhancements-process/">mature enhancements&lt;/a> and &lt;a href="https://github.com/kubernetes/community/blob/master/sig-architecture/production-readiness.md">get them ready for production&lt;/a>.&lt;/p>
&lt;p>Finally, the management overhead for SIG Release and the Release Engineering team diminishes allowing the team to spend more time on improving the quality of the software releases and the tooling that drives them.&lt;/p>
&lt;h2 id="how-you-can-help">How you can help&lt;/h2>
&lt;p>Join the &lt;a href="https://github.com/kubernetes/sig-release/discussions/1566">discussion&lt;/a> about communicating future release dates and be sure to be on the lookout for post release surveys.&lt;/p>
&lt;h2 id="where-you-can-find-out-more">Where you can find out more&lt;/h2>
&lt;ul>
&lt;li>Read the KEP &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-release/2572-release-cadence">here&lt;/a>&lt;/li>
&lt;li>Join the &lt;a href="https://groups.google.com/g/kubernetes-dev">kubernetes-dev&lt;/a> mailing list&lt;/li>
&lt;li>Join &lt;a href="https://slack.k8s.io">Kubernetes Slack&lt;/a> and follow the #announcements channel&lt;/li>
&lt;/ul></description></item><item><title>Blog: Spotlight on SIG Usability</title><link>https://kubernetes.io/blog/2021/07/15/sig-usability-spotlight-2021/</link><pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/15/sig-usability-spotlight-2021/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Kunal Kushwaha, Civo&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Are you interested in learning about what &lt;a href="https://github.com/kubernetes/community/tree/master/sig-usability">SIG Usability&lt;/a> does and how you can get involved? Well, you're at the right place. SIG Usability is all about making Kubernetes more accessible to new folks, and its main activity is conducting user research for the community. In this blog, we have summarized our conversation with &lt;a href="https://twitter.com/morengab">Gaby Moreno&lt;/a>, who walks us through the various aspects of being a part of the SIG and shares some insights about how others can get involved.&lt;/p>
&lt;p>Gaby is a co-lead for SIG Usability. She works as a Product Designer at IBM and enjoys working on the user experience of open, hybrid cloud technologies like Kubernetes, OpenShift, Terraform, and Cloud Foundry.&lt;/p>
&lt;h2 id="a-summary-of-our-conversation">A summary of our conversation&lt;/h2>
&lt;h3 id="q-could-you-tell-us-a-little-about-what-sig-usability-does">Q. Could you tell us a little about what SIG Usability does?&lt;/h3>
&lt;p>A. SIG Usability at a high level started because there was no dedicated user experience team for Kubernetes. The extent of SIG Usability is focussed on the end-client ease of use of the Kubernetes project. The main activity is user research for the community, which includes speaking to Kubernetes users.&lt;/p>
&lt;p>This covers points like user experience and accessibility. The objectives of the SIG are to guarantee that the Kubernetes project is maximally usable by people of a wide range of foundations and capacities, such as incorporating internationalization and ensuring the openness of documentation.&lt;/p>
&lt;h3 id="q-why-should-new-and-existing-contributors-consider-joining-sig-usability">Q. Why should new and existing contributors consider joining SIG Usability?&lt;/h3>
&lt;p>A. There are plenty of territories where new contributors can begin. For example:&lt;/p>
&lt;ul>
&lt;li>User research projects, where people can help understand the usability of the end-user experiences, including error messages, end-to-end tasks, etc.&lt;/li>
&lt;li>Accessibility guidelines for Kubernetes community artifacts, examples include: internationalization of documentation, color choices for people with color blindness, ensuring compatibility with screen reader technology, user interface design for core components with user interfaces, and more.&lt;/li>
&lt;/ul>
&lt;h3 id="q-what-do-you-do-to-help-new-contributors-get-started">Q. What do you do to help new contributors get started?&lt;/h3>
&lt;p>A. New contributors can get started by shadowing one of the user interviews, going through user interview transcripts, analyzing them, and designing surveys.&lt;/p>
&lt;p>SIG Usability is also open to new project ideas. If you have an idea, we’ll do what we can to support it. There are regular SIG Meetings where people can ask their questions live. These meetings are also recorded for those who may not be able to attend. As always, you can reach out to us on Slack as well.&lt;/p>
&lt;h3 id="q-what-does-the-survey-include">Q. What does the survey include?&lt;/h3>
&lt;p>A. In simple terms, the survey gathers information about how people use Kubernetes, such as trends in learning to deploy a new system, error messages they receive, and workflows.&lt;/p>
&lt;p>One of our goals is to standardize the responses accordingly. The ultimate goal is to analyze survey responses for important user stories whose needs aren't being met.&lt;/p>
&lt;h3 id="q-are-there-any-particular-skills-you-d-like-to-recruit-for-what-skills-are-contributors-to-sig-usability-likely-to-learn">Q. Are there any particular skills you’d like to recruit for? What skills are contributors to SIG Usability likely to learn?&lt;/h3>
&lt;p>A. Although contributing to SIG Usability does not have any pre-requisites as such, experience with user research, qualitative research, or prior experience with how to conduct an interview would be great plus points. Quantitative research, like survey design and screening, is also helpful and something that we expect contributors to learn.&lt;/p>
&lt;h3 id="q-what-are-you-getting-positive-feedback-on-and-what-s-coming-up-next-for-sig-usability">Q. What are you getting positive feedback on, and what’s coming up next for SIG Usability?&lt;/h3>
&lt;p>A. We have had new members joining and coming to monthly meetings regularly and showing interests in becoming a contributor and helping the community. We have also had a lot of people reach out to us via Slack showcasing their interest in the SIG.&lt;/p>
&lt;p>Currently, we are focused on finishing the study mentioned in our &lt;a href="https://www.youtube.com/watch?v=Byn0N_ZstE0">talk&lt;/a>, also our project for this year. We are always happy to have new contributors join us.&lt;/p>
&lt;h3 id="q-any-closing-thoughts-resources-you-d-like-to-share">Q: Any closing thoughts/resources you’d like to share?&lt;/h3>
&lt;p>A. We love meeting new contributors and assisting them in investigating different Kubernetes project spaces. We will work with and team up with other SIGs to facilitate engaging with end-users, running studies, and help them integrate accessible design practices into their development practices.&lt;/p>
&lt;p>Here are some resources for you to get started:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-usability">GitHub&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://groups.google.com/g/kubernetes-sig-usability">Mailing list&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/labels/sig%2Fusability">Open Community Issues/PRs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.slack.com/archives/CLC5EF63T">Slack channel #sig-usability&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="wrap-up">Wrap Up&lt;/h2>
&lt;p>SIG Usability hosted a &lt;a href="https://www.youtube.com/watch?v=Byn0N_ZstE0">KubeCon talk&lt;/a> about studying Kubernetes users' experiences. The talk focuses on updates to the user study projects, understanding who is using Kubernetes, what they are trying to achieve, how the project is addressing their needs, and where we need to improve the project and the client experience. Join the SIG's update to find out about the most recent research results, what the plans are for the forthcoming year, and how to get involved in the upstream usability team as a contributor!&lt;/p></description></item><item><title>Blog: Kubernetes API and Feature Removals In 1.22: Here’s What You Need To Know</title><link>https://kubernetes.io/blog/2021/07/14/upcoming-changes-in-kubernetes-1-22/</link><pubDate>Wed, 14 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/07/14/upcoming-changes-in-kubernetes-1-22/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Krishna Kilari (Amazon Web Services), Tim Bannister (The Scale Factory)&lt;/p>
&lt;p>As the Kubernetes API evolves, APIs are periodically reorganized or upgraded.
When APIs evolve, the old APIs they replace are deprecated, and eventually removed.
See &lt;a href="#kubernetes-api-removals">Kubernetes API removals&lt;/a> to read more about Kubernetes'
policy on removing APIs.&lt;/p>
&lt;p>We want to make sure you're aware of some upcoming removals. These are
beta APIs that you can use in current, supported Kubernetes versions,
and they are already deprecated. The reason for all of these removals
is that they have been superseded by a newer, stable (“GA”) API.&lt;/p>
&lt;p>Kubernetes 1.22, due for release in August 2021, will remove a number of deprecated
APIs.
&lt;em>Update&lt;/em>:
&lt;a href="https://kubernetes.io/blog/2021/08/04/kubernetes-1-22-release-announcement/">Kubernetes 1.22: Reaching New Peaks&lt;/a>
has details on the v1.22 release.&lt;/p>
&lt;h2 id="api-changes">API removals for Kubernetes v1.22&lt;/h2>
&lt;p>The &lt;strong>v1.22&lt;/strong> release will stop serving the API versions we've listed immediately below.
These are all beta APIs that were previously deprecated in favor of newer and more stable
API versions.&lt;/p>
&lt;!-- sorted by API group -->
&lt;ul>
&lt;li>Beta versions of the &lt;code>ValidatingWebhookConfiguration&lt;/code> and &lt;code>MutatingWebhookConfiguration&lt;/code> API (the &lt;strong>admissionregistration.k8s.io/v1beta1&lt;/strong> API versions)&lt;/li>
&lt;li>The beta &lt;code>CustomResourceDefinition&lt;/code> API (&lt;strong>apiextensions.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>APIService&lt;/code> API (&lt;strong>apiregistration.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>TokenReview&lt;/code> API (&lt;strong>authentication.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>Beta API versions of &lt;code>SubjectAccessReview&lt;/code>, &lt;code>LocalSubjectAccessReview&lt;/code>, &lt;code>SelfSubjectAccessReview&lt;/code> (API versions from &lt;strong>authorization.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>CertificateSigningRequest&lt;/code> API (&lt;strong>certificates.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>Lease&lt;/code> API (&lt;strong>coordination.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>All beta &lt;code>Ingress&lt;/code> APIs (the &lt;strong>extensions/v1beta1&lt;/strong> and &lt;strong>networking.k8s.io/v1beta1&lt;/strong> API versions)&lt;/li>
&lt;/ul>
&lt;p>The Kubernetes documentation covers these
&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22">API removals for v1.22&lt;/a> and explains
how each of those APIs change between beta and stable.&lt;/p>
&lt;h2 id="what-to-do">What to do&lt;/h2>
&lt;p>We're going to run through each of the resources that are affected by these removals
and explain the steps you'll need to take.&lt;/p>
&lt;dl>
&lt;dt>&lt;code>Ingress&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>networking.k8s.io/v1&lt;/strong>
&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-v1/">Ingress&lt;/a> API,
&lt;a href="https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/#ingress-graduates-to-general-availability">available since v1.19&lt;/a>.&lt;br>
The related API &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-class-v1/">IngressClass&lt;/a>
is designed to complement the &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a>
concept, allowing you to configure multiple kinds of Ingress within one cluster.
If you're currently using the deprecated
&lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#kubernetes-io-ingress-class-deprecated">&lt;code>kubernetes.io/ingress.class&lt;/code>&lt;/a>
annotation, plan to switch to using the &lt;code>.spec.ingressClassName&lt;/code> field instead.&lt;br>
On any cluster running Kubernetes v1.19 or later, you can use the v1 API to
retrieve or update existing Ingress objects, even if they were created using an
older API version.
&lt;p>When you convert an Ingress to the v1 API, you should review each rule in that Ingress.
Older Ingresses use the legacy &lt;code>ImplementationSpecific&lt;/code> path type. Instead of &lt;code>ImplementationSpecific&lt;/code>, switch &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#path-types">path matching&lt;/a> to either &lt;code>Prefix&lt;/code> or &lt;code>Exact&lt;/code>. One of the benefits of moving to these alternative path types is that it becomes easier to migrate between different Ingress classes.&lt;/p>
&lt;p>&lt;strong>ⓘ&lt;/strong> As well as upgrading &lt;em>your&lt;/em> own use of the Ingress API as a client, make sure that
every ingress controller that you use is compatible with the v1 Ingress API.
Read &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#prerequisites">Ingress Prerequisites&lt;/a>
for more context about Ingress and ingress controllers.&lt;/p>
&lt;/dd>
&lt;dt>&lt;code>ValidatingWebhookConfiguration&lt;/code> and &lt;code>MutatingWebhookConfiguration&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>admissionregistration.k8s.io/v1&lt;/strong> API versions of
&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/validating-webhook-configuration-v1/">ValidatingWebhookConfiguration&lt;/a>
and &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/mutating-webhook-configuration-v1/">MutatingWebhookConfiguration&lt;/a>,
available since v1.16.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created using an older API version.&lt;/dd>
&lt;dt>&lt;code>CustomResourceDefinition&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/extend-resources/custom-resource-definition-v1/">CustomResourceDefinition&lt;/a>
&lt;strong>apiextensions.k8s.io/v1&lt;/strong> API, available since v1.16.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created
using an older API version. If you defined any custom resources in your cluster, those
are still served after you upgrade.
&lt;p>If you're using external CustomResourceDefinitions, you can use
&lt;a href="#kubectl-convert">&lt;code>kubectl convert&lt;/code>&lt;/a> to translate existing manifests to use the newer API.
Because there are some functional differences between beta and stable CustomResourceDefinitions,
our advice is to test out each one to make sure it works how you expect after the upgrade.&lt;/p>
&lt;/dd>
&lt;dt>&lt;code>APIService&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>apiregistration.k8s.io/v1&lt;/strong> &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/api-service-v1/">APIService&lt;/a>
API, available since v1.10.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created using an older API version.
If you already have API aggregation using an APIService object, this aggregation continues
to work after you upgrade.&lt;/dd>
&lt;dt>&lt;code>TokenReview&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>authentication.k8s.io/v1&lt;/strong> &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-review-v1/">TokenReview&lt;/a>
API, available since v1.10.
&lt;p>As well as serving this API via HTTP, the Kubernetes API server uses the same format to
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication">send&lt;/a>
TokenReviews to webhooks. The v1.22 release continues to use the v1beta1 API for TokenReviews
sent to webhooks by default. See &lt;a href="#looking-ahead">Looking ahead&lt;/a> for some specific tips about
switching to the stable API.&lt;/p>
&lt;/dd>
&lt;dt>&lt;code>SubjectAccessReview&lt;/code>, &lt;code>SelfSubjectAccessReview&lt;/code> and &lt;code>LocalSubjectAccessReview&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>authorization.k8s.io/v1&lt;/strong> versions of those
&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/">authorization APIs&lt;/a>, available since v1.6.&lt;/dd>
&lt;dt>&lt;code>CertificateSigningRequest&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>certificates.k8s.io/v1&lt;/strong>
&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/certificate-signing-request-v1/">CertificateSigningRequest&lt;/a>
API, available since v1.19.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created
using an older API version. Existing issued certificates retain their validity when you upgrade.&lt;/dd>
&lt;dt>&lt;code>Lease&lt;/code>&lt;/dt>
&lt;dd>Migrate to use the &lt;strong>coordination.k8s.io/v1&lt;/strong> &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/">Lease&lt;/a>
API, available since v1.14.&lt;br>
You can use the v1 API to retrieve or update existing objects, even if they were created
using an older API version.&lt;/dd>
&lt;/dl>
&lt;h3 id="kubectl-convert">&lt;code>kubectl convert&lt;/code>&lt;/h3>
&lt;p>There is a plugin to &lt;code>kubectl&lt;/code> that provides the &lt;code>kubectl convert&lt;/code> subcommand.
It's an official plugin that you can download as part of Kubernetes.
See &lt;a href="https://kubernetes.io/releases/download/">Download Kubernetes&lt;/a> for more details.&lt;/p>
&lt;p>You can use &lt;code>kubectl convert&lt;/code> to update manifest files to use a different API
version. For example, if you have a manifest in source control that uses the beta
Ingress API, you can check that definition out,
and run
&lt;code>kubectl convert -f &amp;lt;manifest&amp;gt; --output-version &amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;&lt;/code>.
You can use the &lt;code>kubectl convert&lt;/code> command to automatically convert an
existing manifest.&lt;/p>
&lt;p>For example, to convert an older Ingress definition to
&lt;code>networking.k8s.io/v1&lt;/code>, you can run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl convert -f ./legacy-ingress.yaml --output-version networking.k8s.io/v1
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The automatic conversion uses a similar technique to how the Kubernetes control plane
updates objects that were originally created using an older API version. Because it's
a mechanical conversion, you might need to go in and change the manifest to adjust
defaults etc.&lt;/p>
&lt;h3 id="rehearse-for-the-upgrade">Rehearse for the upgrade&lt;/h3>
&lt;p>If you manage your cluster's API server component, you can try out these API
removals before you upgrade to Kubernetes v1.22.&lt;/p>
&lt;p>To do that, add the following to the kube-apiserver command line arguments:&lt;/p>
&lt;p>&lt;code>--runtime-config=admissionregistration.k8s.io/v1beta1=false,apiextensions.k8s.io/v1beta1=false,apiregistration.k8s.io/v1beta1=false,authentication.k8s.io/v1beta1=false,authorization.k8s.io/v1beta1=false,certificates.k8s.io/v1beta1=false,coordination.k8s.io/v1beta1=false,extensions/v1beta1/ingresses=false,networking.k8s.io/v1beta1=false&lt;/code>&lt;/p>
&lt;p>(as a side effect, this also turns off v1beta1 of EndpointSlice - watch out for
that when you're testing).&lt;/p>
&lt;p>Once you've switched all the kube-apiservers in your cluster to use that setting,
those beta APIs are removed. You can test that API clients (&lt;code>kubectl&lt;/code>, deployment
tools, custom controllers etc) still work how you expect, and you can revert if
you need to without having to plan a more disruptive downgrade.&lt;/p>
&lt;h3 id="advice-for-software-authors">Advice for software authors&lt;/h3>
&lt;p>Maybe you're reading this because you're a developer of an addon or other
component that integrates with Kubernetes?&lt;/p>
&lt;p>If you develop an Ingress controller, webhook authenticator, an API aggregation, or
any other tool that relies on these deprecated APIs, you should already have started
to switch your software over.&lt;/p>
&lt;p>You can use the tips in
&lt;a href="#rehearse-for-the-upgrade">Rehearse for the upgrade&lt;/a> to run your own Kubernetes
cluster that only uses the new APIs, and make sure that your code works OK.
For your documentation, make sure readers are aware of any steps they should take
for the Kubernetes v1.22 upgrade.&lt;/p>
&lt;p>Where possible, give your users a hand to adopt the new APIs early - perhaps in a
test environment - so they can give you feedback about any problems.&lt;/p>
&lt;p>There are some &lt;a href="#looking-ahead">more deprecations&lt;/a> coming in Kubernetes v1.25,
so plan to have those covered too.&lt;/p>
&lt;h2 id="kubernetes-api-removals">Kubernetes API removals&lt;/h2>
&lt;p>Here's some background about why Kubernetes removes some APIs, and also a promise
about &lt;em>stable&lt;/em> APIs in Kubernetes.&lt;/p>
&lt;p>Kubernetes follows a defined
&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy&lt;/a> for its
features, including the Kubernetes API. That policy allows for replacing stable
(“GA”) APIs from Kubernetes. Importantly, this policy means that a stable API only
be deprecated when a newer stable version of that same API is available.&lt;/p>
&lt;p>That stability guarantee matters: if you're using a stable Kubernetes API, there
won't ever be a new version released that forces you to switch to an alpha or beta
feature.&lt;/p>
&lt;p>Earlier stages are different. Alpha features are under test and potentially
incomplete. Almost always, alpha features are disabled by default.
Kubernetes releases can and do remove alpha features that haven't worked out.&lt;/p>
&lt;p>After alpha, comes beta. These features are typically enabled by default; if the
testing works out, the feature can graduate to stable. If not, it might need
a redesign.&lt;/p>
&lt;p>Last year, Kubernetes officially
&lt;a href="https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/#avoiding-permanent-beta">adopted&lt;/a>
a policy for APIs that have reached their beta phase:&lt;/p>
&lt;blockquote>
&lt;p>For Kubernetes REST APIs, when a new feature's API reaches beta, that starts
a countdown. The beta-quality API now has three releases …
to either:&lt;/p>
&lt;ul>
&lt;li>reach GA, and deprecate the beta, or&lt;/li>
&lt;li>have a new beta version (and deprecate the previous beta).&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>&lt;em>At the time of that article, three Kubernetes releases equated to roughly nine
calendar months. Later that same month, Kubernetes
adopted a new
release cadence of three releases per calendar year, so the countdown period is
now roughly twelve calendar months.&lt;/em>&lt;/p>
&lt;p>Whether an API removal is because of a beta feature graduating to stable, or
because that API hasn't proved successful, Kubernetes will continue to remove
APIs by following its deprecation policy and making sure that migration options
are documented.&lt;/p>
&lt;h3 id="looking-ahead">Looking ahead&lt;/h3>
&lt;p>There's a setting that's relevant if you use webhook authentication checks.
A future Kubernetes release will switch to sending TokenReview objects
to webhooks using the &lt;code>authentication.k8s.io/v1&lt;/code> API by default. At the moment,
the default is to send &lt;code>authentication.k8s.io/v1beta1&lt;/code> TokenReviews to webhooks,
and that's still the default for Kubernetes v1.22.
However, you can switch over to the stable API right now if you want:
add &lt;code>--authentication-token-webhook-version=v1&lt;/code> to the command line options for
the kube-apiserver, and check that webhooks for authentication still work how you
expected.&lt;/p>
&lt;p>Once you're happy it works OK, you can leave the &lt;code>--authentication-token-webhook-version=v1&lt;/code>
option set across your control plane.&lt;/p>
&lt;p>The &lt;strong>v1.25&lt;/strong> release that's planned for next year will stop serving beta versions of
several Kubernetes APIs that are stable right now and have been for some time.
The same v1.25 release will &lt;strong>remove&lt;/strong> PodSecurityPolicy, which is deprecated and won't
graduate to stable. See
&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future&lt;/a>
for more information.&lt;/p>
&lt;p>The official &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25">list of API removals&lt;/a>
planned for Kubernetes 1.25 is:&lt;/p>
&lt;ul>
&lt;li>The beta &lt;code>CronJob&lt;/code> API (&lt;strong>batch/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>EndpointSlice&lt;/code> API (&lt;strong>networking.k8s.io/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>PodDisruptionBudget&lt;/code> API (&lt;strong>policy/v1beta1&lt;/strong>)&lt;/li>
&lt;li>The beta &lt;code>PodSecurityPolicy&lt;/code> API (&lt;strong>policy/v1beta1&lt;/strong>)&lt;/li>
&lt;/ul>
&lt;h2 id="want-to-know-more">Want to know more?&lt;/h2>
&lt;p>Deprecations are announced in the Kubernetes release notes. You can see the announcements
of pending deprecations in the release notes for
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#deprecations">1.19&lt;/a>,
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">1.20&lt;/a>,
and &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">1.21&lt;/a>.&lt;/p>
&lt;p>For information on the process of deprecation and removal, check out the official Kubernetes
&lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api">deprecation policy&lt;/a>
document.&lt;/p></description></item><item><title>Blog: Announcing Kubernetes Community Group Annual Reports</title><link>https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/</link><pubDate>Mon, 28 Jun 2021 10:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Divya Mohan&lt;/p>
&lt;figure>&lt;a href="https://www.cncf.io/reports/kubernetes-community-annual-report-2020/">
&lt;img src="https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/k8s_annual_report_2020.svg"
alt="Community annual report 2020"/> &lt;/a>
&lt;/figure>
&lt;p>Given the growth and scale of the Kubernetes project, the existing reporting mechanisms were proving to be inadequate and challenging.
Kubernetes is a large open source project. With over 100000 commits just to the main k/kubernetes repository, hundreds of other code
repositories in the project, and thousands of contributors, there's a lot going on. In fact, there are 37 contributor groups at the time of
writing. We also value all forms of contribution and not just code changes.&lt;/p>
&lt;p>With that context in mind, the challenge of reporting on all this activity was a call to action for exploring better options. Therefore
inspired by the Apache Software Foundation’s &lt;a href="https://www.apache.org/foundation/board/reporting">open guide to PMC Reporting&lt;/a> and the
&lt;a href="https://www.cncf.io/cncf-annual-report-2020/">CNCF project Annual Reporting&lt;/a>, the Kubernetes project is proud to announce the
&lt;strong>Kubernetes Community Group Annual Reports for Special Interest Groups (SIGs) and Working Groups (WGs)&lt;/strong>. In its flagship edition,
the &lt;a href="https://www.cncf.io/reports/kubernetes-community-annual-report-2020/">2020 Summary report&lt;/a> focuses on bettering the
Kubernetes ecosystem by assessing and promoting the healthiness of the groups within the upstream community.&lt;/p>
&lt;p>Previously, the mechanisms for the Kubernetes project overall to report on groups and their activities were
&lt;a href="https://k8s.devstats.cncf.io/">devstats&lt;/a>, GitHub data, issues, to measure the healthiness of a given UG/WG/SIG/Committee. As a
project spanning several diverse communities, it was essential to have something that captured the human side of things. With 50,000+
contributors, it’s easy to assume that the project has enough help and this report surfaces more information than /help-wanted and
/good-first-issue for end users. This is how we sustain the project. Paraphrasing one of the Steering Committee members,
&lt;a href="https://github.com/parispittman">Paris Pittman&lt;/a>, “There was a requirement for tighter feedback loops - ones that involved more than just
GitHub data and issues. Given that Kubernetes, as a project, has grown in scale and number of contributors over the years, we have
outgrown the existing reporting mechanisms.&amp;quot;&lt;/p>
&lt;p>The existing communication channels between the Steering committee members and the folks leading the groups and committees were also required
to be made as open and as bi-directional as possible. Towards achieving this very purpose, every group and committee has been assigned a
liaison from among the steering committee members for kick off, help, or guidance needed throughout the process. According to
&lt;a href="https://github.com/dims">Davanum Srinivas a.k.a. dims&lt;/a>, “... That was one of the main motivations behind this report. People (leading the
groups/committees) know that they can reach out to us and there’s a vehicle for them to reach out to us… This is our way of setting up a
two-way feedback for them.&amp;quot; The progress on these action items would be updated and tracked on the monthly Steering Committee meetings
ensuring that this is not a one-off activity. Quoting &lt;a href="https://github.com/nikhita">Nikhita Raghunath&lt;/a>, one of the Steering Committee members,
“... Once we have a base, the liaisons will work with these groups to ensure that the problems are resolved. When we have a report next year,
we’ll have a look at the progress made and how we could still do better. But the idea is definitely to not stop at the report.”&lt;/p>
&lt;p>With this report, we hope to empower our end user communities with information that they can use to identify ways in which they can support
the project as well as a sneak peek into the roadmap for upcoming features. As a community, we thrive on feedback and would love to hear your
views about the report. You can get in touch with the &lt;a href="https://github.com/kubernetes/steering#contact">Steering Committee&lt;/a> via
&lt;a href="https://kubernetes.slack.com/messages/steering-committee">Slack&lt;/a> or via the &lt;a href="steering@kubernetes.io">mailing list&lt;/a>.&lt;/p></description></item><item><title>Blog: Writing a Controller for Pod Labels</title><link>https://kubernetes.io/blog/2021/06/21/writing-a-controller-for-pod-labels/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/06/21/writing-a-controller-for-pod-labels/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Arthur Busser (Padok)&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">Operators&lt;/a> are proving to be an excellent solution to
running stateful distributed applications in Kubernetes. Open source tools like
the &lt;a href="https://sdk.operatorframework.io/">Operator SDK&lt;/a> provide ways to build reliable and maintainable
operators, making it easier to extend Kubernetes and implement custom
scheduling.&lt;/p>
&lt;p>Kubernetes operators run complex software inside your cluster. The open source
community has already built &lt;a href="https://operatorhub.io/">many operators&lt;/a> for distributed
applications like Prometheus, Elasticsearch, or Argo CD. Even outside of
open source, operators can help to bring new functionality to your Kubernetes
cluster.&lt;/p>
&lt;p>An operator is a set of &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources&lt;/a> and a
set of &lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/">controllers&lt;/a>. A controller watches for changes to specific
resources in the Kubernetes API and reacts by creating, updating, or deleting
resources.&lt;/p>
&lt;p>The Operator SDK is best suited for building fully-featured operators.
Nonetheless, you can use it to write a single controller. This post will walk
you through writing a Kubernetes controller in Go that will add a &lt;code>pod-name&lt;/code>
label to pods that have a specific annotation.&lt;/p>
&lt;h2 id="why-do-we-need-a-controller-for-this">Why do we need a controller for this?&lt;/h2>
&lt;p>I recently worked on a project where we needed to create a Service that routed
traffic to a specific Pod in a ReplicaSet. The problem is that a Service can
only select pods by label, and all pods in a ReplicaSet have the same labels.
There are two ways to solve this problem:&lt;/p>
&lt;ol>
&lt;li>Create a Service without a selector and manage the Endpoints or
EndpointSlices for that Service directly. We would need to write a custom
controller to insert our Pod's IP address into those resources.&lt;/li>
&lt;li>Add a label to the Pod with a unique value. We could then use this label in
our Service's selector. Again, we would need to write a custom controller to
add this label.&lt;/li>
&lt;/ol>
&lt;p>A controller is a control loop that tracks one or more Kubernetes resource
types. The controller from option n°2 above only needs to track pods, which
makes it simpler to implement. This is the option we are going to walk through
by writing a Kubernetes controller that adds a &lt;code>pod-name&lt;/code> label to our pods.&lt;/p>
&lt;p>StatefulSets &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-name-label">do this natively&lt;/a> by adding a
&lt;code>pod-name&lt;/code> label to each Pod in the set. But what if we don't want to or can't
use StatefulSets?&lt;/p>
&lt;p>We rarely create pods directly; most often, we use a Deployment, ReplicaSet, or
another high-level resource. We can specify labels to add to each Pod in the
PodSpec, but not with dynamic values, so no way to replicate a StatefulSet's
&lt;code>pod-name&lt;/code> label.&lt;/p>
&lt;p>We tried using a &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">mutating admission webhook&lt;/a>. When
anyone creates a Pod, the webhook patches the Pod with a label containing the
Pod's name. Disappointingly, this does not work: not all pods have a name before
being created. For instance, when the ReplicaSet controller creates a Pod, it
sends a &lt;code>namePrefix&lt;/code> to the Kubernetes API server and not a &lt;code>name&lt;/code>. The API
server generates a unique name before persisting the new Pod to etcd, but only
after calling our admission webhook. So in most cases, we can't know a Pod's
name with a mutating webhook.&lt;/p>
&lt;p>Once a Pod exists in the Kubernetes API, it is mostly immutable, but we can
still add a label. We can even do so from the command line:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl label my-pod my-label-key&lt;span style="color:#666">=&lt;/span>my-label-value
&lt;/code>&lt;/pre>&lt;/div>&lt;p>We need to watch for changes to any pods in the Kubernetes API and add the label
we want. Rather than do this manually, we are going to write a controller that
does it for us.&lt;/p>
&lt;h2 id="bootstrapping-a-controller-with-the-operator-sdk">Bootstrapping a controller with the Operator SDK&lt;/h2>
&lt;p>A controller is a reconciliation loop that reads the desired state of a resource
from the Kubernetes API and takes action to bring the cluster's actual state
closer to the desired state.&lt;/p>
&lt;p>In order to write this controller as quickly as possible, we are going to use
the Operator SDK. If you don't have it installed, follow the
&lt;a href="https://sdk.operatorframework.io/docs/installation/">official documentation&lt;/a>.&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">$ operator-sdk version
operator-sdk version: &amp;quot;v1.4.2&amp;quot;, commit: &amp;quot;4b083393be65589358b3e0416573df04f4ae8d9b&amp;quot;, kubernetes version: &amp;quot;v1.19.4&amp;quot;, go version: &amp;quot;go1.15.8&amp;quot;, GOOS: &amp;quot;darwin&amp;quot;, GOARCH: &amp;quot;amd64&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Let's create a new directory to write our controller in:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">mkdir label-operator &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#a2f">cd&lt;/span> label-operator
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, let's initialize a new operator, to which we will add a single controller.
To do this, you will need to specify a domain and a repository. The domain
serves as a prefix for the group your custom Kubernetes resources will belong
to. Because we are not going to be defining custom resources, the domain does
not matter. The repository is going to be the name of the Go module we are going
to write. By convention, this is the repository where you will be storing your
code.&lt;/p>
&lt;p>As an example, here is the command I ran:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Feel free to change the domain and repo values.&lt;/span>
operator-sdk init --domain&lt;span style="color:#666">=&lt;/span>padok.fr --repo&lt;span style="color:#666">=&lt;/span>github.com/busser/label-operator
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, we need a create a new controller. This controller will handle pods and
not a custom resource, so no need to generate the resource code. Let's run this
command to scaffold the code we need:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">operator-sdk create api --group&lt;span style="color:#666">=&lt;/span>core --version&lt;span style="color:#666">=&lt;/span>v1 --kind&lt;span style="color:#666">=&lt;/span>Pod --controller&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f">true&lt;/span> --resource&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f">false&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>We now have a new file: &lt;code>controllers/pod_controller.go&lt;/code>. This file contains a
&lt;code>PodReconciler&lt;/code> type with two methods that we need to implement. The first is
&lt;code>Reconcile&lt;/code>, and it looks like this for now:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> (r &lt;span style="color:#666">*&lt;/span>PodReconciler) &lt;span style="color:#00a000">Reconcile&lt;/span>(ctx context.Context, req ctrl.Request) (ctrl.Result, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>) {
_ = r.Log.&lt;span style="color:#00a000">WithValues&lt;/span>(&lt;span style="color:#b44">&amp;#34;pod&amp;#34;&lt;/span>, req.NamespacedName)
&lt;span style="color:#080;font-style:italic">// your logic here
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>Reconcile&lt;/code> method is called whenever a Pod is created, updated, or deleted.
The name and namespace of the Pod are in the &lt;code>ctrl.Request&lt;/code> the method receives
as a parameter.&lt;/p>
&lt;p>The second method is &lt;code>SetupWithManager&lt;/code> and for now it looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> (r &lt;span style="color:#666">*&lt;/span>PodReconciler) &lt;span style="color:#00a000">SetupWithManager&lt;/span>(mgr ctrl.Manager) &lt;span style="color:#0b0;font-weight:bold">error&lt;/span> {
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.&lt;span style="color:#00a000">NewControllerManagedBy&lt;/span>(mgr).
&lt;span style="color:#080;font-style:italic">// Uncomment the following line adding a pointer to an instance of the controlled resource as an argument
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// For().
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#00a000">Complete&lt;/span>(r)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>SetupWithManager&lt;/code> method is called when the operator starts. It serves to
tell the operator framework what types our &lt;code>PodReconciler&lt;/code> needs to watch. To
use the same &lt;code>Pod&lt;/code> type used by Kubernetes internally, we need to import some of
its code. All of the Kubernetes source code is open source, so you can import
any part you like in your own Go code. You can find a complete list of available
packages in the Kubernetes source code or &lt;a href="https://pkg.go.dev/k8s.io/api">here on pkg.go.dev&lt;/a>. To
use pods, we need the &lt;code>k8s.io/api/core/v1&lt;/code> package.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> controllers
&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#080;font-style:italic">// other imports...
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> corev1 &lt;span style="color:#b44">&amp;#34;k8s.io/api/core/v1&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// other imports...
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Lets use the &lt;code>Pod&lt;/code> type in &lt;code>SetupWithManager&lt;/code> to tell the operator framework we
want to watch pods:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> (r &lt;span style="color:#666">*&lt;/span>PodReconciler) &lt;span style="color:#00a000">SetupWithManager&lt;/span>(mgr ctrl.Manager) &lt;span style="color:#0b0;font-weight:bold">error&lt;/span> {
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.&lt;span style="color:#00a000">NewControllerManagedBy&lt;/span>(mgr).
&lt;span style="color:#00a000">For&lt;/span>(&lt;span style="color:#666">&amp;amp;&lt;/span>corev1.Pod{}).
&lt;span style="color:#00a000">Complete&lt;/span>(r)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Before moving on, we should set the RBAC permissions our controller needs. Above
the &lt;code>Reconcile&lt;/code> method, we have some default permissions:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#080;font-style:italic">// +kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch;create;update;patch;delete
&lt;/span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:rbac:groups=core,resources=pods/status,verbs=get;update;patch
&lt;/span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:rbac:groups=core,resources=pods/finalizers,verbs=update
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We don't need all of those. Our controller will never interact with a Pod's
status or its finalizers. It only needs to read and update pods. Lets remove the
unnecessary permissions and keep only what we need:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#080;font-style:italic">// +kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch;update;patch
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We are now ready to write our controller's reconciliation logic.&lt;/p>
&lt;h2 id="implementing-reconciliation">Implementing reconciliation&lt;/h2>
&lt;p>Here is what we want our &lt;code>Reconcile&lt;/code> method to do:&lt;/p>
&lt;ol>
&lt;li>Use the Pod's name and namespace from the &lt;code>ctrl.Request&lt;/code> to fetch the Pod
from the Kubernetes API.&lt;/li>
&lt;li>If the Pod has an &lt;code>add-pod-name-label&lt;/code> annotation, add a &lt;code>pod-name&lt;/code> label to
the Pod; if the annotation is missing, don't add the label.&lt;/li>
&lt;li>Update the Pod in the Kubernetes API to persist the changes made.&lt;/li>
&lt;/ol>
&lt;p>Lets define some constants for the annotation and label:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">const&lt;/span> (
addPodNameLabelAnnotation = &lt;span style="color:#b44">&amp;#34;padok.fr/add-pod-name-label&amp;#34;&lt;/span>
podNameLabel = &lt;span style="color:#b44">&amp;#34;padok.fr/pod-name&amp;#34;&lt;/span>
)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The first step in our reconciliation function is to fetch the Pod we are working
on from the Kubernetes API:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#080;font-style:italic">// Reconcile handles a reconciliation request for a Pod.
&lt;/span>&lt;span style="color:#080;font-style:italic">// If the Pod has the addPodNameLabelAnnotation annotation, then Reconcile
&lt;/span>&lt;span style="color:#080;font-style:italic">// will make sure the podNameLabel label is present with the correct value.
&lt;/span>&lt;span style="color:#080;font-style:italic">// If the annotation is absent, then Reconcile will make sure the label is too.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> (r &lt;span style="color:#666">*&lt;/span>PodReconciler) &lt;span style="color:#00a000">Reconcile&lt;/span>(ctx context.Context, req ctrl.Request) (ctrl.Result, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>) {
log &lt;span style="color:#666">:=&lt;/span> r.Log.&lt;span style="color:#00a000">WithValues&lt;/span>(&lt;span style="color:#b44">&amp;#34;pod&amp;#34;&lt;/span>, req.NamespacedName)
&lt;span style="color:#080;font-style:italic">/*
&lt;/span>&lt;span style="color:#080;font-style:italic"> Step 0: Fetch the Pod from the Kubernetes API.
&lt;/span>&lt;span style="color:#080;font-style:italic"> */&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">var&lt;/span> pod corev1.Pod
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">:=&lt;/span> r.&lt;span style="color:#00a000">Get&lt;/span>(ctx, req.NamespacedName, &lt;span style="color:#666">&amp;amp;&lt;/span>pod); err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
log.&lt;span style="color:#00a000">Error&lt;/span>(err, &lt;span style="color:#b44">&amp;#34;unable to fetch Pod&amp;#34;&lt;/span>)
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, err
}
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Our &lt;code>Reconcile&lt;/code> method will be called when a Pod is created, updated, or
deleted. In the deletion case, our call to &lt;code>r.Get&lt;/code> will return a specific error.
Let's import the package that defines this error:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> controllers
&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#080;font-style:italic">// other imports...
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> apierrors &lt;span style="color:#b44">&amp;#34;k8s.io/apimachinery/pkg/api/errors&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// other imports...
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can now handle this specific error and — since our controller does not care
about deleted pods — explicitly ignore it:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go"> &lt;span style="color:#080;font-style:italic">/*
&lt;/span>&lt;span style="color:#080;font-style:italic"> Step 0: Fetch the Pod from the Kubernetes API.
&lt;/span>&lt;span style="color:#080;font-style:italic"> */&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">var&lt;/span> pod corev1.Pod
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">:=&lt;/span> r.&lt;span style="color:#00a000">Get&lt;/span>(ctx, req.NamespacedName, &lt;span style="color:#666">&amp;amp;&lt;/span>pod); err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> apierrors.&lt;span style="color:#00a000">IsNotFound&lt;/span>(err) {
&lt;span style="color:#080;font-style:italic">// we&amp;#39;ll ignore not-found errors, since we can get them on deleted requests.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
log.&lt;span style="color:#00a000">Error&lt;/span>(err, &lt;span style="color:#b44">&amp;#34;unable to fetch Pod&amp;#34;&lt;/span>)
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, err
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, lets edit our Pod so that our dynamic label is present if and only if our
annotation is present:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go"> &lt;span style="color:#080;font-style:italic">/*
&lt;/span>&lt;span style="color:#080;font-style:italic"> Step 1: Add or remove the label.
&lt;/span>&lt;span style="color:#080;font-style:italic"> */&lt;/span>
labelShouldBePresent &lt;span style="color:#666">:=&lt;/span> pod.Annotations[addPodNameLabelAnnotation] &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>
labelIsPresent &lt;span style="color:#666">:=&lt;/span> pod.Labels[podNameLabel] &lt;span style="color:#666">==&lt;/span> pod.Name
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> labelShouldBePresent &lt;span style="color:#666">==&lt;/span> labelIsPresent {
&lt;span style="color:#080;font-style:italic">// The desired state and actual state of the Pod are the same.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// No further action is required by the operator at this moment.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> log.&lt;span style="color:#00a000">Info&lt;/span>(&lt;span style="color:#b44">&amp;#34;no update required&amp;#34;&lt;/span>)
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> labelShouldBePresent {
&lt;span style="color:#080;font-style:italic">// If the label should be set but is not, set it.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">if&lt;/span> pod.Labels &lt;span style="color:#666">==&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
pod.Labels = &lt;span style="color:#a2f">make&lt;/span>(&lt;span style="color:#a2f;font-weight:bold">map&lt;/span>[&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>]&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>)
}
pod.Labels[podNameLabel] = pod.Name
log.&lt;span style="color:#00a000">Info&lt;/span>(&lt;span style="color:#b44">&amp;#34;adding label&amp;#34;&lt;/span>)
} &lt;span style="color:#a2f;font-weight:bold">else&lt;/span> {
&lt;span style="color:#080;font-style:italic">// If the label should not be set but is, remove it.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f">delete&lt;/span>(pod.Labels, podNameLabel)
log.&lt;span style="color:#00a000">Info&lt;/span>(&lt;span style="color:#b44">&amp;#34;removing label&amp;#34;&lt;/span>)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, let's push our updated Pod to the Kubernetes API:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go"> &lt;span style="color:#080;font-style:italic">/*
&lt;/span>&lt;span style="color:#080;font-style:italic"> Step 2: Update the Pod in the Kubernetes API.
&lt;/span>&lt;span style="color:#080;font-style:italic"> */&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">:=&lt;/span> r.&lt;span style="color:#00a000">Update&lt;/span>(ctx, &lt;span style="color:#666">&amp;amp;&lt;/span>pod); err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
log.&lt;span style="color:#00a000">Error&lt;/span>(err, &lt;span style="color:#b44">&amp;#34;unable to update Pod&amp;#34;&lt;/span>)
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, err
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>When writing our updated Pod to the Kubernetes API, there is a risk that the Pod
has been updated or deleted since we first read it. When writing a Kubernetes
controller, we should keep in mind that we are not the only actors in the
cluster. When this happens, the best thing to do is start the reconciliation
from scratch, by requeuing the event. Lets do exactly that:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go"> &lt;span style="color:#080;font-style:italic">/*
&lt;/span>&lt;span style="color:#080;font-style:italic"> Step 2: Update the Pod in the Kubernetes API.
&lt;/span>&lt;span style="color:#080;font-style:italic"> */&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">:=&lt;/span> r.&lt;span style="color:#00a000">Update&lt;/span>(ctx, &lt;span style="color:#666">&amp;amp;&lt;/span>pod); err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> apierrors.&lt;span style="color:#00a000">IsConflict&lt;/span>(err) {
&lt;span style="color:#080;font-style:italic">// The Pod has been updated since we read it.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// Requeue the Pod to try to reconciliate again.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{Requeue: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> apierrors.&lt;span style="color:#00a000">IsNotFound&lt;/span>(err) {
&lt;span style="color:#080;font-style:italic">// The Pod has been deleted since we read it.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// Requeue the Pod to try to reconciliate again.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{Requeue: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
log.&lt;span style="color:#00a000">Error&lt;/span>(err, &lt;span style="color:#b44">&amp;#34;unable to update Pod&amp;#34;&lt;/span>)
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, err
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let's remember to return successfully at the end of the method:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go"> &lt;span style="color:#a2f;font-weight:bold">return&lt;/span> ctrl.Result{}, &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And that's it! We are now ready to run the controller on our cluster.&lt;/p>
&lt;h2 id="run-the-controller-on-your-cluster">Run the controller on your cluster&lt;/h2>
&lt;p>To run our controller on your cluster, we need to run the operator. For that,
all you will need is &lt;code>kubectl&lt;/code>. If you don't have a Kubernetes cluster at hand,
I recommend you start one locally with &lt;a href="https://kind.sigs.k8s.io/docs/user/quick-start/#installation">KinD (Kubernetes in Docker)&lt;/a>.&lt;/p>
&lt;p>All it takes to run the operator from your machine is this command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">make run
&lt;/code>&lt;/pre>&lt;/div>&lt;p>After a few seconds, you should see the operator's logs. Notice that our
controller's &lt;code>Reconcile&lt;/code> method was called for all pods already running in the
cluster.&lt;/p>
&lt;p>Let's keep the operator running and, in another terminal, create a new Pod:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl run --image&lt;span style="color:#666">=&lt;/span>nginx my-nginx
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The operator should quickly print some logs, indicating that it reacted to the
Pod's creation and subsequent changes in status:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">INFO controllers.Pod no update required {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
INFO controllers.Pod no update required {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
INFO controllers.Pod no update required {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
INFO controllers.Pod no update required {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Lets check the Pod's labels:&lt;/p>
&lt;pre>&lt;code class="language-terminal" data-lang="terminal">$ kubectl get pod my-nginx --show-labels
NAME READY STATUS RESTARTS AGE LABELS
my-nginx 1/1 Running 0 11m run=my-nginx
&lt;/code>&lt;/pre>&lt;p>Let's add an annotation to the Pod so that our controller knows to add our
dynamic label to it:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl annotate pod my-nginx padok.fr/add-pod-name-label&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f">true&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Notice that the controller immediately reacted and produced a new line in its
logs:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">INFO controllers.Pod adding label {&amp;#34;pod&amp;#34;: &amp;#34;default/my-nginx&amp;#34;}
&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code class="language-terminal" data-lang="terminal">$ kubectl get pod my-nginx --show-labels
NAME READY STATUS RESTARTS AGE LABELS
my-nginx 1/1 Running 0 13m padok.fr/pod-name=my-nginx,run=my-nginx
&lt;/code>&lt;/pre>&lt;p>Bravo! You just successfully wrote a Kubernetes controller capable of adding
labels with dynamic values to resources in your cluster.&lt;/p>
&lt;p>Controllers and operators, both big and small, can be an important part of your
Kubernetes journey. Writing operators is easier now than it has ever been. The
possibilities are endless.&lt;/p>
&lt;h2 id="what-next">What next?&lt;/h2>
&lt;p>If you want to go further, I recommend starting by deploying your controller or
operator inside a cluster. The &lt;code>Makefile&lt;/code> generated by the Operator SDK will do
most of the work.&lt;/p>
&lt;p>When deploying an operator to production, it is always a good idea to implement
robust testing. The first step in that direction is to write unit tests.
&lt;a href="https://sdk.operatorframework.io/docs/building-operators/golang/testing/">This documentation&lt;/a> will guide you in writing tests for
your operator. I wrote tests for the operator we just wrote; you can find all of
my code in &lt;a href="https://github.com/busser/label-operator">this GitHub repository&lt;/a>.&lt;/p>
&lt;h2 id="how-to-learn-more">How to learn more?&lt;/h2>
&lt;p>The &lt;a href="https://sdk.operatorframework.io/docs/">Operator SDK documentation&lt;/a> goes into detail on how you
can go further and implement more complex operators.&lt;/p>
&lt;p>When modeling a more complex use-case, a single controller acting on built-in
Kubernetes types may not be enough. You may need to build a more complex
operator with &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definitions (CRDs)&lt;/a>
and multiple controllers. The Operator SDK is a great tool to help you do this.&lt;/p>
&lt;p>If you want to discuss building an operator, join the &lt;a href="https://kubernetes.slack.com/messages/kubernetes-operators">#kubernetes-operator&lt;/a>
channel in the &lt;a href="https://slack.k8s.io/">Kubernetes Slack workspace&lt;/a>!&lt;/p>
&lt;!-- Links --></description></item><item><title>Blog: Using Finalizers to Control Deletion</title><link>https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/</link><pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Aaron Alpar (Kasten)&lt;/p>
&lt;p>Deleting objects in Kubernetes can be challenging. You may think you’ve deleted something, only to find it still persists. While issuing a &lt;code>kubectl delete&lt;/code> command and hoping for the best might work for day-to-day operations, understanding how Kubernetes &lt;code>delete&lt;/code> commands operate will help you understand why some objects linger after deletion.&lt;/p>
&lt;p>In this post, I’ll look at:&lt;/p>
&lt;ul>
&lt;li>What properties of a resource govern deletion&lt;/li>
&lt;li>How finalizers and owner references impact object deletion&lt;/li>
&lt;li>How the propagation policy can be used to change the order of deletions&lt;/li>
&lt;li>How deletion works, with examples&lt;/li>
&lt;/ul>
&lt;p>For simplicity, all examples will use ConfigMaps and basic shell commands to demonstrate the process. We’ll explore how the commands work and discuss repercussions and results from using them in practice.&lt;/p>
&lt;h2 id="the-basic-delete">The basic &lt;code>delete&lt;/code>&lt;/h2>
&lt;p>Kubernetes has several different commands you can use that allow you to create, read, update, and delete objects. For the purpose of this blog post, we’ll focus on four &lt;code>kubectl&lt;/code> commands: &lt;code>create&lt;/code>, &lt;code>get&lt;/code>, &lt;code>patch&lt;/code>, and &lt;code>delete&lt;/code>.&lt;/p>
&lt;p>Here are examples of the basic &lt;code>kubectl delete&lt;/code> command:&lt;/p>
&lt;pre>&lt;code>kubectl create configmap mymap
configmap/mymap created
&lt;/code>&lt;/pre>&lt;pre>&lt;code>kubectl get configmap/mymap
NAME DATA AGE
mymap 0 12s
&lt;/code>&lt;/pre>&lt;pre>&lt;code>kubectl delete configmap/mymap
configmap &amp;quot;mymap&amp;quot; deleted
&lt;/code>&lt;/pre>&lt;pre>&lt;code>kubectl get configmap/mymap
Error from server (NotFound): configmaps &amp;quot;mymap&amp;quot; not found
&lt;/code>&lt;/pre>&lt;p>Shell commands preceded by &lt;code>$&lt;/code> are followed by their output. You can see that we begin with a &lt;code>kubectl create configmap mymap&lt;/code>, which will create the empty configmap &lt;code>mymap&lt;/code>. Next, we need to &lt;code>get&lt;/code> the configmap to prove it exists. We can then delete that configmap. Attempting to &lt;code>get&lt;/code> it again produces an HTTP 404 error, which means the configmap is not found.&lt;/p>
&lt;p>The state diagram for the basic &lt;code>delete&lt;/code> command is very simple:&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-05-14-using-finalizers-to-control-deletion/state-diagram-delete.png"
alt="State diagram for delete" width="495"/> &lt;figcaption>
&lt;p>State diagram for delete&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>Although this operation is straightforward, other factors may interfere with the deletion, including finalizers and owner references.&lt;/p>
&lt;h2 id="understanding-finalizers">Understanding Finalizers&lt;/h2>
&lt;p>When it comes to understanding resource deletion in Kubernetes, knowledge of how finalizers work is helpful and can help you understand why some objects don’t get deleted.&lt;/p>
&lt;p>Finalizers are keys on resources that signal pre-delete operations. They control the garbage collection on resources, and are designed to alert controllers what cleanup operations to perform prior to removing a resource. However, they don’t necessarily name code that should be executed; finalizers on resources are basically just lists of keys much like annotations. Like annotations, they can be manipulated.&lt;/p>
&lt;p>Some common finalizers you’ve likely encountered are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>kubernetes.io/pv-protection&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes.io/pvc-protection&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>The finalizers above are used on volumes to prevent accidental deletion. Similarly, some finalizers can be used to prevent deletion of any resource but are not managed by any controller.&lt;/p>
&lt;p>Below with a custom configmap, which has no properties but contains a finalizer:&lt;/p>
&lt;pre>&lt;code>cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: ConfigMap
metadata:
name: mymap
finalizers:
- kubernetes
EOF
&lt;/code>&lt;/pre>&lt;p>The configmap resource controller doesn't understand what to do with the &lt;code>kubernetes&lt;/code> finalizer key. I term these “dead” finalizers for configmaps as it is normally used on namespaces. Here’s what happen upon attempting to delete the configmap:&lt;/p>
&lt;pre>&lt;code>kubectl delete configmap/mymap &amp;amp;
configmap &amp;quot;mymap&amp;quot; deleted
jobs
[1]+ Running kubectl delete configmap/mymap
&lt;/code>&lt;/pre>&lt;p>Kubernetes will report back that the object has been deleted, however, it hasn’t been deleted in a traditional sense. Rather, it’s in the process of deletion. When we attempt to &lt;code>get&lt;/code> that object again, we discover the object has been modified to include the deletion timestamp.&lt;/p>
&lt;pre>&lt;code>kubectl get configmap/mymap -o yaml
apiVersion: v1
kind: ConfigMap
metadata:
creationTimestamp: &amp;quot;2020-10-22T21:30:18Z&amp;quot;
deletionGracePeriodSeconds: 0
deletionTimestamp: &amp;quot;2020-10-22T21:30:34Z&amp;quot;
finalizers:
- kubernetes
name: mymap
namespace: default
resourceVersion: &amp;quot;311456&amp;quot;
selfLink: /api/v1/namespaces/default/configmaps/mymap
uid: 93a37fed-23e3-45e8-b6ee-b2521db81638
&lt;/code>&lt;/pre>&lt;p>In short, what’s happened is that the object was updated, not deleted. That’s because Kubernetes saw that the object contained finalizers and put it into a read-only state. The deletion timestamp signals that the object can only be read, with the exception of removing the finalizer key updates. In other words, the deletion will not be complete until we edit the object and remove the finalizer.&lt;/p>
&lt;p>Here's a demonstration of using the &lt;code>patch&lt;/code> command to remove finalizers. If we want to delete an object, we can simply patch it on the command line to remove the finalizers. In this way, the deletion that was running in the background will complete and the object will be deleted. When we attempt to &lt;code>get&lt;/code> that configmap, it will be gone.&lt;/p>
&lt;pre>&lt;code>kubectl patch configmap/mymap \
--type json \
--patch='[ { &amp;quot;op&amp;quot;: &amp;quot;remove&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/metadata/finalizers&amp;quot; } ]'
configmap/mymap patched
[1]+ Done kubectl delete configmap/mymap
kubectl get configmap/mymap -o yaml
Error from server (NotFound): configmaps &amp;quot;mymap&amp;quot; not found
&lt;/code>&lt;/pre>&lt;p>Here's a state diagram for finalization:&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2021-05-14-using-finalizers-to-control-deletion/state-diagram-finalize.png"
alt="State diagram for finalize" width="617"/> &lt;figcaption>
&lt;p>State diagram for finalize&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>So, if you attempt to delete an object that has a finalizer on it, it will remain in finalization until the controller has removed the finalizer keys or the finalizers are removed using Kubectl. Once that finalizer list is empty, the object can actually be reclaimed by Kubernetes and put into a queue to be deleted from the registry.&lt;/p>
&lt;h2 id="owner-references">Owner References&lt;/h2>
&lt;p>Owner references describe how groups of objects are related. They are properties on resources that specify the relationship to one another, so entire trees of resources can be deleted.&lt;/p>
&lt;p>Finalizer rules are processed when there are owner references. An owner reference consists of a name and a UID. Owner references link resources within the same namespace, and it also needs a UID for that reference to work. Pods typically have owner references to the owning replica set. So, when deployments or stateful sets are deleted, then the child replica sets and pods are deleted in the process.&lt;/p>
&lt;p>Here are some examples of owner references and how they work. In the first example, we create a parent object first, then the child. The result is a very simple configmap that contains an owner reference to its parent:&lt;/p>
&lt;pre>&lt;code>cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: ConfigMap
metadata:
name: mymap-parent
EOF
CM_UID=$(kubectl get configmap mymap-parent -o jsonpath=&amp;quot;{.metadata.uid}&amp;quot;)
cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: ConfigMap
metadata:
name: mymap-child
ownerReferences:
- apiVersion: v1
kind: ConfigMap
name: mymap-parent
uid: $CM_UID
EOF
&lt;/code>&lt;/pre>&lt;p>Deleting the child object when an owner reference is involved does not delete the parent:&lt;/p>
&lt;pre>&lt;code>kubectl get configmap
NAME DATA AGE
mymap-child 0 12m4s
mymap-parent 0 12m4s
kubectl delete configmap/mymap-child
configmap &amp;quot;mymap-child&amp;quot; deleted
kubectl get configmap
NAME DATA AGE
mymap-parent 0 12m10s
&lt;/code>&lt;/pre>&lt;p>In this example, we re-created the parent-child configmaps from above. Now, when deleting from the parent (instead of the child) with an owner reference from the child to the parent, when we &lt;code>get&lt;/code> the configmaps, none are in the namespace:&lt;/p>
&lt;pre>&lt;code>kubectl get configmap
NAME DATA AGE
mymap-child 0 10m2s
mymap-parent 0 10m2s
kubectl delete configmap/mymap-parent
configmap &amp;quot;mymap-parent&amp;quot; deleted
kubectl get configmap
No resources found in default namespace.
&lt;/code>&lt;/pre>&lt;p>To sum things up, when there's an override owner reference from a child to a parent, deleting the parent deletes the children automatically. This is called &lt;code>cascade&lt;/code>. The default for cascade is &lt;code>true&lt;/code>, however, you can use the --cascade=orphan option for &lt;code>kubectl delete&lt;/code> to delete an object and orphan its children.&lt;/p>
&lt;p>In the following example, there is a parent and a child. Notice the owner references are still included. If I delete the parent using --cascade=orphan, the parent is deleted but the child still exists:&lt;/p>
&lt;pre>&lt;code>kubectl get configmap
NAME DATA AGE
mymap-child 0 13m8s
mymap-parent 0 13m8s
kubectl delete --cascade=orphan configmap/mymap-parent
configmap &amp;quot;mymap-parent&amp;quot; deleted
kubectl get configmap
NAME DATA AGE
mymap-child 0 13m21s
&lt;/code>&lt;/pre>&lt;p>The --cascade option links to the propagation policy in the API, which allows you to change the order in which objects are deleted within a tree. In the following example uses API access to craft a custom delete API call with the background propagation policy:&lt;/p>
&lt;pre>&lt;code>kubectl proxy --port=8080 &amp;amp;
Starting to serve on 127.0.0.1:8080
curl -X DELETE \
localhost:8080/api/v1/namespaces/default/configmaps/mymap-parent \
-d '{ &amp;quot;kind&amp;quot;:&amp;quot;DeleteOptions&amp;quot;, &amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;, &amp;quot;propagationPolicy&amp;quot;:&amp;quot;Background&amp;quot; }' \
-H &amp;quot;Content-Type: application/json&amp;quot;
{
&amp;quot;kind&amp;quot;: &amp;quot;Status&amp;quot;,
&amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
&amp;quot;metadata&amp;quot;: {},
&amp;quot;status&amp;quot;: &amp;quot;Success&amp;quot;,
&amp;quot;details&amp;quot;: { ... }
}
&lt;/code>&lt;/pre>&lt;p>Note that the propagation policy cannot be specified on the command line using kubectl. You have to specify it using a custom API call. Simply create a proxy, so you have access to the API server from the client, and execute a &lt;code>curl&lt;/code> command with just a URL to execute that &lt;code>delete&lt;/code> command.&lt;/p>
&lt;p>There are three different options for the propagation policy:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Foreground&lt;/code>: Children are deleted before the parent (post-order)&lt;/li>
&lt;li>&lt;code>Background&lt;/code>: Parent is deleted before the children (pre-order)&lt;/li>
&lt;li>&lt;code>Orphan&lt;/code>: Owner references are ignored&lt;/li>
&lt;/ul>
&lt;p>Keep in mind that when you delete an object and owner references have been specified, finalizers will be honored in the process. This can result in trees of objects persisting, and you end up with a partial deletion. At that point, you have to look at any existing owner references on your objects, as well as any finalizers, to understand what’s happening.&lt;/p>
&lt;h2 id="forcing-a-deletion-of-a-namespace">Forcing a Deletion of a Namespace&lt;/h2>
&lt;p>There's one situation that may require forcing finalization for a namespace. If you've deleted a namespace and you've cleaned out all of the objects under it, but the namespace still exists, deletion can be forced by updating the namespace subresource, &lt;code>finalize&lt;/code>. This informs the namespace controller that it needs to remove the finalizer from the namespace and perform any cleanup:&lt;/p>
&lt;pre>&lt;code>cat &amp;lt;&amp;lt;EOF | curl -X PUT \
localhost:8080/api/v1/namespaces/test/finalize \
-H &amp;quot;Content-Type: application/json&amp;quot; \
--data-binary @-
{
&amp;quot;kind&amp;quot;: &amp;quot;Namespace&amp;quot;,
&amp;quot;apiVersion&amp;quot;: &amp;quot;v1&amp;quot;,
&amp;quot;metadata&amp;quot;: {
&amp;quot;name&amp;quot;: &amp;quot;test&amp;quot;
},
&amp;quot;spec&amp;quot;: {
&amp;quot;finalizers&amp;quot;: null
}
}
EOF
&lt;/code>&lt;/pre>&lt;p>This should be done with caution as it may delete the namespace only and leave orphan objects within the, now non-exiting, namespace - a confusing state for Kubernetes. If this happens, the namespace can be re-created manually and sometimes the orphaned objects will re-appear under the just-created namespace which will allow manual cleanup and recovery.&lt;/p>
&lt;h2 id="key-takeaways">Key Takeaways&lt;/h2>
&lt;p>As these examples demonstrate, finalizers can get in the way of deleting resources in Kubernetes, especially when there are parent-child relationships between objects. Often, there is a reason for adding a finalizer into the code, so you should always investigate before manually deleting it. Owner references allow you to specify and remove trees of resources, although finalizers will be honored in the process. Finally, the propagation policy can be used to specify the order of deletion via a custom API call, giving you control over how objects are deleted. Now that you know a little more about how deletions work in Kubernetes, we recommend you try it out on your own, using a test cluster.&lt;/p>
&lt;div class="youtube-quote-sm">
&lt;iframe src="https://www.youtube.com/embed/F7-ZxWwf4sY" allowfullscreen title="Clean Up Your Room! What Does It Mean to Delete Something in K8s">&lt;/iframe>
&lt;/div></description></item><item><title>Blog: Kubernetes 1.21: Metrics Stability hits GA</title><link>https://kubernetes.io/blog/2021/04/23/kubernetes-release-1.21-metrics-stability-ga/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/23/kubernetes-release-1.21-metrics-stability-ga/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Han Kang (Google), Elana Hashman (Red Hat)&lt;/p>
&lt;p>Kubernetes 1.21 marks the graduation of the metrics stability framework and along with it, the first officially supported stable metrics. Not only do stable metrics come with supportability guarantees, the metrics stability framework brings escape hatches that you can use if you encounter problematic metrics.&lt;/p>
&lt;p>See the list of &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/test/instrumentation/testdata/stable-metrics-list.yaml">stable Kubernetes metrics here&lt;/a>&lt;/p>
&lt;h3 id="what-are-stable-metrics-and-why-do-we-need-them">What are stable metrics and why do we need them?&lt;/h3>
&lt;p>A stable metric is one which, from a consumption point of view, can be reliably consumed across a number of Kubernetes versions without risk of ingestion failure.&lt;/p>
&lt;p>Metrics stability is an ongoing community concern. Cluster monitoring infrastructure often assumes the stability of some control plane metrics, so we have introduced a mechanism for versioning metrics as a proper API, with stability guarantees around a formal metrics deprecation process.&lt;/p>
&lt;h3 id="what-are-the-stability-levels-for-metrics">What are the stability levels for metrics?&lt;/h3>
&lt;p>Metrics can currently have one of two stability levels: alpha or stable.&lt;/p>
&lt;p>&lt;em>Alpha metrics&lt;/em> have no stability guarantees; as such they can be modified or deleted at any time. At this time, all Kubernetes metrics implicitly fall into this category.&lt;/p>
&lt;p>&lt;em>Stable metrics&lt;/em> can be guaranteed to not change, except that the metric may become marked deprecated for a future Kubernetes version. By not change, we mean three things:&lt;/p>
&lt;ol>
&lt;li>the metric itself will not be deleted or renamed&lt;/li>
&lt;li>the type of metric will not be modified&lt;/li>
&lt;li>no labels can be added or removed from this metric&lt;/li>
&lt;/ol>
&lt;p>From an ingestion point of view, it is backwards-compatible to add or remove possible values for labels which already do exist, but not labels themselves. Therefore, adding or removing values from an existing label is permitted. Stable metrics can also be marked as deprecated for a future Kubernetes version, since this is tracked in a metadata field and does not actually change the metric itself.&lt;/p>
&lt;p>Removing or adding labels from stable metrics is not permitted. In order to add or remove a label from an existing stable metric, one would have to introduce a new metric and deprecate the stable one; otherwise this would violate compatibility agreements.&lt;/p>
&lt;h4 id="how-are-metrics-deprecated">How are metrics deprecated?&lt;/h4>
&lt;p>While deprecation policies only affect stability guarantees for stable metrics (and not alpha ones), deprecation information may be optionally provided on alpha metrics to help component owners inform users of future intent and assist with transition plans.&lt;/p>
&lt;p>A stable metric undergoing the deprecation process signals that the metric will eventually be deleted. The metrics deprecation lifecycle looks roughly like this (with each stage representing a Kubernetes release):&lt;/p>
&lt;p>&lt;img src="lifecycle-metric.png" alt="Stable metric → Deprecated metric → Hidden metric → Deletion">&lt;/p>
&lt;p>&lt;em>Deprecated metrics&lt;/em> have the same stability guarantees of their stable counterparts. If a stable metric is deprecated, then a deprecated stable metric is guaranteed to not change. When deprecating a stable metric, a future Kubernetes release is specified as the point from which the metric will be considered deprecated.&lt;/p>
&lt;p>Deprecated metrics will have their description text prefixed with a deprecation notice string “(Deprecated from x.y)” and a warning log will be emitted during metric registration, in the spirit of the official Kubernetes deprecation policy.&lt;/p>
&lt;p>Like their stable metric counterparts, deprecated metrics will be automatically registered to the metrics endpoint. On a subsequent release (when the metric's deprecatedVersion is equal to &lt;em>current_kubernetes_version - 4&lt;/em>)), a deprecated metric will become a &lt;em>hidden&lt;/em> metric. &lt;em>Hidden metrics&lt;/em> are not automatically registered, and hence are hidden by default from end users. These hidden metrics can be explicitly re-enabled for one release after they reach the hidden state, to provide a migration path for cluster operators.&lt;/p>
&lt;h4 id="as-an-owner-of-a-kubernetes-component-how-do-i-add-stable-metrics">As an owner of a Kubernetes component, how do I add stable metrics?&lt;/h4>
&lt;p>During metric instantiation, stability can be specified by setting the metadata field, StabilityLevel, to “Stable”. When a StabilityLevel is not explicitly set, metrics default to “Alpha” stability. Note that metrics which have fields determined at runtime cannot be marked as Stable. Stable metrics will be detected during static analysis during the pre-commit phase, and must be reviewed by sig-instrumentation.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-golang" data-lang="golang">&lt;span style="color:#a2f;font-weight:bold">var&lt;/span> metricDefinition = kubemetrics.CounterOpts{
Name: &lt;span style="color:#b44">&amp;#34;some_metric&amp;#34;&lt;/span>,
Help: &lt;span style="color:#b44">&amp;#34;some description&amp;#34;&lt;/span>,
StabilityLevel: kubemetrics.STABLE,
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>For more examples of setting metrics stability and deprecation, see the &lt;a href="http://bit.ly/metrics-stability">Metrics Stability KEP&lt;/a>.&lt;/p>
&lt;h3 id="how-do-i-get-involved">How do I get involved?&lt;/h3>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.
We offer a huge thank you to all the contributors in Kubernetes community who helped review the design and implementation of the project, including but not limited to the following:&lt;/p>
&lt;ul>
&lt;li>Han Kang (logicalhan)&lt;/li>
&lt;li>Frederic Branczyk (brancz)&lt;/li>
&lt;li>Marek Siarkowicz (serathius)&lt;/li>
&lt;li>Elana Hashman (ehashman)&lt;/li>
&lt;li>Solly Ross (DirectXMan12)&lt;/li>
&lt;li>Stefan Schimanski (sttts)&lt;/li>
&lt;li>David Ashpole (dashpole)&lt;/li>
&lt;li>Yuchen Zhou (yoyinzyc)&lt;/li>
&lt;li>Yu Yi (erain)&lt;/li>
&lt;/ul>
&lt;p>If you’re interested in getting involved with the design and development of instrumentation or any part of the Kubernetes metrics system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-instrumentation">Kubernetes Instrumentation Special Interest Group (SIG)&lt;/a>. We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Evolving Kubernetes networking with the Gateway API</title><link>https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/</link><pubDate>Thu, 22 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/22/evolving-kubernetes-networking-with-the-gateway-api/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Mark Church (Google), Harry Bagdi (Kong), Daneyon Hanson (Red Hat), Nick Young (VMware), Manuel Zapf (Traefik Labs)&lt;/p>
&lt;p>The Ingress resource is one of the many Kubernetes success stories. It created a &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">diverse ecosystem of Ingress controllers&lt;/a> which were used across hundreds of thousands of clusters in a standardized and consistent way. This standardization helped users adopt Kubernetes. However, five years after the creation of Ingress, there are signs of fragmentation into different but &lt;a href="https://dave.cheney.net/paste/ingress-is-dead-long-live-ingressroute.pdf">strikingly similar CRDs&lt;/a> and &lt;a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/">overloaded annotations&lt;/a>. The same portability that made Ingress pervasive also limited its future.&lt;/p>
&lt;p>It was at Kubecon 2019 San Diego when a passionate group of contributors gathered to discuss the &lt;a href="https://static.sched.com/hosted_files/kccncna19/a5/Kubecon%20San%20Diego%202019%20-%20Evolving%20the%20Kubernetes%20Ingress%20APIs%20to%20GA%20and%20Beyond%20%5BPUBLIC%5D.pdf">evolution of Ingress&lt;/a>. The discussion overflowed to the hotel lobby across the street and what came out of it would later be known as the &lt;a href="https://gateway-api.sigs.k8s.io">Gateway API&lt;/a>. This discussion was based on a few key assumptions:&lt;/p>
&lt;ol>
&lt;li>The API standards underlying route matching, traffic management, and service exposure are commoditized and provide little value to their implementers and users as custom APIs&lt;/li>
&lt;li>It’s possible to represent L4/L7 routing and traffic management through common core API resources&lt;/li>
&lt;li>It’s possible to provide extensibility for more complex capabilities in a way that does not sacrifice the user experience of the core API&lt;/li>
&lt;/ol>
&lt;h2 id="introducing-the-gateway-api">Introducing the Gateway API&lt;/h2>
&lt;p>This led to design principles that allow the Gateway API to improve upon Ingress:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Expressiveness&lt;/strong> - In addition to HTTP host/path matching and TLS, Gateway API can express capabilities like HTTP header manipulation, traffic weighting &amp;amp; mirroring, TCP/UDP routing, and other capabilities that were only possible in Ingress through custom annotations.&lt;/li>
&lt;li>&lt;strong>Role-oriented design&lt;/strong> - The API resource model reflects the separation of responsibilities that is common in routing and Kubernetes service networking.&lt;/li>
&lt;li>&lt;strong>Extensibility&lt;/strong> - The resources allow arbitrary configuration attachment at various layers within the API. This makes granular customization possible at the most appropriate places.&lt;/li>
&lt;li>&lt;strong>Flexible conformance&lt;/strong> - The Gateway API defines varying conformance levels - core (mandatory support), extended (portable if supported), and custom (no portability guarantee), known together as &lt;a href="https://gateway-api.sigs.k8s.io/concepts/guidelines/#conformance">flexible conformance&lt;/a>. This promotes a highly portable core API (like Ingress) that still gives flexibility for Gateway controller implementers.&lt;/li>
&lt;/ul>
&lt;h3 id="what-does-the-gateway-api-look-like">What does the Gateway API look like?&lt;/h3>
&lt;p>The Gateway API introduces a few new resource types:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.GatewayClass">GatewayClasses&lt;/a>&lt;/strong> are cluster-scoped resources that act as templates to explicitly define behavior for Gateways derived from them. This is similar in concept to StorageClasses, but for networking data-planes.&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.Gateway">Gateways&lt;/a>&lt;/strong> are the deployed instances of GatewayClasses. They are the logical representation of the data-plane which performs routing, which may be in-cluster proxies, hardware LBs, or cloud LBs.&lt;/li>
&lt;li>&lt;strong>Routes&lt;/strong> are not a single resource, but represent many different protocol-specific Route resources. The &lt;a href="https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.HTTPRoute">HTTPRoute&lt;/a> has matching, filtering, and routing rules that get applied to Gateways that can process HTTP and HTTPS traffic. Similarly, there are &lt;a href="https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.TCPRoute">TCPRoutes&lt;/a>, &lt;a href="https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.UDPRoute">UDPRoutes&lt;/a>, and &lt;a href="https://gateway-api.sigs.k8s.io/references/spec/#networking.x-k8s.io/v1alpha1.TLSRoute">TLSRoutes&lt;/a> which also have protocol-specific semantics. This model also allows the Gateway API to incrementally expand its protocol support in the future.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="gateway-api-resources.png" alt="The resources of the Gateway API">&lt;/p>
&lt;h3 id="gateway-controller-implementations">Gateway Controller Implementations&lt;/h3>
&lt;p>The good news is that although Gateway is in &lt;a href="https://github.com/kubernetes-sigs/gateway-api/releases">Alpha&lt;/a>, there are already several &lt;a href="https://gateway-api.sigs.k8s.io/references/implementations/">Gateway controller implementations&lt;/a> that you can run. Since it’s a standardized spec, the following example could be run on any of them and should function the exact same way. Check out &lt;a href="https://gateway-api.sigs.k8s.io/guides/getting-started/">getting started&lt;/a> to see how to install and use one of these Gateway controllers.&lt;/p>
&lt;h2 id="getting-hands-on-with-the-gateway-api">Getting Hands-on with the Gateway API&lt;/h2>
&lt;p>In the following example, we’ll demonstrate the relationships between the different API Resources and walk you through a common use case:&lt;/p>
&lt;ul>
&lt;li>Team foo has their app deployed in the foo Namespace. They need to control the routing logic for the different pages of their app.&lt;/li>
&lt;li>Team bar is running in the bar Namespace. They want to be able to do blue-green rollouts of their application to reduce risk.&lt;/li>
&lt;li>The platform team is responsible for managing the load balancer and network security of all the apps in the Kubernetes cluster.&lt;/li>
&lt;/ul>
&lt;p>The following foo-route does path matching to various Services in the foo Namespace and also has a default route to a 404 server. This exposes foo-auth and foo-home Services via &lt;code>foo.example.com/login&lt;/code> and &lt;code>foo.example.com/home&lt;/code> respectively.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HTTPRoute&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.x-k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo-route&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">gateway&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>external-https-prod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">hostnames&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;foo.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rules&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matches&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Prefix&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/login&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">forwardTo&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo-auth&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matches&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Prefix&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/home&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">forwardTo&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo-home&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matches&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Prefix&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">forwardTo&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo-404&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The bar team, operating in the bar Namespace of the same Kubernetes cluster, also wishes to expose their application to the internet, but they also want to control their own canary and blue-green rollouts. The following HTTPRoute is configured for the following behavior:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>For traffic to &lt;code>bar.example.com&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>Send 90% of the traffic to bar-v1&lt;/li>
&lt;li>Send 10% of the traffic to bar-v2&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>For traffic to &lt;code>bar.example.com&lt;/code> with the HTTP header &lt;code>env: canary&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>Send all the traffic to bar-v2&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="httproute.png" alt="The routing rules configured for the bar-v1 and bar-v2 Services">&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HTTPRoute&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.x-k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bar-route&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bar&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">gateway&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>external-https-prod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">hostnames&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;bar.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rules&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">forwardTo&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bar-v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">90&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bar-v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matches&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">forwardTo&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>bar-v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="route-and-gateway-binding">Route and Gateway Binding&lt;/h3>
&lt;p>So we have two HTTPRoutes matching and routing traffic to different Services. You might be wondering, where are these Services accessible? Through which networks or IPs are they exposed?&lt;/p>
&lt;p>How Routes are exposed to clients is governed by &lt;a href="https://gateway-api.sigs.k8s.io/concepts/api-overview/#route-binding">Route binding&lt;/a>, which describes how Routes and Gateways create a bidirectional relationship between each other. When Routes are bound to a Gateway it means their collective routing rules are configured on the underlying load balancers or proxies and the Routes are accessible through the Gateway. Thus, a Gateway is a logical representation of a networking data plane that can be configured through Routes.&lt;/p>
&lt;p>&lt;img src="route-binding.png" alt="How Routes bind with Gateways">&lt;/p>
&lt;h3 id="administrative-delegation">Administrative Delegation&lt;/h3>
&lt;p>The split between Gateway and Route resources allows the cluster administrator to delegate some of the routing configuration to individual teams while still retaining centralized control. The following Gateway resource exposes HTTPS on port 443 and terminates all traffic on the port with a certificate controlled by the cluster administrator.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Gateway&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.x-k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>prod-web&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">gatewayClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>acme-lb&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">listeners&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">protocol&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HTTPS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">443&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HTTPRoute&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">gateway&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>external-https-prod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespaces&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">from&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>All&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">tls&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">certificateRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admin-controlled-cert&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The following HTTPRoute shows how the Route can ensure it matches the Gateway's selector via it’s &lt;code>kind&lt;/code> (HTTPRoute) and resource labels (&lt;code>gateway=external-https-prod&lt;/code>).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#080;font-style:italic"># Matches the required kind selector on the Gateway&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>HTTPRoute&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.x-k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo-route&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>foo-ns&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Matches the required label selector on the Gateway&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">gateway&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>external-https-prod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#00f;font-weight:bold">...&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="role-oriented-design">Role Oriented Design&lt;/h3>
&lt;p>When you put it all together, you have a single load balancing infrastructure that can be safely shared by multiple teams. The Gateway API is not only a more expressive API for advanced routing, but is also a role-oriented API, designed for multi-tenant infrastructure. Its extensibility ensures that it will evolve for future use-cases while preserving portability. Ultimately these characteristics will allow the Gateway API to adapt to different organizational models and implementations well into the future.&lt;/p>
&lt;h3 id="try-it-out-and-get-involved">Try it out and get involved&lt;/h3>
&lt;p>There are many resources to check out to learn more.&lt;/p>
&lt;ul>
&lt;li>Check out the &lt;a href="https://gateway-api.sigs.k8s.io/guides/getting-started/">user guides&lt;/a> to see what use-cases can be addressed.&lt;/li>
&lt;li>Try out one of the &lt;a href="https://gateway-api.sigs.k8s.io/references/implementations/">existing Gateway controllers &lt;/a>&lt;/li>
&lt;li>Or &lt;a href="https://gateway-api.sigs.k8s.io/contributing/community/">get involved&lt;/a> and help design and influence the future of Kubernetes service networking!&lt;/li>
&lt;/ul></description></item><item><title>Blog: Graceful Node Shutdown Goes Beta</title><link>https://kubernetes.io/blog/2021/04/21/graceful-node-shutdown-beta/</link><pubDate>Wed, 21 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/21/graceful-node-shutdown-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> David Porter (Google), Mrunal Patel (Red Hat), and Tim Bannister (The Scale Factory)&lt;/p>
&lt;p>Graceful node shutdown, beta in 1.21, enables kubelet to gracefully evict pods during a node shutdown.&lt;/p>
&lt;p>Kubernetes is a distributed system and as such we need to be prepared for inevitable failures — nodes will fail, containers might crash or be restarted, and - ideally - your workloads will be able to withstand these catastrophic events.&lt;/p>
&lt;p>One of the common classes of issues are workload failures on node shutdown or restart. The best practice prior to bringing your node down is to &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/">safely drain and cordon your node&lt;/a>. This will ensure that all pods running on this node can safely be evicted. An eviction will ensure your pods can follow the expected &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">pod termination lifecycle&lt;/a> meaning receiving a SIGTERM in your container and/or running &lt;code>preStopHooks&lt;/code>.&lt;/p>
&lt;p>Prior to Kubernetes 1.20 (when graceful node shutdown was introduced as an alpha feature), safe node draining was not easy: it required users to manually take action and drain the node beforehand. If someone or something shut down your node without draining it first, most likely your pods would not be safely evicted from your node and shutdown abruptly. Other services talking to those pods might see errors due to the pods exiting abruptly. Some examples of this situation may be caused by a reboot due to security patches or preemption of short lived cloud compute instances.&lt;/p>
&lt;p>Kubernetes 1.21 brings graceful node shutdown to beta. Graceful node shutdown gives you more control over some of those unexpected shutdown situations. With graceful node shutdown, the kubelet is aware of underlying system shutdown events and can propagate these events to pods, ensuring containers can shut down as gracefully as possible. This gives the containers a chance to checkpoint their state or release back any resources they are holding.&lt;/p>
&lt;p>Note, that for the best availability, even with graceful node shutdown, you should still design your deployments to be resilient to node failures.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>On Linux, your system can shut down in many different situations. For example:&lt;/p>
&lt;ul>
&lt;li>A user or script running &lt;code>shutdown -h now&lt;/code> or &lt;code>systemctl poweroff&lt;/code> or &lt;code>systemctl reboot&lt;/code>.&lt;/li>
&lt;li>Physically pressing a power button on the machine.&lt;/li>
&lt;li>Stopping a VM instance on a cloud provider, e.g. &lt;code>gcloud compute instances stop&lt;/code> on GCP.&lt;/li>
&lt;li>A Preemptible VM or Spot Instance that your cloud provider can terminate unexpectedly, but with a brief warning.&lt;/li>
&lt;/ul>
&lt;p>Many of these situations can be unexpected and there is no guarantee that a cluster administrator drained the node prior to these events. With the graceful node shutdown feature, kubelet uses a systemd mechanism called &lt;a href="https://www.freedesktop.org/wiki/Software/systemd/inhibit">&amp;quot;Inhibitor Locks&amp;quot;&lt;/a> to allow draining in most cases. Using Inhibitor Locks, kubelet instructs systemd to postpone system shutdown for a specified duration, giving a chance for the node to drain and evict pods on the system.&lt;/p>
&lt;p>Kubelet makes use of this mechanism to ensure your pods will be terminated cleanly. When the kubelet starts, it acquires a systemd delay-type inhibitor lock. When the system is about to shut down, the kubelet can delay that shutdown for a configurable, short duration utilizing the delay-type inhibitor lock it acquired earlier. This gives your pods extra time to terminate. As a result, even during unexpected shutdowns, your application will receive a SIGTERM, &lt;a href="https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks">preStop hooks&lt;/a> will execute, and kubelet will properly update &lt;code>Ready&lt;/code> node condition and respective pod statuses to the api-server.&lt;/p>
&lt;p>For example, on a node with graceful node shutdown enabled, you can see that the inhibitor lock is taken by the kubelet:&lt;/p>
&lt;pre>&lt;code>kubelet-node ~ # systemd-inhibit --list
Who: kubelet (UID 0/root, PID 1515/kubelet)
What: shutdown
Why: Kubelet needs time to handle node shutdown
Mode: delay
1 inhibitors listed.
&lt;/code>&lt;/pre>&lt;p>One important consideration we took when designing this feature is that not all pods are created equal. For example, some of the pods running on a node such as a logging related daemonset should stay running as long as possible to capture important logs during the shutdown itself. As a result, pods are split into two categories: &amp;quot;regular&amp;quot; and &amp;quot;critical&amp;quot;. &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical">Critical pods&lt;/a> are those that have &lt;code>priorityClassName&lt;/code> set to &lt;code>system-cluster-critical&lt;/code> or &lt;code>system-node-critical&lt;/code>; all other pods are considered regular.&lt;/p>
&lt;p>In our example, the logging DaemonSet would run as a critical pod. During the graceful node shutdown, regular pods are terminated first, followed by critical pods. As an example, this would allow a critical pod associated with a logging daemonset to continue functioning, and collecting logs during the termination of regular pods.&lt;/p>
&lt;p>We will evaluate during the beta phase if we need more flexibility for different pod priority classes and add support if needed, please let us know if you have some scenarios in mind.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>Graceful node shutdown is controlled with the &lt;code>GracefulNodeShutdown&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates">feature gate&lt;/a> and is enabled by default in Kubernetes 1.21.&lt;/p>
&lt;p>You can configure the graceful node shutdown behavior using two kubelet configuration options: &lt;code>ShutdownGracePeriod&lt;/code> and &lt;code>ShutdownGracePeriodCriticalPods&lt;/code>. To configure these options, you edit the kubelet configuration file that is passed to kubelet via the &lt;code>--config&lt;/code> flag; for more details, refer to &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/">Set kubelet parameters via a configuration file&lt;/a>.&lt;/p>
&lt;p>During a shutdown, kubelet terminates pods in two phases. You can configure how long each of these phases lasts.&lt;/p>
&lt;ol>
&lt;li>Terminate regular pods running on the node.&lt;/li>
&lt;li>Terminate critical pods running on the node.&lt;/li>
&lt;/ol>
&lt;p>The settings that control the duration of shutdown are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>ShutdownGracePeriod&lt;/code>
&lt;ul>
&lt;li>Specifies the total duration that the node should delay the shutdown by. This is the total grace period for pod termination for both regular and critical pods.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>ShutdownGracePeriodCriticalPods&lt;/code>
&lt;ul>
&lt;li>Specifies the duration used to terminate critical pods during a node shutdown. This should be less than &lt;code>ShutdownGracePeriod&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For example, if &lt;code>ShutdownGracePeriod=30s&lt;/code>, and &lt;code>ShutdownGracePeriodCriticalPods=10s&lt;/code>, kubelet will delay the node shutdown by 30 seconds. During this time, the first 20 seconds (30-10) would be reserved for gracefully terminating normal pods, and the last 10 seconds would be reserved for terminating critical pods.&lt;/p>
&lt;p>Note that by default, both configuration options described above, &lt;code>ShutdownGracePeriod&lt;/code> and &lt;code>ShutdownGracePeriodCriticalPods&lt;/code> are set to zero, so you will need to configure them as appropriate for your environment to activate graceful node shutdown functionality.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;ul>
&lt;li>Read the &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown">documentation&lt;/a>&lt;/li>
&lt;li>Read the enhancement proposal, &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2000-graceful-node-shutdown">KEP 2000&lt;/a>&lt;/li>
&lt;li>View the &lt;a href="https://github.com/kubernetes/kubernetes/tree/release-1.21/pkg/kubelet/nodeshutdown">code&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>Your feedback is always welcome! SIG Node meets regularly and can be reached via &lt;a href="https://slack.k8s.io">Slack&lt;/a> (channel &lt;code>#sig-node&lt;/code>), or the SIG's &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#contact">mailing list&lt;/a>&lt;/p></description></item><item><title>Blog: Annotating Kubernetes Services for Humans</title><link>https://kubernetes.io/blog/2021/04/20/annotating-k8s-for-humans/</link><pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/20/annotating-k8s-for-humans/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Richard Li, Ambassador Labs&lt;/p>
&lt;p>Have you ever been asked to troubleshoot a failing Kubernetes service and struggled to find basic information about the service such as the source repository and owner?&lt;/p>
&lt;p>One of the problems as Kubernetes applications grow is the proliferation of services. As the number of services grows, developers start to specialize working with specific services. When it comes to troubleshooting, however, developers need to be able to find the source, understand the service and dependencies, and chat with the owning team for any service.&lt;/p>
&lt;h2 id="human-service-discovery">Human service discovery&lt;/h2>
&lt;p>Troubleshooting always begins with information gathering. While much attention has been paid to centralizing machine data (e.g., logs, metrics), much less attention has been given to the human aspect of service discovery. Who owns a particular service? What Slack channel does the team work on? Where is the source for the service? What issues are currently known and being tracked?&lt;/p>
&lt;h2 id="kubernetes-annotations">Kubernetes annotations&lt;/h2>
&lt;p>Kubernetes annotations are designed to solve exactly this problem. Oft-overlooked, Kubernetes annotations are designed to add metadata to Kubernetes objects. The Kubernetes documentation says annotations can “attach arbitrary non-identifying metadata to objects.” This means that annotations should be used for attaching metadata that is external to Kubernetes (i.e., metadata that Kubernetes won’t use to identify objects. As such, annotations can contain any type of data. This is a contrast to labels, which are designed for uses internal to Kubernetes. As such, label structure and values are &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set">constrained&lt;/a> so they can be efficiently used by Kubernetes.&lt;/p>
&lt;h2 id="kubernetes-annotations-in-action">Kubernetes annotations in action&lt;/h2>
&lt;p>Here is an example. Imagine you have a Kubernetes service for quoting, called the quote service. You can do the following:&lt;/p>
&lt;pre>&lt;code>kubectl annotate service quote a8r.io/owner=”@sally”
&lt;/code>&lt;/pre>&lt;p>In this example, we've just added an annotation called &lt;code>a8r.io/owner&lt;/code> with the value of @sally. Now, we can use &lt;code>kubectl describe&lt;/code> to get the information.&lt;/p>
&lt;pre>&lt;code>Name: quote
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: a8r.io/owner: @sally
Selector: app=quote
Type: ClusterIP
IP: 10.109.142.131
Port: http 80/TCP
TargetPort: 8080/TCP
Endpoints: &amp;lt;none&amp;gt;
Session Affinity: None
Events: &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;p>If you’re practicing GitOps (and you should be!) you’ll want to code these values directly into your Kubernetes manifest, e.g.,&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>quote&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">a8r.io/owner&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>“@sally”&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>http&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">targetPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>quote&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="a-convention-for-annotations">A Convention for Annotations&lt;/h2>
&lt;p>Adopting a common convention for annotations ensures consistency and understandability. Typically, you’ll want to attach the annotation to the service object, as services are the high-level resource that maps most clearly to a team’s responsibility. Namespacing your annotations is also very important. Here is one set of conventions, documented at &lt;a href="https://a8r.io">a8r.io&lt;/a>, and reproduced below:&lt;/p>
&lt;table>&lt;caption style="display: none;">Annotation convention for human-readable services&lt;/caption>
&lt;thead>
&lt;tr>
&lt;th>Annotation&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>a8r.io/description&lt;/code>&lt;/td>
&lt;td>Unstructured text description of the service for humans.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/owner&lt;/code>&lt;/td>
&lt;td>SSO username (GitHub), email address (linked to GitHub account), or unstructured owner description.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/chat&lt;/code>&lt;/td>
&lt;td>Slack channel, or link to external chat system.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/bugs&lt;/code>&lt;/td>
&lt;td>Link to external bug tracker.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/logs&lt;/code>&lt;/td>
&lt;td>Link to external log viewer.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/documentation&lt;/code>&lt;/td>
&lt;td>Link to external project documentation.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/repository&lt;/code>&lt;/td>
&lt;td>Link to external VCS repository.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/support&lt;/code>&lt;/td>
&lt;td>Link to external support center.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/runbook&lt;/code>&lt;/td>
&lt;td>Link to external project runbook.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/incidents&lt;/code>&lt;/td>
&lt;td>Link to external incident dashboard.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/uptime&lt;/code>&lt;/td>
&lt;td>Link to external uptime dashboard.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/performance&lt;/code>&lt;/td>
&lt;td>Link to external performance dashboard.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>a8r.io/dependencies&lt;/code>&lt;/td>
&lt;td>Unstructured text describing the service dependencies for humans.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="visualizing-annotations-service-catalogs">Visualizing annotations: Service Catalogs&lt;/h2>
&lt;p>As the number of microservices and annotations proliferate, running &lt;code>kubectl describe&lt;/code> can get tedious. Moreover, using &lt;code>kubectl describe&lt;/code> requires every developer to have some direct access to the Kubernetes cluster. Over the past few years, service catalogs have gained greater visibility in the Kubernetes ecosystem. Popularized by tools such as &lt;a href="https://shopify.engineering/scaling-mobile-development-by-treating-apps-as-services">Shopify's ServicesDB&lt;/a> and &lt;a href="https://dzone.com/articles/modeling-microservices-at-spotify-with-petter-mari">Spotify's System Z&lt;/a>, service catalogs are internally-facing developer portals that present critical information about microservices.&lt;/p>
&lt;p>Note that these service catalogs should not be confused with the &lt;a href="https://svc-cat.io/">Kubernetes Service Catalog project&lt;/a>. Built on the Open Service Broker API, the Kubernetes Service Catalog enables Kubernetes operators to plug in different services (e.g., databases) to their cluster.&lt;/p>
&lt;h2 id="annotate-your-services-now-and-thank-yourself-later">Annotate your services now and thank yourself later&lt;/h2>
&lt;p>Much like implementing observability within microservice systems, you often don’t realize that you need human service discovery until it’s too late. Don't wait until something is on fire in production to start wishing you had implemented better metrics and also documented how to get in touch with the part of your organization that looks after it.&lt;/p>
&lt;p>There's enormous benefits to building an effective “version 0” service: a &lt;a href="https://containerjournal.com/topics/container-management/dancing-skeleton-apis-and-microservices/">&lt;em>dancing skeleton&lt;/em>&lt;/a> application with a thin slice of complete functionality that can be deployed to production with a minimal yet effective continuous delivery pipeline.&lt;/p>
&lt;p>Adding service annotations should be an essential part of your “version 0” for all of your services. Add them now, and you’ll thank yourself later.&lt;/p></description></item><item><title>Blog: Defining Network Policy Conformance for Container Network Interface (CNI) providers</title><link>https://kubernetes.io/blog/2021/04/20/defining-networkpolicy-conformance-cni-providers/</link><pubDate>Tue, 20 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/20/defining-networkpolicy-conformance-cni-providers/</guid><description>
&lt;p>Authors: Matt Fenwick (Synopsys), Jay Vyas (VMWare), Ricardo Katz, Amim Knabben (Loadsmart), Douglas Schilling Landgraf (Red Hat), Christopher Tomkins (Tigera)&lt;/p>
&lt;p>Special thanks to Tim Hockin and Bowie Du (Google), Dan Winship and Antonio Ojea (Red Hat),
Casey Davenport and Shaun Crampton (Tigera), and Abhishek Raut and Antonin Bas (VMware) for
being supportive of this work, and working with us to resolve issues in different Container Network Interfaces (CNIs) over time.&lt;/p>
&lt;p>A brief conversation around &amp;quot;node local&amp;quot; Network Policies in April of 2020 inspired the creation of a NetworkPolicy subproject from SIG Network. It became clear that as a community,
we need a rock-solid story around how to do pod network security on Kubernetes, and this story needed a community around it, so as to grow the cultural adoption of enterprise security patterns in K8s.&lt;/p>
&lt;p>In this post we'll discuss:&lt;/p>
&lt;ul>
&lt;li>Why we created a subproject for &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Network Policies&lt;/a>&lt;/li>
&lt;li>How we changed the Kubernetes e2e framework to &lt;code>visualize&lt;/code> NetworkPolicy implementation of your CNI provider&lt;/li>
&lt;li>The initial results of our comprehensive NetworkPolicy conformance validator, &lt;em>Cyclonus&lt;/em>, built around these principles&lt;/li>
&lt;li>Improvements subproject contributors have made to the NetworkPolicy user experience&lt;/li>
&lt;/ul>
&lt;h2 id="why-we-created-a-subproject-for-networkpolicies">Why we created a subproject for NetworkPolicies&lt;/h2>
&lt;p>In April of 2020 it was becoming clear that many CNIs were emerging, and many vendors
implement these CNIs in subtly different ways. Users were beginning to express a little bit
of confusion around how to implement policies for different scenarios, and asking for new features.
It was clear that we needed to begin unifying the way we think about Network Policies
in Kubernetes, to avoid API fragmentation and unnecessary complexity.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;ul>
&lt;li>In order to be flexible to the user’s environment, Calico as a CNI provider can be run using IPIP or VXLAN mode, or without encapsulation overhead. CNIs such as Antrea
and Cilium offer similar configuration options as well.&lt;/li>
&lt;li>Some CNI plugins offer iptables for NetworkPolicies amongst other options, whereas other CNIs use a completely
different technology stack (for example, the Antrea project uses Open vSwitch rules).&lt;/li>
&lt;li>Some CNI plugins only implement a subset of the Kubernetes NetworkPolicy API, and some a superset. For example, certain plugins don't support the
ability to target a named port; others don't work with certain IP address types, and there are diverging semantics for similar policy types.&lt;/li>
&lt;li>Some CNI plugins combine with OTHER CNI plugins in order to implement NetworkPolicies (canal), some CNI's might mix implementations (multus), and some clouds do routing separately from NetworkPolicy implementation.&lt;/li>
&lt;/ul>
&lt;p>Although this complexity is to some extent necessary to support different environments, end-users find that they need to follow a multistep process to implement Network Policies to secure their applications:&lt;/p>
&lt;ul>
&lt;li>Confirm that their network plugin supports NetworkPolicies (some don't, such as Flannel)&lt;/li>
&lt;li>Confirm that their cluster's network plugin supports the specific NetworkPolicy features that they are interested in (again, the named port or port range examples come to mind here)&lt;/li>
&lt;li>Confirm that their application's Network Policy definitions are doing the right thing&lt;/li>
&lt;li>Find out the nuances of a vendor's implementation of policy, and check whether or not that implementation has a CNI neutral implementation (which is sometimes adequate for users)&lt;/li>
&lt;/ul>
&lt;p>The NetworkPolicy project in upstream Kubernetes aims at providing a community where
people can learn about, and contribute to, the Kubernetes NetworkPolicy API and the surrounding ecosystem.&lt;/p>
&lt;h2 id="the-first-step-a-validation-framework-for-networkpolicies-that-was-intuitive-to-use-and-understand">The First step: A validation framework for NetworkPolicies that was intuitive to use and understand&lt;/h2>
&lt;p>The Kubernetes end to end suite has always had NetworkPolicy tests, but these weren't
run in CI, and the way they were implemented didn't provide holistic, easily consumable
information about how a policy was working in a cluster.
This is because the original tests didn't provide any kind of visual summary of connectivity
across a cluster. We thus initially set out to make it easy to confirm CNI support for NetworkPolicies by
making the end to end tests (which are often used by administrators or users to diagnose cluster conformance) easy to interpret.&lt;/p>
&lt;p>To solve the problem of confirming that CNIs support the basic features most users care about
for a policy, we built a new NetworkPolicy validation tool into the Kubernetes e2e
framework which allows for visual inspection of policies and their effect on a standard set of pods in a cluster.
For example, take the following test output. We found a bug in
&lt;a href="https://github.com/ovn-org/ovn-kubernetes/issues/1782">OVN Kubernetes&lt;/a>. This bug has now been resolved. With this tool the bug was really
easy to characterize, wherein certain policies caused a state-modification that,
later on, caused traffic to incorrectly be blocked (even after all Network Policies were deleted from the cluster).&lt;/p>
&lt;p>This is the network policy for the test in question:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">creationTimestamp&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">null&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>allow-ingress-port-80&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ingress&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>serve-80-tcp&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">podSelector&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{}&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>These are the expected connectivity results. The test setup is 9 pods (3 namespaces: x, y, and z;
and 3 pods in each namespace: a, b, and c); each pod runs a server on the same port and protocol
that can be reached through HTTP calls in the absence of network policies. Connectivity is verified
by using the &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/test/images/agnhost">agnhost&lt;/a> network utility to issue HTTP calls on a port and protocol that other pods are
expected to be serving. A test scenario first
runs a connectivity check to ensure that each pod can reach each other pod, for 81 (= 9 x 9) data
points. This is the &amp;quot;control&amp;quot;. Then perturbations are applied, depending on the test scenario:
policies are created, updated, and deleted; labels are added and removed from pods and namespaces,
and so on. After each change, the connectivity matrix is recollected and compared to the expected
connectivity.&lt;/p>
&lt;p>These results give a visual indication of connectivity in a simple matrix. Going down the leftmost column is the &amp;quot;source&amp;quot;
pod, or the pod issuing the request; going across the topmost row is the &amp;quot;destination&amp;quot; pod, or the pod
receiving the request. A &lt;code>.&lt;/code> means that the connection was allowed; an &lt;code>X&lt;/code> means the connection was
blocked. For example:&lt;/p>
&lt;pre>&lt;code>Nov 4 16:58:43.449: INFO: expected:
- x/a x/b x/c y/a y/b y/c z/a z/b z/c
x/a . . . . . . . . .
x/b . . . . . . . . .
x/c . . . . . . . . .
y/a . . . . . . . . .
y/b . . . . . . . . .
y/c . . . . . . . . .
z/a . . . . . . . . .
z/b . . . . . . . . .
z/c . . . . . . . . .
&lt;/code>&lt;/pre>&lt;p>Below are the observed connectivity results in the case of the OVN Kubernetes bug. Notice how the top three rows indicate that
all requests from namespace x regardless of pod and destination were blocked. Since these
experimental results do not match the expected results, a failure will be reported. Note
how the specific pattern of failure provides clear insight into the nature of the problem --
since all requests from a specific namespace fail, we have a clear clue to start our
investigation.&lt;/p>
&lt;pre>&lt;code>Nov 4 16:58:43.449: INFO: observed:
- x/a x/b x/c y/a y/b y/c z/a z/b z/c
x/a X X X X X X X X X
x/b X X X X X X X X X
x/c X X X X X X X X X
y/a . . . . . . . . .
y/b . . . . . . . . .
y/c . . . . . . . . .
z/a . . . . . . . . .
z/b . . . . . . . . .
z/c . . . . . . . . .
&lt;/code>&lt;/pre>&lt;p>This was one of our earliest wins in the Network Policy group, as we were able to
identify and work with the OVN Kubernetes group to fix a bug in egress policy processing.&lt;/p>
&lt;p>However, even though this tool has made it easy to validate roughly 30 common scenarios,
it doesn't validate &lt;em>all&lt;/em> Network Policy scenarios - because there are an enormous number of possible
permutations that one might create (technically, we might say this number is
infinite given that there's an infinite number of possible namespace/pod/port/protocol variations one can create).&lt;/p>
&lt;p>Once these tests were in play, we worked with the Upstream SIG Network and SIG Testing communities
(thanks to Antonio Ojea and Ben Elder) to put a testgrid Network Policy job in place. This job
continuously runs the entire suite of Network Policy tests against
&lt;a href="https://testgrid.k8s.io/sig-network-gce#presubmit-network-policies,%20google-gce">GCE with Calico as a Network Policy provider&lt;/a>.&lt;/p>
&lt;p>Part of our role as a subproject is to help make sure that, when these tests break, we can help triage them effectively.&lt;/p>
&lt;h2 id="cyclonus">Cyclonus: The next step towards Network Policy conformance&lt;/h2>
&lt;p>Around the time that we were finishing the validation work, it became clear from the community that,
in general, we needed to solve the overall problem of testing ALL possible Network Policy implementations.
For example, a KEP was recently written which introduced the concept of micro versioning to
Network Policies to accommodate &lt;a href="https://github.com/kubernetes/enhancements/pull/2137/files">describing this at the API level&lt;/a>, by Dan Winship.&lt;/p>
&lt;p>In response to this increasingly obvious need to comprehensively evaluate Network
Policy implementations from all vendors, Matt Fenwick decided to evolve our approach to Network Policy validation again by creating Cyclonus.&lt;/p>
&lt;p>Cyclonus is a comprehensive Network Policy fuzzing tool which verifies a CNI provider
against hundreds of different Network Policy scenarios, by defining similar truth table/policy
combinations as demonstrated in the end to end tests, while also providing a hierarchical
representation of policy &amp;quot;categories&amp;quot;. We've found some interesting nuances and issues
in almost every CNI we've tested so far, and have even contributed some fixes back.&lt;/p>
&lt;p>To perform a Cyclonus validation run, you create a Job manifest similar to:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cyclonus&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ./cyclonus&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- generate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --perturbation-wait-seconds=15&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --server-protocol=tcp,udp&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cyclonus&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>IfNotPresent&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mfenwick100/cyclonus:latest&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">serviceAccount&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cyclonus&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Cyclonus outputs a report of all the test cases it will run:&lt;/p>
&lt;pre>&lt;code>test cases to run by tag:
- target: 6
- peer-ipblock: 4
- udp: 16
- delete-pod: 1
- conflict: 16
- multi-port/protocol: 14
- ingress: 51
- all-pods: 14
- egress: 51
- all-namespaces: 10
- sctp: 10
- port: 56
- miscellaneous: 22
- direction: 100
- multi-peer: 0
- any-port-protocol: 2
- set-namespace-labels: 1
- upstream-e2e: 0
- allow-all: 6
- namespaces-by-label: 6
- deny-all: 10
- pathological: 6
- action: 6
- rule: 30
- policy-namespace: 4
- example: 0
- tcp: 16
- target-namespace: 3
- named-port: 24
- update-policy: 1
- any-peer: 2
- target-pod-selector: 3
- IP-block-with-except: 2
- pods-by-label: 6
- numbered-port: 28
- protocol: 42
- peer-pods: 20
- create-policy: 2
- policy-stack: 0
- any-port: 14
- delete-namespace: 1
- delete-policy: 1
- create-pod: 1
- IP-block-no-except: 2
- create-namespace: 1
- set-pod-labels: 1
testing 112 cases
&lt;/code>&lt;/pre>&lt;p>Note that Cyclonus tags its tests based on the type of policy being created, because
the policies themselves are auto-generated, and thus have no meaningful names to be recognized by.&lt;/p>
&lt;p>For each test, Cyclonus outputs a truth table, which is again similar to that of the
E2E tests, along with the policy being validated:&lt;/p>
&lt;pre>&lt;code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
creationTimestamp: null
name: base
namespace: x
spec:
egress:
- ports:
- port: 81
to:
- namespaceSelector:
matchExpressions:
- key: ns
operator: In
values:
- &amp;quot;y&amp;quot;
- z
podSelector:
matchExpressions:
- key: pod
operator: In
values:
- a
- b
- ports:
- port: 53
protocol: UDP
ingress:
- from:
- namespaceSelector:
matchExpressions:
- key: ns
operator: In
values:
- x
- &amp;quot;y&amp;quot;
podSelector:
matchExpressions:
- key: pod
operator: In
values:
- b
- c
ports:
- port: 80
protocol: TCP
podSelector:
matchLabels:
pod: a
policyTypes:
- Ingress
- Egress
0 wrong, 0 ignored, 81 correct
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| TCP/80 | X/A | X/B | X/C | Y/A | Y/B | Y/C | Z/A | Z/B | Z/C |
| TCP/81 | | | | | | | | | |
| UDP/80 | | | | | | | | | |
| UDP/81 | | | | | | | | | |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| x/a | X | X | X | X | X | X | X | X | X |
| | X | X | X | . | . | X | . | . | X |
| | X | X | X | X | X | X | X | X | X |
| | X | X | X | X | X | X | X | X | X |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| x/b | . | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| x/c | . | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| y/a | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| y/b | . | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| y/c | . | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| z/a | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| z/b | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
| z/c | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
| | X | . | . | . | . | . | . | . | . |
+--------+-----+-----+-----+-----+-----+-----+-----+-----+-----+
&lt;/code>&lt;/pre>&lt;p>Both Cyclonus and the e2e tests use the same strategy to validate a Network Policy - probing pods over TCP or UDP, with
SCTP support available as well for CNIs that support it (such as Calico).&lt;/p>
&lt;p>As examples of how we use Cyclonus to help make CNI implementations better from a Network Policy perspective, you can see the following issues:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/vmware-tanzu/antrea/issues/1764">Antrea: NetworkPolicy: unable to allow ingress by CIDR&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/projectcalico/libcalico-go/pull/1373">Calico: default missing protocol to TCP; don't let single port overwrite all ports&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cilium/cilium/issues/14678">Cilium: Egress Network Policy allows traffic that should be denied&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The good news is that Antrea and Calico have already merged fixes for all the issues found and other CNI providers are working on it,
with the support of SIG Network and the Network Policy subproject.&lt;/p>
&lt;p>Are you interested in verifying NetworkPolicy functionality on your cluster?
(if you care about security or offer multi-tenant SaaS, you should be)
If so, you can run the upstream end to end tests, or Cyclonus, or both.&lt;/p>
&lt;ul>
&lt;li>If you're just getting started with NetworkPolicies and want to simply
verify the &amp;quot;common&amp;quot; NetworkPolicy cases that most CNIs should be
implementing correctly, in a way that is quick to diagnose, then you're
better off running the e2e tests only.&lt;/li>
&lt;li>If you are deeply curious about your CNI provider's NetworkPolicy
implementation, and want to verify it: use Cyclonus.&lt;/li>
&lt;li>If you want to test &lt;em>hundreds&lt;/em> of policies, and evaluate your CNI plugin
for comprehensive functionality, for deep discovery of potential security
holes: use Cyclonus, and also consider running end-to-end cluster tests.&lt;/li>
&lt;li>If you're thinking of getting involved with the upstream NetworkPolicy efforts:
use Cyclonus, and read at least an outline of which e2e tests are relevant.&lt;/li>
&lt;/ul>
&lt;h2 id="where-to-start-with-networkpolicy-testing">Where to start with NetworkPolicy testing?&lt;/h2>
&lt;ul>
&lt;li>Cyclonus is easy to run on your cluster, check out the &lt;a href="https://github.com/mattfenwick/cyclonus#run-as-a-kubernetes-job">instructions on github&lt;/a>,
and determine whether &lt;em>your&lt;/em> specific CNI configuration is fully conformant to the hundreds of different
Kubernetes Network Policy API constructs.&lt;/li>
&lt;li>Alternatively, you can use a tool like &lt;a href="https://github.com/vmware-tanzu/sonobuoy">sonobuoy&lt;/a>
to run the existing E2E tests in Kubernetes, with the &lt;code>--ginkgo.focus=NetworkPolicy&lt;/code> flag.
Make sure that you use the K8s conformance image for K8s 1.21 or above (for example, by using the &lt;code>--kube-conformance-image-version v1.21.0&lt;/code> flag),
as older images will not have the &lt;em>new&lt;/em> Network Policy tests in them.&lt;/li>
&lt;/ul>
&lt;h2 id="improvements-to-the-networkpolicy-api-and-user-experience">Improvements to the NetworkPolicy API and user experience&lt;/h2>
&lt;p>In addition to cleaning up the validation story for CNI plugins that implement NetworkPolicies,
subproject contributors have also spent some time improving the Kubernetes NetworkPolicy API for a few commonly requested features.
After months of deliberation, we eventually settled on a few core areas for improvement:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Port Range policies: We now allow you to specify a &lt;em>range&lt;/em> of ports for a policy.
This allows users interested in scenarios like FTP or virtualization to enable advanced policies.
The port range option for network policies will be available to use in Kubernetes 1.21.
Read more in &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#targeting-a-range-of-ports">targeting a range of ports&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Namespace as name policies: Allowing users in Kubernetes &amp;gt;= 1.21 to target namespaces using names,
when building Network Policy objects. This was done in collaboration with Jordan Liggitt and Tim Hockin on the API Machinery side.
This change allowed us to improve the Network Policy user experience without actually
changing the API! For more details, you can read
&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#automatic-labelling">Automatic labelling&lt;/a> in the page about Namespaces.
The TL,DR; is that for Kubernetes 1.21 and later, &lt;strong>all namespaces&lt;/strong> have the following label added by default:&lt;/p>
&lt;pre>&lt;code>kubernetes.io/metadata.name: &amp;lt;name-of-namespace&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>This means you can write a namespace policy against this namespace, even if you can't edit its labels.
For example, this policy, will 'just work', without needing to run a command such as &lt;code>kubectl edit namespace&lt;/code>.
In fact, it will even work if you can't edit or view this namespace's data at all, because of the magic of API server defaulting.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NetworkPolicy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-network-policy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">podSelector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">role&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>db&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">policyTypes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- Ingress&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Allow inbound traffic to Pods labelled role=db, in the namespace &amp;#39;default&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># provided that the source is a Pod in the namespace &amp;#39;my-namespace&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ingress&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">from&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">namespaceSelector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kubernetes.io/metadata.name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-namespace&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="results">Results&lt;/h2>
&lt;p>In our tests, we found that:&lt;/p>
&lt;ul>
&lt;li>Antrea and Calico are at a point where they support all of cyclonus's scenarios, modulo a few very minor tweaks which we've made.&lt;/li>
&lt;li>Cilium also conformed to the majority of the policies, outside known features that aren't fully supported (for example, related to the way Cilium deals with pod CIDR policies).&lt;/li>
&lt;/ul>
&lt;p>If you are a CNI provider and interested in helping us to do a better job curating large tests of network policies, please reach out! We are continuing to curate the Network Policy conformance results from Cyclonus &lt;a href="https://raw.githubusercontent.com/K8sbykeshed/cyclonus-artifacts/">here&lt;/a>, but
we are not capable of maintaining all of the subtleties in NetworkPolicy testing data on our own. For now, we use github actions and Kind to test in CI.&lt;/p>
&lt;h2 id="the-future">The Future&lt;/h2>
&lt;p>We're also working on some improvements for the future of Network Policies, including:&lt;/p>
&lt;ul>
&lt;li>Fully qualified Domain policies: The Google Cloud team created a prototype (which
we are really excited about) of &lt;a href="https://github.com/GoogleCloudPlatform/gke-fqdnnetworkpolicies-golang">FQDN policies&lt;/a>.
This tool uses the Network Policy API to enforce policies against L7 URLs, by finding
their IPs and blocking them proactively when requests are made.&lt;/li>
&lt;li>Cluster Administrative policies: We're working hard at enabling &lt;em>administrative&lt;/em> or
&lt;em>cluster scoped&lt;/em> Network Policies for the future. These are being presented iteratively to the NetworkPolicy subproject.
You can read about them here in &lt;a href="https://docs.google.com/presentation/d/1Jk86jtS3TcGAugVSM_I4Yds5ukXFJ4F1ZCvxN5v2BaY/">Cluster Scoped Network Policy&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>The Network Policy subproject meets on mondays at 4PM EST. For details, check out the
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-network">SIG Network community repo&lt;/a>. We'd love
to hang out with you, hack on stuff, and help you adopt K8s Network Policies for your cluster wherever possible.&lt;/p>
&lt;h3 id="a-quick-note-on-user-feedback">A quick note on User Feedback&lt;/h3>
&lt;p>We've gotten a lot of ideas and feedback from users on Network Policies. A lot of people have interesting ideas about Network Policies,
but we've found that as a subproject, very few people were deeply interested in implementing these ideas to the full extent.&lt;/p>
&lt;p>Almost every change to the NetworkPolicy API includes weeks or months of discussion to cover different cases, and ensure no CVEs are being introduced. Thus, long term ownership
is the biggest impediment in improving the NetworkPolicy user experience for us, over time.&lt;/p>
&lt;ul>
&lt;li>We've documented a lot of the history of the Network Policy dialogue &lt;a href="https://github.com/jayunit100/network-policy-subproject/blob/master/history.md">here&lt;/a>.&lt;/li>
&lt;li>We've also taken a poll of users, for what they'd like to see in the Network Policy API &lt;a href="https://github.com/jayunit100/network-policy-subproject/blob/master/p0_user_stories.md">here&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>We encourage anyone to provide us with feedback, but our most pressing issues right now
involve finding &lt;em>long term owners to help us drive changes&lt;/em>.&lt;/p>
&lt;p>This doesn't require a lot of technical knowledge, but rather, just a long term commitment to helping us stay organized, do paperwork,
and iterate through the many stages of the K8s feature process. If you want to help us and get involved, please reach out on the SIG Network mailing list, or in the SIG Network room in the k8s.io slack channel!&lt;/p>
&lt;p>Anyone can put an oar in the water and help make NetworkPolices better!&lt;/p></description></item><item><title>Blog: Introducing Indexed Jobs</title><link>https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/</link><pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Aldo Culquicondor (Google)&lt;/p>
&lt;p>Once you have containerized a non-parallel &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Job&lt;/a>,
it is quite easy to get it up and running on Kubernetes without modifications to
the binary. In most cases, when running parallel distributed Jobs, you had
to set a separate system to partition the work among the workers. For
example, you could set up a task queue to &lt;a href="https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/">assign one work item to each
Pod&lt;/a> or &lt;a href="https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/">multiple items
to each Pod until the queue is emptied&lt;/a>.&lt;/p>
&lt;p>The Kubernetes 1.21 release introduces a new field to control Job &lt;em>completion mode&lt;/em>,
a configuration option that allows you to control how Pod completions affect the
overall progress of a Job, with two possible options (for now):&lt;/p>
&lt;ul>
&lt;li>&lt;code>NonIndexed&lt;/code> (default): the Job is considered complete when there has been
a number of successfully completed Pods equal to the specified number in
&lt;code>.spec.completions&lt;/code>. In other words, each Pod completion is homologous to
each other. Any Job you might have created before the introduction of
completion modes is implicitly NonIndexed.&lt;/li>
&lt;li>&lt;code>Indexed&lt;/code>: the Job is considered complete when there is one successfully
completed Pod associated with each index from 0 to &lt;code>.spec.completions-1&lt;/code>. The
index is exposed to each Pod in the &lt;code>batch.kubernetes.io/job-completion-index&lt;/code>
annotation and the &lt;code>JOB_COMPLETION_INDEX&lt;/code> environment variable.&lt;/li>
&lt;/ul>
&lt;p>You can start using Jobs with Indexed completion mode, or Indexed Jobs, for
short, to easily start parallel Jobs. Then, each worker Pod can have a statically
assigned partition of the data based on the index. This saves you from having to
set up a queuing system or even having to modify your binary!&lt;/p>
&lt;h2 id="creating-an-indexed-job">Creating an Indexed Job&lt;/h2>
&lt;p>To create an Indexed Job, you just have to add &lt;code>completionMode: Indexed&lt;/code> to the
Job spec and make use of the &lt;code>JOB_COMPLETION_INDEX&lt;/code> environment variable.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;sample-job&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">completions&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parallelism&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">completionMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Indexed&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#39;bash&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#39;-c&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#39;echo &amp;#34;My partition: ${JOB_COMPLETION_INDEX}&amp;#34;&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;docker.io/library/bash&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;sample-load&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that completion mode is an alpha feature in the 1.21 release. To be able to
use it in your cluster, make sure to enable the &lt;code>IndexedJob&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature
gate&lt;/a> on the
&lt;a href="docs/reference/command-line-tools-reference/kube-apiserver/">API server&lt;/a> and
the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">controller manager&lt;/a>.&lt;/p>
&lt;p>When you run the example, you will see that each of the three created Pods gets a
different completion index. For the user's convenience, the control plane sets the
&lt;code>JOB_COMPLETION_INDEX&lt;/code> environment variable, but you can choose to &lt;a href="https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">set your
own&lt;/a>
or &lt;a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">expose the index as a file&lt;/a>.&lt;/p>
&lt;p>See &lt;a href="https://kubernetes.io/docs/tasks/job/indexed-parallel-processing-static/">Indexed Job for parallel processing with static work
assignment&lt;/a> for a
step-by-step guide, and a few more examples.&lt;/p>
&lt;h2 id="future-plans">Future plans&lt;/h2>
&lt;p>SIG Apps envisions that there might be more completion modes that enable more
use cases for the Job API. We welcome you to open issues in
&lt;a href="https://github.com/kubernetes/kubernetes">kubernetes/kubernetes&lt;/a> with your
suggestions.&lt;/p>
&lt;p>In particular, we are considering an &lt;code>IndexedAndUnique&lt;/code> mode where the indexes
are not just available as annotation, but they are part of the Pod names,
similar to &lt;a class='glossary-tooltip' title='Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.' data-toggle='tooltip' data-placement='top' href='https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/' target='_blank' aria-label='StatefulSet'>StatefulSet&lt;/a>.
This should facilitate inter-Pod communication for tightly coupled Pods.
You can join the discussion in the &lt;a href="https://github.com/kubernetes/kubernetes/issues/99497">open issue&lt;/a>.&lt;/p>
&lt;h2 id="wrap-up">Wrap-up&lt;/h2>
&lt;p>Indexed Jobs allows you to statically partition work among the workers of your
parallel Jobs. SIG Apps hopes that this feature facilitates the migration of
more batch workloads to Kubernetes.&lt;/p></description></item><item><title>Blog: Volume Health Monitoring Alpha Update</title><link>https://kubernetes.io/blog/2021/04/16/volume-health-monitoring-alpha-update/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/16/volume-health-monitoring-alpha-update/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Xing Yang (VMware)&lt;/p>
&lt;p>The CSI Volume Health Monitoring feature, originally introduced in 1.19 has undergone a large update for the 1.21 release.&lt;/p>
&lt;h2 id="why-add-volume-health-monitoring-to-kubernetes">Why add Volume Health Monitoring to Kubernetes?&lt;/h2>
&lt;p>Without Volume Health Monitoring, Kubernetes has no knowledge of the state of the underlying volumes of a storage system after a PVC is provisioned and used by a Pod. Many things could happen to the underlying storage system after a volume is provisioned in Kubernetes. For example, the volume could be deleted by accident outside of Kubernetes, the disk that the volume resides on could fail, it could be out of capacity, the disk may be degraded which affects its performance, and so on. Even when the volume is mounted on a pod and used by an application, there could be problems later on such as read/write I/O errors, file system corruption, accidental unmounting of the volume outside of Kubernetes, etc. It is very hard to debug and detect root causes when something happened like this.&lt;/p>
&lt;p>Volume health monitoring can be very beneficial to Kubernetes users. It can communicate with the CSI driver to retrieve errors detected by the underlying storage system. PVC events can be reported up to the user to take action. For example, if the volume is out of capacity, they could request a volume expansion to get more space.&lt;/p>
&lt;h2 id="what-is-volume-health-monitoring">What is Volume Health Monitoring?&lt;/h2>
&lt;p>CSI Volume Health Monitoring allows CSI Drivers to detect abnormal volume conditions from the underlying storage systems and report them as events on PVCs or Pods.&lt;/p>
&lt;p>The Kubernetes components that monitor the volumes and report events with volume health information include the following:&lt;/p>
&lt;ul>
&lt;li>Kubelet, in addition to gathering the existing volume stats will watch the volume health of the PVCs on that node. If a PVC has an abnormal health condition, an event will be reported on the pod object using the PVC. If multiple pods are using the same PVC, events will be reported on all pods using that PVC.&lt;/li>
&lt;li>An &lt;a href="https://github.com/kubernetes-csi/external-health-monitor">External Volume Health Monitor Controller&lt;/a> watches volume health of the PVCs and reports events on the PVCs.&lt;/li>
&lt;/ul>
&lt;p>Note that the node side volume health monitoring logic was an external agent when this feature was first introduced in the Kubernetes 1.19 release. In Kubernetes 1.21, the node side volume health monitoring logic was moved from the external agent into the Kubelet, to avoid making duplicate CSI function calls. With this change in 1.21, a new alpha &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a> &lt;code>CSIVolumeHealth&lt;/code> was introduced for the volume health monitoring logic in Kubelet.&lt;/p>
&lt;p>Currently the Volume Health Monitoring feature is informational only as it only reports abnormal volume health events on PVCs or Pods. Users will need to check these events and manually fix the problems. This feature serves as a stepping stone towards programmatic detection and resolution of volume health issues by Kubernetes in the future.&lt;/p>
&lt;h2 id="how-do-i-use-volume-health-on-my-kubernetes-cluster">How do I use Volume Health on my Kubernetes Cluster?&lt;/h2>
&lt;p>To use the Volume Health feature, first make sure the CSI driver you are using supports this feature. Refer to this &lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">CSI drivers doc&lt;/a> to find out which CSI drivers support this feature.&lt;/p>
&lt;p>To enable Volume Health Monitoring from the node side, the alpha feature gate &lt;code>CSIVolumeHealth&lt;/code> needs to be enabled.&lt;/p>
&lt;p>If a CSI driver supports the Volume Health Monitoring feature from the controller side, events regarding abnormal volume conditions will be recorded on PVCs.&lt;/p>
&lt;p>If a CSI driver supports the Volume Health Monitoring feature from the controller side, user can also get events regarding node failures if the &lt;code>enable-node-watcher&lt;/code> flag is set to true when deploying the External Health Monitor Controller. When a node failure event is detected, an event will be reported on the PVC to indicate that pods using this PVC are on a failed node.&lt;/p>
&lt;p>If a CSI driver supports the Volume Health Monitoring feature from the node side, events regarding abnormal volume conditions will be recorded on pods using the PVCs.&lt;/p>
&lt;h2 id="as-a-storage-vendor-how-do-i-add-support-for-volume-health-to-my-csi-driver">As a storage vendor, how do I add support for volume health to my CSI driver?&lt;/h2>
&lt;p>Volume Health Monitoring includes two parts:&lt;/p>
&lt;ul>
&lt;li>An External Volume Health Monitoring Controller monitors volume health from the controller side.&lt;/li>
&lt;li>Kubelet monitors volume health from the node side.&lt;/li>
&lt;/ul>
&lt;p>For details, see the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI spec&lt;/a> and the &lt;a href="https://kubernetes-csi.github.io/docs/volume-health-monitor.html">Kubernetes-CSI Driver Developer Guide&lt;/a>.&lt;/p>
&lt;p>There is a sample implementation for volume health in &lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path">CSI host path driver&lt;/a>.&lt;/p>
&lt;h3 id="controller-side-volume-health-monitoring">Controller Side Volume Health Monitoring&lt;/h3>
&lt;p>To learn how to deploy the External Volume Health Monitoring controller, see &lt;a href="https://kubernetes-csi.github.io/docs/external-health-monitor-controller.html">CSI external-health-monitor-controller&lt;/a> in the CSI documentation.&lt;/p>
&lt;p>The External Health Monitor Controller calls either &lt;code>ListVolumes&lt;/code> or &lt;code>ControllerGetVolume&lt;/code> CSI RPC and reports VolumeConditionAbnormal events with messages on PVCs if abnormal volume conditions are detected. Only CSI drivers with &lt;code>LIST_VOLUMES&lt;/code> and &lt;code>VOLUME_CONDITION&lt;/code> controller capability or &lt;code>GET_VOLUME&lt;/code> and &lt;code>VOLUME_CONDITION&lt;/code> controller capability support Volume Health Monitoring in the external controller.&lt;/p>
&lt;p>To implement the volume health feature from the controller side, a CSI driver &lt;strong>must&lt;/strong> add support for the new controller capabilities.&lt;/p>
&lt;p>If a CSI driver supports &lt;code>LIST_VOLUMES&lt;/code> and &lt;code>VOLUME_CONDITION&lt;/code> controller capabilities, it &lt;strong>must&lt;/strong> implement controller RPC &lt;code>ListVolumes&lt;/code> and report the volume condition in the response.&lt;/p>
&lt;p>If a CSI driver supports &lt;code>GET_VOLUME&lt;/code> and &lt;code>VOLUME_CONDITION&lt;/code> controller capability, it &lt;strong>must&lt;/strong> implement controller PRC &lt;code>ControllerGetVolume&lt;/code> and report the volume condition in the response.&lt;/p>
&lt;p>If a CSI driver supports &lt;code>LIST_VOLUMES&lt;/code>, &lt;code>GET_VOLUME&lt;/code>, and &lt;code>VOLUME_CONDITION&lt;/code> controller capabilities, only &lt;code>ListVolumes&lt;/code> CSI RPC will be invoked by the External Health Monitor Controller.&lt;/p>
&lt;h3 id="node-side-volume-health-monitoring">Node Side Volume Health Monitoring&lt;/h3>
&lt;p>Kubelet calls &lt;code>NodeGetVolumeStats&lt;/code> CSI RPC and reports VolumeConditionAbnormal events with messages on Pods if abnormal volume conditions are detected. Only CSI drivers with &lt;code>VOLUME_CONDITION&lt;/code> node capability support Volume Health Monitoring in Kubelet.&lt;/p>
&lt;p>To implement the volume health feature from the node side, a CSI driver &lt;strong>must&lt;/strong> add support for the new node capabilities.&lt;/p>
&lt;p>If a CSI driver supports &lt;code>VOLUME_CONDITION&lt;/code> node capability, it &lt;strong>must&lt;/strong> report the volume condition in node RPC &lt;code>NodeGetVoumeStats&lt;/code>.&lt;/p>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>Depending on feedback and adoption, the Kubernetes team plans to push the CSI volume health implementation to beta in either 1.22 or 1.23.&lt;/p>
&lt;p>We are also exploring how to use volume health information for programmatic detection and automatic reconcile in Kubernetes.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>To learn the design details for Volume Health Monitoring, read the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor">Volume Health Monitor&lt;/a> enhancement proposal.&lt;/p>
&lt;p>The Volume Health Monitor controller source code is at &lt;a href="https://github.com/kubernetes-csi/external-health-monitor">https://github.com/kubernetes-csi/external-health-monitor&lt;/a>.&lt;/p>
&lt;p>There are also more details about volume health checks in the &lt;a href="https://kubernetes-csi.github.io/docs/">Container Storage Interface Documentation&lt;/a>.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>The &lt;a href="https://kubernetes.slack.com/messages/csi">Kubernetes Slack channel #csi&lt;/a> and any of the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">standard SIG Storage communication channels&lt;/a> are great mediums to reach out to the SIG Storage and the CSI team.&lt;/p>
&lt;p>We offer a huge thank you to the contributors who helped release this feature in 1.21. We want to thank Yuquan Ren (&lt;a href="https://github.com/nickrenren">NickrenREN&lt;/a>) who implemented the initial volume health monitor controller and agent in the external health monitor repo, thank Ran Xu (&lt;a href="https://github.com/fengzixu">fengzixu&lt;/a>) who moved the volume health monitoring logic from the external agent to Kubelet in 1.21, and we offer special thanks to the following people for their insightful reviews: David Ashpole (&lt;a href="https://github.com/dashpole">dashpole&lt;/a>), Michelle Au (&lt;a href="https://github.com/msau42">msau42&lt;/a>), David Eads (&lt;a href="https://github.com/deads2k">deads2k&lt;/a>), Elana Hashman (&lt;a href="https://github.com/ehashman">ehashman&lt;/a>), Seth Jennings (&lt;a href="https://github.com/sjenning">sjenning&lt;/a>), and Jiawei Wang (&lt;a href="https://github.com/Jiawei0227">Jiawei0227&lt;/a>).&lt;/p>
&lt;p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group&lt;/a> (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Three Tenancy Models For Kubernetes</title><link>https://kubernetes.io/blog/2021/04/15/three-tenancy-models-for-kubernetes/</link><pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/15/three-tenancy-models-for-kubernetes/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Ryan Bezdicek (Medtronic), Jim Bugwadia (Nirmata), Tasha Drew (VMware), Fei Guo (Alibaba), Adrian Ludwin (Google)&lt;/p>
&lt;p>Kubernetes clusters are typically used by several teams in an organization. In other cases, Kubernetes may be used to deliver applications to end users requiring segmentation and isolation of resources across users from different organizations. Secure sharing of Kubernetes control plane and worker node resources allows maximizing productivity and saving costs in both cases.&lt;/p>
&lt;p>The Kubernetes Multi-Tenancy Working Group is chartered with defining tenancy models for Kubernetes and making it easier to operationalize tenancy related use cases. This blog post, from the working group members, describes three common tenancy models and introduces related working group projects.&lt;/p>
&lt;p>We will also be presenting on this content and discussing different use cases at our Kubecon EU 2021 panel session, &lt;a href="https://sched.co/iE66">Multi-tenancy vs. Multi-cluster: When Should you Use What?&lt;/a>.&lt;/p>
&lt;h2 id="namespaces-as-a-service">Namespaces as a Service&lt;/h2>
&lt;p>With the &lt;em>namespaces-as-a-service&lt;/em> model, tenants share a cluster and tenant workloads are restricted to a set of Namespaces assigned to the tenant. The cluster control plane resources like the API server and scheduler, and worker node resources like CPU, memory, etc. are available for use across all tenants.&lt;/p>
&lt;p>To isolate tenant workloads, each namespace must also contain:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding">role bindings&lt;/a>:&lt;/strong> for controlling access to the namespace&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">network policies&lt;/a>:&lt;/strong> to prevent network traffic across tenants&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">resource quotas&lt;/a>:&lt;/strong> to limit usage and ensure fairness across tenants&lt;/li>
&lt;/ul>
&lt;p>With this model, tenants share cluster-wide resources like ClusterRoles and CustomResourceDefinitions (CRDs) and hence cannot create or update these cluster-wide resources.&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/blog/2020/08/14/introducing-hierarchical-namespaces/">Hierarchical Namespace Controller (HNC)&lt;/a> project makes it easier to manage namespace based tenancy by allowing users to create additional namespaces under a namespace, and propagating resources within the namespace hierarchy. This allows self-service namespaces for tenants, without requiring cluster-wide permissions.&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/benchmarks">Multi-Tenancy Benchmarks (MTB)&lt;/a> project provides benchmarks and a command-line tool that performs several configuration and runtime checks to report if tenant namespaces are properly isolated and the necessary security controls are implemented.&lt;/p>
&lt;h2 id="clusters-as-a-service">Clusters as a Service&lt;/h2>
&lt;p>With the &lt;em>clusters-as-a-service&lt;/em> usage model, each tenant gets their own cluster. This model allows tenants to have different versions of cluster-wide resources such as CRDs, and provides full isolation of the Kubernetes control plane.&lt;/p>
&lt;p>The tenant clusters may be provisioned using projects like &lt;a href="https://cluster-api.sigs.k8s.io/">Cluster API (CAPI)&lt;/a> where a management cluster is used to provision multiple workload clusters. A workload cluster is assigned to a tenant and tenants have full control over cluster resources. Note that in most enterprises a central platform team may be responsible for managing required add-on services such as security and monitoring services, and for providing cluster lifecycle management services such as patching and upgrades. A tenant administrator may be restricted from modifying the centrally managed services and other critical cluster information.&lt;/p>
&lt;h2 id="control-planes-as-a-service">Control planes as a Service&lt;/h2>
&lt;p>In a variation of the &lt;em>clusters-as-a-service&lt;/em> model, the tenant cluster may be a &lt;strong>virtual cluster&lt;/strong> where each tenant gets their own dedicated Kubernetes control plane but share worker node resources. As with other forms of virtualization, users of a virtual cluster see no significant differences between a virtual cluster and other Kubernetes clusters. This is sometimes referred to as &lt;code>Control Planes as a Service&lt;/code> (CPaaS).&lt;/p>
&lt;p>A virtual cluster of this type shares worker node resources and workload state independent control plane components, like the scheduler. Other workload aware control-plane components, like the API server, are created on a per-tenant basis to allow overlaps, and additional components are used to synchronize and manage state across the per-tenant control plane and the underlying shared cluster resources. With this model users can manage their own cluster-wide resources.&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/virtualcluster">Virtual Cluster&lt;/a> project implements this model, where a &lt;code>supercluster&lt;/code> is shared by multiple &lt;code>virtual clusters&lt;/code>. The &lt;a href="https://github.com/kubernetes-sigs/cluster-api-provider-nested">Cluster API Nested&lt;/a> project is extending this work to conform to the CAPI model, allowing use of familiar API resources to provision and manage virtual clusters.&lt;/p>
&lt;h2 id="security-considerations">Security considerations&lt;/h2>
&lt;p>Cloud native security involves different system layers and lifecycle phases as described in the &lt;a href="https://kubernetes.io/blog/2020/11/18/cloud-native-security-for-your-clusters">Cloud Native Security Whitepaper&lt;/a> from CNCF SIG Security. Without proper security measures implemented across all layers and phases, Kubernetes tenant isolation can be compromised and a security breach with one tenant can threaten other tenants.&lt;/p>
&lt;p>It is important for any new user to Kubernetes to realize that the default installation of a new upstream Kubernetes cluster is not secure, and you are going to need to invest in hardening it in order to avoid security issues.&lt;/p>
&lt;p>At a minimum, the following security measures are required:&lt;/p>
&lt;ul>
&lt;li>image scanning: container image vulnerabilities can be exploited to execute commands and access additional resources.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC&lt;/a>: for &lt;em>namespaces-as-a-service&lt;/em> user roles and permissions must be properly configured at a per-namespace level; for other models tenants may need to be restricted from accessing centrally managed add-on services and other cluster-wide resources.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">network policies&lt;/a>: for &lt;em>namespaces-as-a-service&lt;/em> default network policies that deny all ingress and egress traffic are recommended to prevent cross-tenant network traffic and may also be used as a best practice for other tenancy models.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Kubernetes Pod Security Standards&lt;/a>: to enforce Pod hardening best practices the &lt;code>Restricted&lt;/code> policy is recommended as the default for tenant workloads with exclusions configured only as needed.&lt;/li>
&lt;li>&lt;a href="https://www.cisecurity.org/benchmark/kubernetes/">CIS Benchmarks for Kubernetes&lt;/a>: the CIS Benchmarks for Kubernetes guidelines should be used to properly configure Kubernetes control-plane and worker node components.&lt;/li>
&lt;/ul>
&lt;p>Additional recommendations include using:&lt;/p>
&lt;ul>
&lt;li>policy engines: for configuration security best practices, such as only allowing trusted registries.&lt;/li>
&lt;li>runtime scanners: to detect and report runtime security events.&lt;/li>
&lt;li>VM-based container sandboxing: for stronger data plane isolation.&lt;/li>
&lt;/ul>
&lt;p>While proper security is required independently of tenancy models, not having essential security controls like &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">pod security&lt;/a> in a shared cluster provides attackers with means to compromise tenancy models and possibly access sensitive information across tenants increasing the overall risk profile.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>A 2020 CNCF survey showed that production Kubernetes usage has increased by over 300% since 2016. As an increasing number of Kubernetes workloads move to production, organizations are looking for ways to share Kubernetes resources across teams for agility and cost savings.&lt;/p>
&lt;p>The &lt;strong>namespaces as a service&lt;/strong> tenancy model allows sharing clusters and hence enables resource efficiencies. However, it requires proper security configurations and has limitations as all tenants share the same cluster-wide resources.&lt;/p>
&lt;p>The &lt;strong>clusters as a service&lt;/strong> tenancy model addresses these limitations, but with higher management and resource overhead.&lt;/p>
&lt;p>The &lt;strong>control planes as a service&lt;/strong> model provides a way to share resources of a single Kubernetes cluster and also let tenants manage their own cluster-wide resources. Sharing worker node resources increases resource effeciencies, but also exposes cross tenant security and isolation concerns that exist for shared clusters.&lt;/p>
&lt;p>In many cases, organizations will use multiple tenancy models to address different use cases and as different product and development teams will have varying needs. Following security and management best practices, such as applying &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a> and not using the &lt;code>default&lt;/code> namespace, makes it easer to switch from one model to another.&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy">Kubernetes Multi-Tenancy Working Group&lt;/a> has created several projects like &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc">Hierarchical Namespaces Controller&lt;/a>, &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/virtualcluster">Virtual Cluster&lt;/a> / &lt;a href="https://github.com/kubernetes-sigs/cluster-api-provider-nested">CAPI Nested&lt;/a>, and &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/benchmarks">Multi-Tenancy Benchmarks&lt;/a> to make it easier to provision and manage multi-tenancy models.&lt;/p>
&lt;p>If you are interested in multi-tenancy topics, or would like to share your use cases, please join us in an upcoming &lt;a href="https://github.com/kubernetes/community/blob/master/wg-multitenancy/README.md">community meeting&lt;/a> or reach out on the &lt;em>wg-multitenancy channel&lt;/em> on the &lt;a href="https://slack.k8s.io/">Kubernetes slack&lt;/a>.&lt;/p></description></item><item><title>Blog: Local Storage: Storage Capacity Tracking, Distributed Provisioning and Generic Ephemeral Volumes hit Beta</title><link>https://kubernetes.io/blog/2021/04/14/local-storage-features-go-beta/</link><pubDate>Wed, 14 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/14/local-storage-features-go-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes">&amp;quot;generic ephemeral
volumes&amp;quot;&lt;/a>
and &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity/">&amp;quot;storage capacity
tracking&amp;quot;&lt;/a>
features in Kubernetes are getting promoted to beta in Kubernetes
1.21. Together with the &lt;a href="https://github.com/kubernetes-csi/external-provisioner#deployment-on-each-node">distributed provisioning
support&lt;/a>
in the CSI external-provisioner, development and deployment of
Container Storage Interface (CSI) drivers which manage storage locally
on a node become a lot easier.&lt;/p>
&lt;p>This blog post explains how such drivers worked before and how these
features can be used to make drivers simpler.&lt;/p>
&lt;h2 id="problems-we-are-solving">Problems we are solving&lt;/h2>
&lt;p>There are drivers for local storage, like
&lt;a href="https://github.com/cybozu-go/topolvm">TopoLVM&lt;/a> for traditional disks
and &lt;a href="https://intel.github.io/pmem-csi/latest/README.html">PMEM-CSI&lt;/a>
for &lt;a href="https://pmem.io/">persistent memory&lt;/a>. They work and are ready for
usage today also on older Kubernetes releases, but making that possible
was not trivial.&lt;/p>
&lt;h3 id="central-component-required">Central component required&lt;/h3>
&lt;p>The first problem is volume provisioning: it is handled through the
Kubernetes control plane. Some component must react to
&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims&lt;/a>
(PVCs)
and create volumes. Usually, that is handled by a central deployment
of the &lt;a href="https://kubernetes-csi.github.io/docs/external-provisioner.html">CSI
external-provisioner&lt;/a>
and a CSI driver component that then connects to the storage
backplane. But for local storage, there is no such backplane.&lt;/p>
&lt;p>TopoLVM solved this by having its different components communicate
with each other through the Kubernetes API server by creating and
reacting to custom resources. So although TopoLVM is based on CSI, a
standard that is independent of a particular container orchestrator,
TopoLVM only works on Kubernetes.&lt;/p>
&lt;p>PMEM-CSI created its own storage backplane with communication through
gRPC calls. Securing that communication depends on TLS certificates,
which made driver deployment more complicated.&lt;/p>
&lt;h3 id="informing-pod-scheduler-about-capacity">Informing Pod scheduler about capacity&lt;/h3>
&lt;p>The next problem is scheduling. When volumes get created independently
of pods (&amp;quot;immediate binding&amp;quot;), the CSI driver must pick a node without
knowing anything about the pod(s) that are going to use it. Topology
information then forces those pods to run on the node where the volume
was created. If other resources like RAM or CPU are exhausted there,
the pod cannot start. This can be avoided by configuring in the
StorageClass that volume creation is meant to wait for the first pod
that uses a volume (&lt;code>volumeBinding: WaitForFirstConsumer&lt;/code>). In that
mode, the Kubernetes scheduler tentatively picks a node based on other
constraints and then the external-provisioner is asked to create a
volume such that it is usable there. If local storage is exhausted,
the provisioner &lt;a href="https://github.com/kubernetes-csi/external-provisioner/blob/master/doc/design.md">can
ask&lt;/a>
for another scheduling round. But without information about available
capacity, the scheduler might always pick the same unsuitable node.&lt;/p>
&lt;p>Both TopoLVM and PMEM-CSI solved this with scheduler extenders. This
works, but it is hard to configure when deploying the driver because
communication between kube-scheduler and the driver is very dependent
on how the cluster was set up.&lt;/p>
&lt;h3 id="rescheduling">Rescheduling&lt;/h3>
&lt;p>A common use case for local storage is scratch space. A better fit for
that use case than persistent volumes are ephemeral volumes that get
created for a pod and destroyed together with it. The initial API for
supporting ephemeral volumes with CSI drivers (hence called &lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes">&amp;quot;&lt;em>CSI&lt;/em>
ephemeral
volumes&amp;quot;&lt;/a>)
was &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190122-csi-inline-volumes.md">designed for light-weight
volumes&lt;/a>
where volume creation is unlikely to fail. Volume creation happens
after pods have been permanently scheduled onto a node, in contrast to
the traditional provisioning where volume creation is tried before
scheduling a pod onto a node. CSI drivers must be modified to support
&amp;quot;CSI ephemeral volumes&amp;quot;, which was done for TopoLVM and PMEM-CSI. But
due to the design of the feature in Kubernetes, pods can get stuck
permanently if storage capacity runs out on a node. The scheduler
extenders try to avoid that, but cannot be 100% reliable.&lt;/p>
&lt;h2 id="enhancements-in-kubernetes-1-21">Enhancements in Kubernetes 1.21&lt;/h2>
&lt;h3 id="distributed-provisioning">Distributed provisioning&lt;/h3>
&lt;p>Starting with &lt;a href="https://github.com/kubernetes-csi/external-provisioner/releases/tag/v2.1.0">external-provisioner
v2.1.0&lt;/a>,
released for Kubernetes 1.20, provisioning can be handled by
external-provisioner instances that get &lt;a href="https://github.com/kubernetes-csi/external-provisioner#deployment-on-each-node">deployed together with the
CSI driver on each
node&lt;/a>
and then cooperate to provision volumes (&amp;quot;distributed
provisioning&amp;quot;). There is no need any more to have a central component
and thus no need for communication between nodes, at least not for
provisioning.&lt;/p>
&lt;h3 id="storage-capacity-tracking">Storage capacity tracking&lt;/h3>
&lt;p>A scheduler extender still needs some way to find out about capacity
on each node. When PMEM-CSI switched to distributed provisioning in
v0.9.0, this was done by querying the metrics data exposed by the
local driver containers. But it is better also for users to eliminate
the need for a scheduler extender completely because the driver
deployment becomes simpler. &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity/">Storage capacity
tracking&lt;/a>, &lt;a href="https://kubernetes.io/blog/2020/09/01/ephemeral-volumes-with-storage-capacity-tracking/">introduced in
1.19&lt;/a>
and promoted to beta in Kubernetes 1.21, achieves that. It works by
publishing information about capacity in &lt;code>CSIStorageCapacity&lt;/code>
objects. The scheduler itself then uses that information to filter out
unsuitable nodes. Because information might be not quite up-to-date,
pods may still get assigned to nodes with insufficient storage, it's
just less likely and the next scheduling attempt for a pod should work
better once the information got refreshed.&lt;/p>
&lt;h3 id="generic-ephemeral-volumes">Generic ephemeral volumes&lt;/h3>
&lt;p>So CSI drivers still need the ability to recover from a bad scheduling
decision, something that turned out to be impossible to implement for
&amp;quot;CSI ephemeral volumes&amp;quot;. &lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes">&amp;quot;&lt;em>Generic&lt;/em> ephemeral
volumes&amp;quot;&lt;/a>,
another feature that got promoted to beta in 1.21, don't have that
limitation. This feature adds a controller that will create and manage
PVCs with the lifetime of the Pod and therefore the normal recovery
mechanism also works for them. Existing storage drivers will be able
to process these PVCs without any new logic to handle this new
scenario.&lt;/p>
&lt;h2 id="known-limitations">Known limitations&lt;/h2>
&lt;p>Both generic ephemeral volumes and storage capacity tracking increase
the load on the API server. Whether that is a problem depends a lot on
the kind of workload, in particular how many pods have volumes and how
often those need to be created and destroyed.&lt;/p>
&lt;p>No attempt was made to model how scheduling decisions affect storage
capacity. That's because the effect can vary considerably depending on
how the storage system handles storage. The effect is that multiple
pods with unbound volumes might get assigned to the same node even
though there is only sufficient capacity for one pod. Scheduling
should recover, but it would be more efficient if the scheduler knew
more about storage.&lt;/p>
&lt;p>Because storage capacity gets published by a running CSI driver and
the cluster autoscaler needs information about a node that hasn't been
created yet, it will currently not scale up a cluster for pods that
need volumes. There is an &lt;a href="https://github.com/kubernetes/autoscaler/pull/3887">idea how to provide that
information&lt;/a>, but
more work is needed in that area.&lt;/p>
&lt;p>Distributed snapshotting and resizing are not currently supported. It
should be doable to adapt the respective sidecar and there are
tracking issues for external-snapshotter and external-resizer open
already, they just need some volunteer.&lt;/p>
&lt;p>The recovery from a bad scheduling decising can fail for pods with
multiple volumes, in particular when those volumes are local to nodes:
if one volume can be created and then storage is insufficient for
another volume, the first volume continues to exist and forces the
scheduler to put the pod onto the node of that volume. There is an
idea how do deal with this, &lt;a href="https://github.com/kubernetes/enhancements/pull/1703">rolling back the provision of the
volume&lt;/a>, but
this is only in the very early stages of brainstorming and not even a
merged KEP yet. For now it is better to avoid creating pods with more
than one persistent volume.&lt;/p>
&lt;h2 id="enabling-the-new-features-and-next-steps">Enabling the new features and next steps&lt;/h2>
&lt;p>With the feature entering beta in the 1.21 release, no additional actions are needed to enable it. Generic
ephemeral volumes also work without changes in CSI drivers. For more
information, see the
&lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes">documentation&lt;/a>
and the &lt;a href="https://kubernetes.io/blog/2020/09/01/ephemeral-volumes-with-storage-capacity-tracking/">previous blog
post&lt;/a>
about it. The API has not changed at all between alpha and beta.&lt;/p>
&lt;p>For the other two features, the external-provisioner documentation
explains how CSI driver developers must change how their driver gets
deployed to support &lt;a href="https://github.com/kubernetes-csi/external-provisioner#capacity-support">storage capacity
tracking&lt;/a>
and &lt;a href="https://github.com/kubernetes-csi/external-provisioner#deployment-on-each-node">distributed
provisioning&lt;/a>.
These two features are independent, therefore it is okay to enable
only one of them.&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">SIG
Storage&lt;/a>
would like to hear from you if you are using these new features. We
can be reached through
&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-storage">email&lt;/a>,
&lt;a href="https://slack.k8s.io/">Slack&lt;/a> (channel &lt;a href="https://kubernetes.slack.com/messages/sig-storage">&lt;code>#sig-storage&lt;/code>&lt;/a>) and in the
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meeting">regular SIG
meeting&lt;/a>.
A description of your workload would be very useful to validate design
decisions, set up performance tests and eventually promote these
features to GA.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>Thanks a lot to the members of the community who have contributed to these
features or given feedback including members of SIG Scheduling, SIG Auth,
￼and of course SIG Storage!&lt;/p></description></item><item><title>Blog: kube-state-metrics goes v2.0</title><link>https://kubernetes.io/blog/2021/04/13/kube-state-metrics-v-2-0/</link><pubDate>Tue, 13 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/13/kube-state-metrics-v-2-0/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Lili Cosic (Red Hat), Frederic Branczyk (Polar Signals), Manuel Rüger (Sony Interactive Entertainment), Tariq Ibrahim (Salesforce)&lt;/p>
&lt;h2 id="what">What?&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics&lt;/a>, a project under the Kubernetes organization, generates Prometheus format metrics based on the current state of the Kubernetes native resources. It does this by listening to the Kubernetes API and gathering information about resources and objects, e.g. Deployments, Pods, Services, and StatefulSets. A full list of resources is available in the &lt;a href="https://github.com/kubernetes/kube-state-metrics/tree/master/docs">documentation&lt;/a> of kube-state-metrics.&lt;/p>
&lt;h2 id="why">Why?&lt;/h2>
&lt;p>There are numerous useful metrics and insights provided by &lt;code>kube-state-metrics&lt;/code> right out of the box! These metrics can be used to serve as an insight into your cluster: Either through metrics alone, in the form of dashboards, or through an alerting pipeline. To provide a few examples:&lt;/p>
&lt;ul>
&lt;li>&lt;code>kube_pod_container_status_restarts_total&lt;/code> can be used to alert on a crashing pod.&lt;/li>
&lt;li>&lt;code>kube_deployment_status_replicas&lt;/code> which together with &lt;code>kube_deployment_status_replicas_available&lt;/code> can be used to alert on whether a deployment is rolled out successfully or stuck.&lt;/li>
&lt;li>&lt;code>kube_pod_container_resource_requests&lt;/code> and &lt;code>kube_pod_container_resource_limits&lt;/code> can be used in capacity planning dashboards.&lt;/li>
&lt;/ul>
&lt;p>And there are many more metrics available! To learn more about the other metrics and their details, please check out the &lt;a href="https://github.com/kubernetes/kube-state-metrics/tree/master/docs#readme">documentation&lt;/a>.&lt;/p>
&lt;h2 id="what-is-new-in-v2-0">What is new in v2.0?&lt;/h2>
&lt;p>So now that we know what kube-state-metrics is, we are excited to announce the next release: kube-state-metrics v2.0! This release was long-awaited and started with an alpha release in September 2020. To ease maintenance we removed tech debt and also adjusted some confusing wording around user-facing flags and APIs. We also removed some metrics that caused unnecessarily high cardinality in Prometheus! For the 2.0 release, we took the time to set up scale and performance testing. This allows us to better understand if we hit any issues in large clusters and also to document resource request recommendations for your clusters. In this release (and v1.9.8) container builds providing support for multiple architectures were introduced allowing you to run kube-state-metrics on ARM, ARM64, PPC64 and S390x as well!&lt;/p>
&lt;p>So without further ado, here is the list of more noteworthy user-facing breaking changes. A full list of changes, features and bug fixes is available in the changelog at the end of this post.&lt;/p>
&lt;ul>
&lt;li>Flag &lt;code>--namespace&lt;/code> was renamed to &lt;code>--namespaces&lt;/code>. If you are using the former, please make sure to update the flag before deploying the latest release.&lt;/li>
&lt;li>Flag &lt;code>--collectors&lt;/code> was renamed to &lt;code>--resources&lt;/code>.&lt;/li>
&lt;li>Flags &lt;code>--metric-blacklist&lt;/code> and &lt;code>--metric-whitelist&lt;/code> were renamed to &lt;code>--metric-denylist&lt;/code> and &lt;code>--metric-allowlist&lt;/code>.&lt;/li>
&lt;li>Flag &lt;code>--metric-labels-allowlist&lt;/code> allows you to specify a list of Kubernetes labels that get turned into the dimensions of the &lt;code>kube_&amp;lt;resource-name&amp;gt;_labels&lt;/code> metrics. By default, the metric contains only name and namespace labels.&lt;/li>
&lt;li>All metrics with a prefix of &lt;code>kube_hpa_*&lt;/code> were renamed to &lt;code>kube_horizontalpodautoscaler_*&lt;/code>.&lt;/li>
&lt;li>Metric labels that relate to Kubernetes were converted to snake_case.&lt;/li>
&lt;li>If you are importing kube-state-metrics as a library, we have updated our go module path to &lt;code>k8s.io/kube-state-metrics/v2&lt;/code>&lt;/li>
&lt;li>All deprecated stable metrics were removed as per the &lt;a href="https://github.com/kubernetes/kube-state-metrics/tree/release-1.9/docs#metrics-deprecation">notice in the v1.9 release&lt;/a>.&lt;/li>
&lt;li>&lt;code>quay.io/coreos/kube-state-metrics&lt;/code> images will no longer be updated. &lt;code>k8s.gcr.io/kube-state-metrics/kube-state-metrics&lt;/code> is the new canonical location.&lt;/li>
&lt;li>The helm chart that is part of the kubernetes/kube-state-metrics repository is deprecated. &lt;a href="https://github.com/prometheus-community/helm-charts">https://github.com/prometheus-community/helm-charts&lt;/a> will be its new location.&lt;/li>
&lt;/ul>
&lt;p>For the full list of v2.0 release changes includes features, bug fixes and other breaking changes see the full &lt;a href="https://github.com/kubernetes/kube-state-metrics/blob/master/CHANGELOG.md">CHANGELOG&lt;/a>.&lt;/p>
&lt;h2 id="found-a-problem">Found a problem?&lt;/h2>
&lt;p>Thanks to all our users for testing so far and thank you to all our contributors for your issue reports as well as code and documentation changes! If you find any problems, we the &lt;a href="https://github.com/kubernetes/kube-state-metrics/blob/master/OWNERS">maintainers&lt;/a> are more than happy to look into them, so please report them by opening a &lt;a href="https://github.com/kubernetes/kube-state-metrics/issues/new/choose">GitHub issue&lt;/a>.&lt;/p></description></item><item><title>Blog: Introducing Suspended Jobs</title><link>https://kubernetes.io/blog/2021/04/12/introducing-suspended-jobs/</link><pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/12/introducing-suspended-jobs/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Adhityaa Chandrasekar (Google)&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Jobs&lt;/a> are a crucial part of
Kubernetes' API. While other kinds of workloads such as &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments&lt;/a>,
&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSets&lt;/a>,
&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets&lt;/a>, and
&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSets&lt;/a>
solve use-cases that require Pods to run forever, Jobs are useful when Pods need
to run to completion. Commonly used in parallel batch processing, Jobs can be
used in a variety of applications ranging from video rendering and database
maintenance to sending bulk emails and scientific computing.&lt;/p>
&lt;p>While the amount of parallelism and the conditions for Job completion are
configurable, the Kubernetes API lacked the ability to suspend and resume Jobs.
This is often desired when cluster resources are limited and a higher priority
Job needs to execute in the place of another Job. Deleting the lower priority
Job is a poor workaround as Pod completion history and other metrics associated
with the Job will be lost.&lt;/p>
&lt;p>With the recent Kubernetes 1.21 release, you will be able to suspend a Job by
updating its spec. The feature is currently in &lt;strong>alpha&lt;/strong> and requires you to
enable the &lt;code>SuspendJob&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a>
on the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">API server&lt;/a>
and the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">controller manager&lt;/a>
in order to use it.&lt;/p>
&lt;h2 id="api-changes">API changes&lt;/h2>
&lt;p>We introduced a new boolean field &lt;code>suspend&lt;/code> into the &lt;code>.spec&lt;/code> of Jobs. Let's say
I create the following Job:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-job&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">suspend&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parallelism&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">2&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">completions&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-container&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>busybox&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;sleep&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;5&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Jobs are not suspended by default, so I'm explicitly setting the &lt;code>suspend&lt;/code> field
to &lt;em>true&lt;/em> in the &lt;code>.spec&lt;/code> of the above Job manifest. In the above example, the
Job controller will refrain from creating Pods until I'm ready to start the Job,
which I can do by updating &lt;code>suspend&lt;/code> to false.&lt;/p>
&lt;p>As another example, consider a Job that was created with the &lt;code>suspend&lt;/code> field
omitted. The Job controller will happily create Pods to work towards Job
completion. However, before the Job completes, if I explicitly set the field to
true with a Job update, the Job controller will terminate all active Pods that
are running and will wait indefinitely for the flag to be flipped back to false.
Typically, Pod termination is done by sending a SIGTERM signal to all container
processes in the Pod; the &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">graceful termination period&lt;/a>
defined in the Pod spec will be honoured. Pods terminated this way will not be
counted as failures by the Job controller.&lt;/p>
&lt;p>It is important to understand that succeeded and failed Pods from the past will
continue to exist after you suspend a Job. That is, that they will count towards
Job completion once you resume it. You can verify this by looking at Job's
status before and after suspension.&lt;/p>
&lt;p>Read the &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job#suspending-a-job">documentation&lt;/a>
for a full overview of this new feature.&lt;/p>
&lt;h2 id="where-is-this-useful">Where is this useful?&lt;/h2>
&lt;p>Let's say I'm the operator of a large cluster. I have many users submitting Jobs
to the cluster, but not all Jobs are created equal — some Jobs are more
important than others. Cluster resources aren't infinite either, so all users
must share resources. If all Jobs were created in the suspended state and placed
in a pending queue, I can achieve priority-based Job scheduling by resuming Jobs
in the right order.&lt;/p>
&lt;p>As another motivational use-case, consider a cloud provider where compute
resources are cheaper at night than in the morning. If I have a long-running Job
that takes multiple days to complete, being able to suspend the Job in the
morning and then resume it in the evening every day can reduce costs.&lt;/p>
&lt;p>Since this field is a part of the Job spec, &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJobs&lt;/a>
automatically get this feature for free too.&lt;/p>
&lt;h2 id="references-and-next-steps">References and next steps&lt;/h2>
&lt;p>If you're interested in a deeper dive into the rationale behind this feature and
the decisions we have taken, consider reading the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/2232-suspend-jobs">enhancement proposal&lt;/a>.
There's more detail on suspending and resuming jobs in the documentation for &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job#suspending-a-job">Job&lt;/a>.&lt;/p>
&lt;p>As previously mentioned, this feature is currently in alpha and is available
only if you explicitly opt-in through the &lt;code>SuspendJob&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a>.
If this is a feature you're interested in, please consider testing suspended
Jobs in your cluster and providing feedback. You can discuss this enhancement &lt;a href="https://github.com/kubernetes/enhancements/issues/2232">on GitHub&lt;/a>.
The SIG Apps community also &lt;a href="https://github.com/kubernetes/community/tree/master/sig-apps#meetings">meets regularly&lt;/a>
and can be reached through &lt;a href="https://github.com/kubernetes/community/tree/master/sig-apps#contact">Slack or the mailing list&lt;/a>.
Barring any unexpected changes to the API, we intend to graduate the feature to
beta in Kubernetes 1.22, so that the feature becomes available by default.&lt;/p></description></item><item><title>Blog: Kubernetes 1.21: CronJob Reaches GA</title><link>https://kubernetes.io/blog/2021/04/09/kubernetes-release-1.21-cronjob-ga/</link><pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/09/kubernetes-release-1.21-cronjob-ga/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Alay Patel (Red Hat), and Maciej Szulik (Red Hat)&lt;/p>
&lt;p>In Kubernetes v1.21, the
&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob&lt;/a> resource
reached general availability (GA). We've also substantially improved the
performance of CronJobs since Kubernetes v1.19, by implementing a new
controller.&lt;/p>
&lt;p>In Kubernetes v1.20 we launched a revised v2 controller for CronJobs,
initially as an alpha feature. Kubernetes 1.21 uses the newer controller by
default, and the CronJob resource itself is now GA (group version: &lt;code>batch/v1&lt;/code>).&lt;/p>
&lt;p>In this article, we'll take you through the driving forces behind this new
development, give you a brief description of controller design for core
Kubernetes, and we'll outline what you will gain from this improved controller.&lt;/p>
&lt;p>The driving force behind promoting the API was Kubernetes' policy choice to
&lt;a href="https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/">ensure APIs move beyond beta&lt;/a>.
That policy aims to prevent APIs from being stuck in a “permanent beta” state.
Over the years the old CronJob controller implementation had received healthy
feedback from the community, with reports of several widely recognized
&lt;a href="https://github.com/kubernetes/kubernetes/issues/82659">issues&lt;/a>.&lt;/p>
&lt;p>If the beta API for CronJob was to be supported as GA, the existing controller
code would need substantial rework. Instead, the SIG Apps community decided
to introduce a new controller and gradually replace the old one.&lt;/p>
&lt;h2 id="how-do-controllers-work">How do controllers work?&lt;/h2>
&lt;p>Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/">controllers&lt;/a> are control
loops that watch the state of resource(s) in your cluster, then make or
request changes where needed. Each controller tries to move part of the
current cluster state closer to the desired state.&lt;/p>
&lt;p>The v1 CronJob controller works by performing a periodic poll and sweep of all
the CronJob objects in your cluster, in order to act on them. It is a single
worker implementation that gets all CronJobs every 10 seconds, iterates over
each one of them, and syncs them to their desired state. This was the default
way of doing things almost 5 years ago when the controller was initially
written. In hindsight, we can certainly say that such an approach can
overload the API server at scale.&lt;/p>
&lt;p>These days, every core controller in kubernetes must follow the guidelines
described in &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/controllers.md#readme">Writing Controllers&lt;/a>.
Among many details, that document prescribes using
&lt;a href="https://www.cncf.io/blog/2019/10/15/extend-kubernetes-via-a-shared-informer/">shared informers&lt;/a>
to “receive notifications of adds, updates, and deletes for a particular
resource”. Upon any such events, the related object(s) is placed in a queue.
Workers pull items from the queue and process them one at a time. This
approach ensures consistency and scalability.&lt;/p>
&lt;p>The picture below shows the flow of information from kubernetes API server,
through shared informers and queue, to the main part of a controller - a
reconciliation loop which is responsible for performing the core functionality.&lt;/p>
&lt;p>&lt;img src="controller-flowchart.svg" alt="Controller flowchart">&lt;/p>
&lt;p>The CronJob controller V2 uses a queue that implements the DelayingInterface to
handle the scheduling aspect. This queue allows processing an element after a
specific time interval. Every time there is a change in a CronJob or its related
Jobs, the key that represents the CronJob is pushed to the queue. The main
handler pops the key, processes the CronJob, and after completion
pushes the key back into the queue for the next scheduled time interval. This is
immediately a more performant implementation, as it no longer requires a linear
scan of all the CronJobs. On top of that, this controller can be scaled by
increasing the number of workers processing the CronJobs in parallel.&lt;/p>
&lt;h2 id="performance-impact">Performance impact of the new controller&lt;/h2>
&lt;p>In order to test the performance difference of the two controllers a VM instance
with 128 GiB RAM and 64 vCPUs was used to set up a single node Kubernetes cluster.
Initially, a sample workload was created with 20 CronJob instances with a schedule
to run every minute, and 2100 CronJobs running every 20 hours. Additionally,
over the next few minutes we added 1000 CronJobs with a schedule to run every
20 hours, until we reached a total of 5120 CronJobs.&lt;/p>
&lt;p>&lt;img src="performance-impact-graph.svg" alt="Visualization of performance">&lt;/p>
&lt;p>We observed that for every 1000 CronJobs added, the old controller used
around 90 to 120 seconds more wall-clock time to schedule 20 Jobs every cycle.
That is, at 5120 CronJobs, the old controller took approximately 9 minutes
to create 20 Jobs. Hence, during each cycle, about 8 schedules were missed.
The new controller, implemented with architectural change explained above,
created 20 Jobs without any delay, even when we created an additional batch
of 1000 CronJobs reaching a total of 6120.&lt;/p>
&lt;p>As a closing remark, the new controller exposes a histogram metric
&lt;code>cronjob_controller_cronjob_job_creation_skew_duration_seconds&lt;/code> which helps
monitor the time difference between when a CronJob is meant to run and when
the actual Job is created.&lt;/p>
&lt;p>Hopefully the above description is a sufficient argument to follow the
guidelines and standards set in the Kubernetes project, even for your own
controllers. As mentioned before, the new controller is on by default starting
from Kubernetes v1.21; if you want to check it out in the previous release (1.20),
you can enable the &lt;code>CronJobControllerV2&lt;/code>
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a>
for the kube-controller-manager: &lt;code>--feature-gate=&amp;quot;CronJobControllerV2=true&amp;quot;&lt;/code>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.21: Power to the Community</title><link>https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/</link><pubDate>Thu, 08 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/08/kubernetes-1-21-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.21/release-team.md">Kubernetes 1.21 Release Team&lt;/a>&lt;/p>
&lt;p>We’re pleased to announce the release of Kubernetes 1.21, our first release of 2021! This release consists of 51 enhancements: 13 enhancements have graduated to stable, 16 enhancements are moving to beta, 20 enhancements are entering alpha, and 2 features have been deprecated.&lt;/p>
&lt;p>This release cycle, we saw a major shift in ownership of processes around the release team. We moved from a synchronous mode of communication, where we periodically asked the community for inputs, to a mode where the community opts-in to contribute features and/or blogs to the release. These changes have resulted in an increase in collaboration and teamwork across the community. The result of all that is reflected in Kubernetes 1.21 having the most number of features in the recent times.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="cronjobs-graduate-to-stable">CronJobs Graduate to Stable!&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJobs&lt;/a> (previously ScheduledJobs) has been a beta feature since Kubernetes 1.8! With 1.21 we get to finally see this widely used API graduate to stable.&lt;/p>
&lt;p>CronJobs are meant for performing regular scheduled actions such as backups, report generation, and so on. Each of those tasks should be configured to recur indefinitely (for example: once a day / week / month); you can define the point in time within that interval when the job should start.&lt;/p>
&lt;h3 id="immutable-secrets-and-configmaps">Immutable Secrets and ConfigMaps&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/#secret-immutable">Immutable Secrets&lt;/a> and &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable">ConfigMaps&lt;/a> add a new field to those resource types that will reject changes to those objects if set. Secrets and ConfigMaps by default are mutable which is beneficial for pods that are able to consume changes. Mutating Secrets and ConfigMaps can also cause problems if a bad configuration is pushed for pods that use them.&lt;/p>
&lt;p>By marking Secrets and ConfigMaps as immutable you can be sure your application configuration won't change. If you want to make changes you'll need to create a new, uniquly named Secret or ConfigMap and deploy a new pod to consume that resource. Immutable resources also have scaling benefits because controllers do not need to poll the API server to watch for changes.&lt;/p>
&lt;p>This feature has graduated to stable in Kubernetes 1.21.&lt;/p>
&lt;h3 id="ipv4-ipv6-dual-stack-support">IPv4/IPv6 dual-stack support&lt;/h3>
&lt;p>IP addresses are a consumable resource that cluster operators and administrators need to make sure are not exhausted. In particular, public IPv4 addresses are now scarce. Having dual-stack support enables native IPv6 routing to pods and services, whilst still allowing your cluster to talk IPv4 where needed. Dual-stack cluster networking also improves a possible scaling limitation for workloads.&lt;/p>
&lt;p>Dual-stack support in Kubernetes means that pods, services, and nodes can get IPv4 addresses and IPv6 addresses. In Kubernetes 1.21 &lt;a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/">dual-stack networking&lt;/a> has graduated from alpha to beta, and is now enabled by default.&lt;/p>
&lt;h3 id="graceful-node-shutdown">Graceful Node Shutdown&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown">Graceful Node shutdown&lt;/a> also graduated to beta with this release (and will now be available to a much larger group of users)! This is a hugely beneficial feature that allows the kubelet to be aware of node shutdown, and gracefully terminate pods that are scheduled to that node.&lt;/p>
&lt;p>Currently, when a node shuts down, pods do not follow the expected termination lifecycle and are not shut down gracefully. This can introduce problems with a lot of different workloads. Going forward, the kubelet will be able to detect imminent system shutdown through systemd, then inform running pods so they can terminate as gracefully as possible.&lt;/p>
&lt;h3 id="persistentvolume-health-monitor">PersistentVolume Health Monitor&lt;/h3>
&lt;p>Persistent Volumes (PV) are commonly used in applications to get local, file-based storage. They can be used in many different ways and help users migrate applications without needing to re-write storage backends.&lt;/p>
&lt;p>Kubernetes 1.21 has a new alpha feature which allows PVs to be monitored for health of the volume and marked accordingly if the volume becomes unhealthy. Workloads will be able to react to the health state to protect data from being written or read from a volume that is unhealthy.&lt;/p>
&lt;h3 id="reducing-kubernetes-build-maintenance">Reducing Kubernetes Build Maintenance&lt;/h3>
&lt;p>Previously Kubernetes has maintained multiple build systems. This has often been a source of friction and complexity for new and current contributors.&lt;/p>
&lt;p>Over the last release cycle, a lot of work has been put in to simplify the build process, and standardize on the native Golang build tools. This should empower broader community maintenance, and lower the barrier to entry for new contributors.&lt;/p>
&lt;h2 id="major-changes">Major Changes&lt;/h2>
&lt;h3 id="podsecuritypolicy-deprecation">PodSecurityPolicy Deprecation&lt;/h3>
&lt;p>In Kubernetes 1.21, PodSecurityPolicy is deprecated. As with all Kubernetes feature deprecations, PodSecurityPolicy will continue to be available and fully-functional for several more releases. PodSecurityPolicy, previously in the beta stage, is planned for removal in Kubernetes 1.25.&lt;/p>
&lt;p>What's next? We're developing a new built-in mechanism to help limit Pod privileges, with a working title of “PSP Replacement Policy.” Our plan is for this new mechanism to cover the key PodSecurityPolicy use cases, with greatly improved ergonomics and maintainability. To learn more, read &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future">PodSecurityPolicy Deprecation: Past, Present, and Future&lt;/a>.&lt;/p>
&lt;h3 id="topologykeys-deprecation">TopologyKeys Deprecation&lt;/h3>
&lt;p>The Service field &lt;code>topologyKeys&lt;/code> is now deprecated; all the component features that used this field were previously alpha, and are now also deprecated.
We've replaced &lt;code>topologyKeys&lt;/code> with a way to implement topology-aware routing, called topology-aware hints. Topology-aware hints are an alpha feature in Kubernetes 1.21. You can read more details about the replacement feature in &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service-topology/">Topology Aware Hints&lt;/a>; the related &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/2433-topology-aware-hints/README.md">KEP&lt;/a> explains the context for why we switched.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/752">EndpointSlice&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/34">Add sysctl support&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/85">PodDisruptionBudgets&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="notable-feature-updates">Notable Feature Updates&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/541">External client-go credential providers&lt;/a> - beta in 1.21&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1602">Structured logging&lt;/a> - graduating to beta in 1.22&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/592">TTL after finish cleanup for Jobs and Pods&lt;/a> - graduated to beta&lt;/li>
&lt;/ul>
&lt;h1 id="release-notes">Release notes&lt;/h1>
&lt;p>You can check out the full details of the 1.21 release in the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md">release notes&lt;/a>.&lt;/p>
&lt;h1 id="availability-of-release">Availability of release&lt;/h1>
&lt;p>Kubernetes 1.21 is available for &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.21.0">download on GitHub&lt;/a>. There are some great resources out there for getting started with Kubernetes. You can check out some &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> on the main Kubernetes site, or run a local cluster on your machine using Docker containers with &lt;a href="https://kind.sigs.k8s.io">kind&lt;/a>. If you’d like to try building a cluster from scratch, check out the &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes the Hard Way&lt;/a> tutorial by Kelsey Hightower.&lt;/p>
&lt;h1 id="release-team">Release Team&lt;/h1>
&lt;p>This release was made possible by a very dedicated group of individuals, who came together as a team in the midst of a lot of things happening out in the world. A huge thank you to the release lead Nabarun Pal, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.21 release for the community.&lt;/p>
&lt;h1 id="release-logo">Release Logo&lt;/h1>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2021-04-08-kubernetes-release-1.21/globe_250px.png" alt="Kubernetes 1.21 Release Logo">&lt;/p>
&lt;p>The Kubernetes 1.21 Release Logo portrays the global nature of the Release Team, with release team members residing in timezones from UTC+8 all the way to UTC-8. The diversity of the release team brought in a lot of challenges, but the team tackled them all by adopting more asynchronous communication practices. The heptagonal globe in the release logo signifies the sheer determination of the community to overcome the challenges as they come. It celebrates the amazing teamwork of the release team over the last 3 months to bring in a fun packed Kubernetes release!&lt;/p>
&lt;p>The logo is designed by &lt;a href="https://www.behance.net/noblebatman">Aravind Sekar&lt;/a>, an independent designer based out of India. Aravind helps open source communities like PyCon India in their design efforts.&lt;/p>
&lt;h1 id="user-highlights">User Highlights&lt;/h1>
&lt;ul>
&lt;li>CNCF welcomes 47 new organizations across the globe as members to advance Cloud Native technology further at the start of 2021! These &lt;a href="https://www.cncf.io/announcements/2021/02/24/cloud-native-computing-foundation-welcomes-47-new-members-at-the-start-of-2021/">new members&lt;/a> will join CNCF at the upcoming 2021 KubeCon + CloudNativeCon events, including &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">KubeCon + CloudNativeCom EU – Virtual&lt;/a> from May 4 – 7, 2021, and &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/">KubeCon + CloudNativeCon NA in Los Angeles&lt;/a> from October 12 – 15, 2021.&lt;/li>
&lt;/ul>
&lt;h1 id="project-velocity">Project Velocity&lt;/h1>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/">CNCF K8s DevStats project&lt;/a> aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is a neat illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p>
&lt;p>In the v1.21 release cycle, which ran for 12 weeks (January 11 to April 8), we saw contributions from &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.20.0%20-%20now&amp;amp;var-metric=contributions">999 companies&lt;/a> and &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.20.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All">1279 individuals&lt;/a>.&lt;/p>
&lt;h1 id="ecosystem-updates">Ecosystem Updates&lt;/h1>
&lt;ul>
&lt;li>In the wake of rising racism &amp;amp; attacks on global Asian communities, read the statement from CNCF General Priyanka Sharma on the &lt;a href="https://www.cncf.io/blog/2021/03/18/statement-from-cncf-general-manager-priyanka-sharma-on-the-unacceptable-attacks-against-aapi-and-asian-communities/">CNCF blog&lt;/a> reinstating the community's commitment towards inclusive values &amp;amp; diversity-powered resilience.&lt;/li>
&lt;li>We now have a process in place for migration of the default branch from master → main. Learn more about the guidelines &lt;a href="k8s.dev/rename">here&lt;/a>&lt;/li>
&lt;li>CNCF and the Linux Foundation have announced the availability of their new training course, &lt;a href="https://training.linuxfoundation.org/training/kubernetes-security-essentials-lfs260/">LFS260 – Kubernetes Security Essentials&lt;/a>. In addition to providing skills and knowledge on a broad range of best practices for securing container-based applications and Kubernetes platforms, the course is also a great way to prepare for the recently launched &lt;a href="https://training.linuxfoundation.org/certification/certified-kubernetes-security-specialist/">Certified Kubernetes Security Specialist&lt;/a> certification exam.&lt;/li>
&lt;/ul>
&lt;h1 id="event-updates">Event Updates&lt;/h1>
&lt;ul>
&lt;li>KubeCon + CloudNativeCon Europe 2021 will take place May 4 - 7, 2021! You can find more information about the conference &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">here&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://kubernetescommunitydays.org/">Kubernetes Community Days&lt;/a> are being relaunched! Q2 2021 will start with Africa and Bengaluru.&lt;/li>
&lt;/ul>
&lt;h1 id="upcoming-release-webinar">Upcoming release webinar&lt;/h1>
&lt;p>Join the members of the Kubernetes 1.21 release team on May 13th, 2021 to learn about the major features in this release including IPv4/IPv6 dual-stack support, PersistentVolume Health Monitor, Immutable Secrets and ConfigMaps, and many more. Register here: &lt;a href="https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-121-release/">https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-121-release/&lt;/a>&lt;/p>
&lt;h1 id="get-involved">Get Involved&lt;/h1>
&lt;p>If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels:&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributor website&lt;/a>&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://github.com/cncf/foundation/blob/master/case-study-guidelines.md">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: PodSecurityPolicy Deprecation: Past, Present, and Future</title><link>https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</link><pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Tabitha Sable (Kubernetes SIG Security)&lt;/p>
&lt;p>PodSecurityPolicy (PSP) is being deprecated in Kubernetes 1.21, to be released later this week. This starts the countdown to its removal, but doesn’t change anything else. PodSecurityPolicy will continue to be fully functional for several more releases before being removed completely. In the meantime, we are developing a replacement for PSP that covers key use cases more easily and sustainably.&lt;/p>
&lt;p>What are Pod Security Policies? Why did we need them? Why are they going away, and what’s next? How does this affect you? These key questions come to mind as we prepare to say goodbye to PSP, so let’s walk through them together. We’ll start with an overview of how features get removed from Kubernetes.&lt;/p>
&lt;h2 id="what-does-deprecation-mean-in-kubernetes">What does deprecation mean in Kubernetes?&lt;/h2>
&lt;p>Whenever a Kubernetes feature is set to go away, our &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy&lt;/a> is our guide. First the feature is marked as deprecated, then after enough time has passed, it can finally be removed.&lt;/p>
&lt;p>Kubernetes 1.21 starts the deprecation process for PodSecurityPolicy. As with all feature deprecations, PodSecurityPolicy will continue to be fully functional for several more releases. The current plan is to remove PSP from Kubernetes in the 1.25 release.&lt;/p>
&lt;p>Until then, PSP is still PSP. There will be at least a year during which the newest Kubernetes releases will still support PSP, and nearly two years until PSP will pass fully out of all supported Kubernetes versions.&lt;/p>
&lt;h2 id="what-is-podsecuritypolicy">What is PodSecurityPolicy?&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">PodSecurityPolicy&lt;/a> is a built-in &lt;a href="https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/">admission controller&lt;/a> that allows a cluster administrator to control security-sensitive aspects of the Pod specification.&lt;/p>
&lt;p>First, one or more PodSecurityPolicy resources are created in a cluster to define the requirements Pods must meet. Then, RBAC rules are created to control which PodSecurityPolicy applies to a given pod. If a pod meets the requirements of its PSP, it will be admitted to the cluster as usual. In some cases, PSP can also modify Pod fields, effectively creating new defaults for those fields. If a Pod does not meet the PSP requirements, it is rejected, and cannot run.&lt;/p>
&lt;p>One more important thing to know about PodSecurityPolicy: it’s not the same as &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context">PodSecurityContext&lt;/a>.&lt;/p>
&lt;p>A part of the Pod specification, PodSecurityContext (and its per-container counterpart &lt;code>SecurityContext&lt;/code>) is the collection of fields that specify many of the security-relevant settings for a Pod. The security context dictates to the kubelet and container runtime how the Pod should actually be run. In contrast, the PodSecurityPolicy only constrains (or defaults) the values that may be set on the security context.&lt;/p>
&lt;p>The deprecation of PSP does not affect PodSecurityContext in any way.&lt;/p>
&lt;h2 id="why-did-we-need-podsecuritypolicy">Why did we need PodSecurityPolicy?&lt;/h2>
&lt;p>In Kubernetes, we define resources such as Deployments, StatefulSets, and Services that represent the building blocks of software applications. The various controllers inside a Kubernetes cluster react to these resources, creating further Kubernetes resources or configuring some software or hardware to accomplish our goals.&lt;/p>
&lt;p>In most Kubernetes clusters, RBAC (Role-Based Access Control) &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole">rules&lt;/a> control access to these resources. &lt;code>list&lt;/code>, &lt;code>get&lt;/code>, &lt;code>create&lt;/code>, &lt;code>edit&lt;/code>, and &lt;code>delete&lt;/code> are the sorts of API operations that RBAC cares about, but &lt;em>RBAC does not consider what settings are being put into the resources it controls&lt;/em>. For example, a Pod can be almost anything from a simple webserver to a privileged command prompt offering full access to the underlying server node and all the data. It’s all the same to RBAC: a Pod is a Pod is a Pod.&lt;/p>
&lt;p>To control what sorts of settings are allowed in the resources defined in your cluster, you need Admission Control in addition to RBAC. Since Kubernetes 1.3, PodSecurityPolicy has been the built-in way to do that for security-related Pod fields. Using PodSecurityPolicy, you can prevent “create Pod” from automatically meaning “root on every cluster node,” without needing to deploy additional external admission controllers.&lt;/p>
&lt;h2 id="why-is-podsecuritypolicy-going-away">Why is PodSecurityPolicy going away?&lt;/h2>
&lt;p>In the years since PodSecurityPolicy was first introduced, we have realized that PSP has some serious usability problems that can’t be addressed without making breaking changes.&lt;/p>
&lt;p>The way PSPs are applied to Pods has proven confusing to nearly everyone that has attempted to use them. It is easy to accidentally grant broader permissions than intended, and difficult to inspect which PSP(s) apply in a given situation. The “changing Pod defaults” feature can be handy, but is only supported for certain Pod settings and it’s not obvious when they will or will not apply to your Pod. Without a “dry run” or audit mode, it’s impractical to retrofit PSP to existing clusters safely, and it’s impossible for PSP to ever be enabled by default.&lt;/p>
&lt;p>For more information about these and other PSP difficulties, check out SIG Auth’s KubeCon NA 2019 Maintainer Track session video:
&lt;div class="youtube-quote-sm">
&lt;iframe src="https://www.youtube.com/embed/SFtHRmPuhEw?start=953" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>Today, you’re not limited only to deploying PSP or writing your own custom admission controller. Several external admission controllers are available that incorporate lessons learned from PSP to provide a better user experience. &lt;a href="https://github.com/cruise-automation/k-rail">K-Rail&lt;/a>, &lt;a href="https://github.com/kyverno/kyverno/">Kyverno&lt;/a>, and &lt;a href="https://github.com/open-policy-agent/gatekeeper/">OPA/Gatekeeper&lt;/a> are all well-known, and each has its fans.&lt;/p>
&lt;p>Although there are other good options available now, we believe there is still value in having a built-in admission controller available as a choice for users. With this in mind, we turn toward building what’s next, inspired by the lessons learned from PSP.&lt;/p>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>Kubernetes SIG Security, SIG Auth, and a diverse collection of other community members have been working together for months to ensure that what’s coming next is going to be awesome. We have developed a Kubernetes Enhancement Proposal (&lt;a href="https://github.com/kubernetes/enhancements/issues/2579">KEP 2579&lt;/a>) and a prototype for a new feature, currently being called by the temporary name &amp;quot;PSP Replacement Policy.&amp;quot; We are targeting an Alpha release in Kubernetes 1.22.&lt;/p>
&lt;p>PSP Replacement Policy starts with the realization that since there is a robust ecosystem of external admission controllers already available, PSP’s replacement doesn’t need to be all things to all people. Simplicity of deployment and adoption is the key advantage a built-in admission controller has compared to an external webhook, so we have focused on how to best utilize that advantage.&lt;/p>
&lt;p>PSP Replacement Policy is designed to be as simple as practically possible while providing enough flexibility to really be useful in production at scale. It has soft rollout features to enable retrofitting it to existing clusters, and is configurable enough that it can eventually be active by default. It can be deactivated partially or entirely, to coexist with external admission controllers for advanced use cases.&lt;/p>
&lt;h2 id="what-does-this-mean-for-you">What does this mean for you?&lt;/h2>
&lt;p>What this all means for you depends on your current PSP situation. If you’re already using PSP, there’s plenty of time to plan your next move. Please review the PSP Replacement Policy KEP and think about how well it will suit your use case.&lt;/p>
&lt;p>If you’re making extensive use of the flexibility of PSP with numerous PSPs and complex binding rules, you will likely find the simplicity of PSP Replacement Policy too limiting. Use the next year to evaluate the other admission controller choices in the ecosystem. There are resources available to ease this transition, such as the &lt;a href="https://github.com/open-policy-agent/gatekeeper-library">Gatekeeper Policy Library&lt;/a>.&lt;/p>
&lt;p>If your use of PSP is relatively simple, with a few policies and straightforward binding to service accounts in each namespace, you will likely find PSP Replacement Policy to be a good match for your needs. Evaluate your PSPs compared to the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a> to get a feel for where you’ll be able to use the Restricted, Baseline, and Privileged policies. Please follow along with or contribute to the KEP and subsequent development, and try out the Alpha release of PSP Replacement Policy when it becomes available.&lt;/p>
&lt;p>If you’re just beginning your PSP journey, you will save time and effort by keeping it simple. You can approximate the functionality of PSP Replacement Policy today by using the Pod Security Standards’ PSPs. If you set the cluster default by binding a Baseline or Restricted policy to the &lt;code>system:serviceaccounts&lt;/code> group, and then make a more-permissive policy available as needed in certain Namespaces &lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#run-another-pod">using ServiceAccount bindings&lt;/a>, you will avoid many of the PSP pitfalls and have an easy migration to PSP Replacement Policy. If your needs are much more complex than this, your effort is probably better spent adopting one of the more fully-featured external admission controllers mentioned above.&lt;/p>
&lt;p>We’re dedicated to making Kubernetes the best container orchestration tool we can, and sometimes that means we need to remove longstanding features to make space for better things to come. When that happens, the Kubernetes deprecation policy ensures you have plenty of time to plan your next move. In the case of PodSecurityPolicy, several options are available to suit a range of needs and use cases. Start planning ahead now for PSP’s eventual removal, and please consider contributing to its replacement! Happy securing!&lt;/p>
&lt;p>&lt;strong>Acknowledgment:&lt;/strong> It takes a wonderful group to make wonderful software. Thanks are due to everyone who has contributed to the PSP replacement effort, especially (in alphabetical order) Tim Allclair, Ian Coldwater, and Jordan Liggitt. It’s been a joy to work with y’all on this.&lt;/p></description></item><item><title>Blog: The Evolution of Kubernetes Dashboard</title><link>https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/</guid><description>
&lt;p>Authors: Marcin Maciaszczyk, Kubermatic &amp;amp; Sebastian Florek, Kubermatic&lt;/p>
&lt;p>In October 2020, the Kubernetes Dashboard officially turned five. As main project maintainers, we can barely believe that so much time has passed since our very first commits to the project. However, looking back with a bit of nostalgia, we realize that quite a lot has happened since then. Now it’s due time to celebrate “our baby” with a short recap.&lt;/p>
&lt;h2 id="how-it-all-began">How It All Began&lt;/h2>
&lt;p>The initial idea behind the Kubernetes Dashboard project was to provide a web interface for Kubernetes. We wanted to reflect the kubectl functionality through an intuitive web UI. The main benefit from using the UI is to be able to quickly see things that do not work as expected (monitoring and troubleshooting). Also, the Kubernetes Dashboard is a great starting point for users that are new to the Kubernetes ecosystem.&lt;/p>
&lt;p>The very &lt;a href="https://github.com/kubernetes/dashboard/commit/5861187fa807ac1cc2d9b2ac786afeced065076c">first commit&lt;/a> to the Kubernetes Dashboard was made by Filip Grządkowski from Google on 16th October 2015 – just a few months from the initial commit to the Kubernetes repository. Our initial commits go back to November 2015 (&lt;a href="https://github.com/kubernetes/dashboard/commit/09e65b6bb08c49b926253de3621a73da05e400fd">Sebastian committed on 16 November 2015&lt;/a>; &lt;a href="https://github.com/kubernetes/dashboard/commit/1da4b1c25ef040818072c734f71333f9b4733f55">Marcin committed on 23 November 2015&lt;/a>). Since that time, we’ve become regular contributors to the project. For the next two years, we worked closely with the Googlers, eventually becoming main project maintainers ourselves.&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/first-ui.png"
alt="The First Version of the User Interface"/> &lt;figcaption>
&lt;p>The First Version of the User Interface&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/along-the-way-ui.png"
alt="Prototype of the New User Interface"/> &lt;figcaption>
&lt;p>Prototype of the New User Interface&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;figure>
&lt;img src="https://kubernetes.io/blog/2021/03/09/the-evolution-of-kubernetes-dashboard/current-ui.png"
alt="The Current User Interface"/> &lt;figcaption>
&lt;p>The Current User Interface&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;p>As you can see, the initial look and feel of the project were completely different from the current one. We have changed the design multiple times. The same has happened with the code itself.&lt;/p>
&lt;h2 id="growing-up-the-big-migration">Growing Up - The Big Migration&lt;/h2>
&lt;p>At &lt;a href="https://github.com/kubernetes/dashboard/pull/2727">the beginning of 2018&lt;/a>, we reached a point where AngularJS was getting closer to the end of its life, while the new Angular versions were published quite often. A lot of the libraries and the modules that we were using were following the trend. That forced us to spend a lot of the time rewriting the frontend part of the project to make it work with newer technologies.&lt;/p>
&lt;p>The migration came with many benefits like being able to refactor a lot of the code, introduce design patterns, reduce code complexity, and benefit from the new modules. However, you can imagine that the scale of the migration was huge. Luckily, there were a number of contributions from the community helping us with the resource support, new Kubernetes version support, i18n, and much more. After many long days and nights, we finally released the &lt;a href="https://github.com/kubernetes/dashboard/releases/tag/v2.0.0-beta1">first beta version&lt;/a> in July 2019, followed by the &lt;a href="https://github.com/kubernetes/dashboard/releases/tag/v2.0.0">2.0 release&lt;/a> in April 2020 — our baby had grown up.&lt;/p>
&lt;h2 id="where-are-we-standing-in-2021">Where Are We Standing in 2021?&lt;/h2>
&lt;p>Due to limited resources, unfortunately, we were not able to offer extensive support for many different Kubernetes versions. So, we’ve decided to always try and support the latest Kubernetes version available at the time of the Kubernetes Dashboard release. The latest release, &lt;a href="https://github.com/kubernetes/dashboard/releases/tag/v2.2.0">Dashboard v2.2.0&lt;/a> provides support for Kubernetes v1.20.&lt;/p>
&lt;p>On top of that, we put in a great deal of effort into &lt;a href="https://github.com/kubernetes/dashboard/issues/5232">improving resource support&lt;/a>. Meanwhile, we do offer support for most of the Kubernetes resources. Also, the Kubernetes Dashboard supports multiple languages: English, German, French, Japanese, Korean, Chinese (Traditional, Simplified, Traditional Hong Kong). Persian and Russian localizations are currently in progress. Moreover, we are working on the support for 3rd party themes and the design of the app in general. As you can see, quite a lot of things are going on.&lt;/p>
&lt;p>Luckily, we do have regular contributors with domain knowledge who are taking care of the project, updating the Helm charts, translations, Go modules, and more. But as always, there could be many more hands on deck. So if you are thinking about contributing to Kubernetes, keep us in mind ;)&lt;/p>
&lt;h2 id="what-s-next">What’s Next&lt;/h2>
&lt;p>The Kubernetes Dashboard has been growing and prospering for more than 5 years now. It provides the community with an intuitive Web UI, thereby decreasing the complexity of Kubernetes and increasing its accessibility to new community members. We are proud of what the project has achieved so far, but this is by far not the end. These are our priorities for the future:&lt;/p>
&lt;ul>
&lt;li>Keep providing support for the new Kubernetes versions&lt;/li>
&lt;li>Keep improving the support for the existing resources&lt;/li>
&lt;li>Keep working on auth system improvements&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/dashboard/pull/5449">Rewrite the API to use gRPC and shared informers&lt;/a>: This will allow us to improve the performance of the application but, most importantly, to support live updates coming from the Kubernetes project. It is one of the most requested features from the community.&lt;/li>
&lt;li>Split the application into two containers, one with the UI and the second with the API running inside.&lt;/li>
&lt;/ul>
&lt;h2 id="the-kubernetes-dashboard-in-numbers">The Kubernetes Dashboard in Numbers&lt;/h2>
&lt;ul>
&lt;li>Initial commit made on October 16, 2015&lt;/li>
&lt;li>Over 100 million pulls from Dockerhub since the v2 release&lt;/li>
&lt;li>8 supported languages and the next 2 in progress&lt;/li>
&lt;li>Over 3360 closed PRs&lt;/li>
&lt;li>Over 2260 closed issues&lt;/li>
&lt;li>100% coverage of the supported core Kubernetes resources&lt;/li>
&lt;li>Over 9000 stars on GitHub&lt;/li>
&lt;li>Over 237 000 lines of code&lt;/li>
&lt;/ul>
&lt;h2 id="join-us">Join Us&lt;/h2>
&lt;p>As mentioned earlier, we are currently looking for more people to help us further develop and grow the project. We are open to contributions in multiple areas, i.e., &lt;a href="https://github.com/kubernetes/dashboard/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22">issues with help wanted label&lt;/a>. Please feel free to reach out via GitHub or the #sig-ui channel in the &lt;a href="https://slack.k8s.io/">Kubernetes Slack&lt;/a>.&lt;/p></description></item><item><title>Blog: A Custom Kubernetes Scheduler to Orchestrate Highly Available Applications</title><link>https://kubernetes.io/blog/2020/12/21/writing-crl-scheduler/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/21/writing-crl-scheduler/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Chris Seto (Cockroach Labs)&lt;/p>
&lt;p>As long as you're willing to follow the rules, deploying on Kubernetes and air travel can be quite pleasant. More often than not, things will &amp;quot;just work&amp;quot;. However, if one is interested in travelling with an alligator that must remain alive or scaling a database that must remain available, the situation is likely to become a bit more complicated. It may even be easier to build one's own plane or database for that matter. Travelling with reptiles aside, scaling a highly available stateful system is no trivial task.&lt;/p>
&lt;p>Scaling any system has two main components:&lt;/p>
&lt;ol>
&lt;li>Adding or removing infrastructure that the system will run on, and&lt;/li>
&lt;li>Ensuring that the system knows how to handle additional instances of itself being added and removed.&lt;/li>
&lt;/ol>
&lt;p>Most stateless systems, web servers for example, are created without the need to be aware of peers. Stateful systems, which includes databases like CockroachDB, have to coordinate with their peer instances and shuffle around data. As luck would have it, CockroachDB handles data redistribution and replication. The tricky part is being able to tolerate failures during these operations by ensuring that data and instances are distributed across many failure domains (availability zones).&lt;/p>
&lt;p>One of Kubernetes' responsibilities is to place &amp;quot;resources&amp;quot; (e.g, a disk or container) into the cluster and satisfy the constraints they request. For example: &amp;quot;I must be in availability zone &lt;em>A&lt;/em>&amp;quot; (see &lt;a href="https://kubernetes.io/docs/setup/best-practices/multiple-zones/#nodes-are-labeled">Running in multiple zones&lt;/a>), or &amp;quot;I can't be placed onto the same node as this other Pod&amp;quot; (see &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">Affinity and anti-affinity&lt;/a>).&lt;/p>
&lt;p>As an addition to those constraints, Kubernetes offers &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">Statefulsets&lt;/a> that provide identity to Pods as well as persistent storage that &amp;quot;follows&amp;quot; these identified pods. Identity in a StatefulSet is handled by an increasing integer at the end of a pod's name. It's important to note that this integer must always be contiguous: in a StatefulSet, if pods 1 and 3 exist then pod 2 must also exist.&lt;/p>
&lt;p>Under the hood, CockroachCloud deploys each region of CockroachDB as a StatefulSet in its own Kubernetes cluster - see &lt;a href="https://www.cockroachlabs.com/docs/stable/orchestrate-cockroachdb-with-kubernetes.html">Orchestrate CockroachDB in a Single Kubernetes Cluster&lt;/a>.
In this article, I'll be looking at an individual region, one StatefulSet and one Kubernetes cluster which is distributed across at least three availability zones.&lt;/p>
&lt;p>A three-node CockroachCloud cluster would look something like this:&lt;/p>
&lt;p>&lt;img src="image01.png" alt="3-node, multi-zone cockroachdb cluster">&lt;/p>
&lt;p>When adding additional resources to the cluster we also distribute them across zones. For the speediest user experience, we add all Kubernetes nodes at the same time and then scale up the StatefulSet.&lt;/p>
&lt;p>&lt;img src="image02.png" alt="illustration of phases: adding Kubernetes nodes to the multi-zone cockroachdb cluster">&lt;/p>
&lt;p>Note that anti-affinities are satisfied no matter the order in which pods are assigned to Kubernetes nodes. In the example, pods 0, 1 and 2 were assigned to zones A, B, and C respectively, but pods 3 and 4 were assigned in a different order, to zones B and A respectively. The anti-affinity is still satisfied because the pods are still placed in different zones.&lt;/p>
&lt;p>To remove resources from a cluster, we perform these operations in reverse order.&lt;/p>
&lt;p>We first scale down the StatefulSet and then remove from the cluster any nodes lacking a CockroachDB pod.&lt;/p>
&lt;p>&lt;img src="image03.png" alt="illustration of phases: scaling down pods in a multi-zone cockroachdb cluster in Kubernetes">&lt;/p>
&lt;p>Now, remember that pods in a StatefulSet of size &lt;em>n&lt;/em> must have ids in the range &lt;code>[0,n)&lt;/code>. When scaling down a StatefulSet by &lt;em>m&lt;/em>, Kubernetes removes &lt;em>m&lt;/em> pods, starting from the highest ordinals and moving towards the lowest, &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees">the reverse in which they were added&lt;/a>.
Consider the cluster topology below:&lt;/p>
&lt;p>&lt;img src="image04.png" alt="illustration: cockroachdb cluster: 6 nodes distributed across 3 availability zones">&lt;/p>
&lt;p>As ordinals 5 through 3 are removed from this cluster, the statefulset continues to have a presence across all 3 availability zones.&lt;/p>
&lt;p>&lt;img src="image05.png" alt="illustration: removing 3 nodes from a 6-node, 3-zone cockroachdb cluster">&lt;/p>
&lt;p>However, Kubernetes' scheduler doesn't &lt;em>guarantee&lt;/em> the placement above as we expected at first.&lt;/p>
&lt;p>Our combined knowledge of the following is what lead to this misconception.&lt;/p>
&lt;ul>
&lt;li>Kubernetes' ability to &lt;a href="https://kubernetes.io/docs/setup/best-practices/multiple-zones/#pods-are-spread-across-zones">automatically spread Pods across zone&lt;/a>&lt;/li>
&lt;li>The behavior that a StatefulSet with &lt;em>n&lt;/em> replicas, when Pods are being deployed, they are created sequentially, in order from &lt;code>{0..n-1}&lt;/code>. See &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees">StatefulSet&lt;/a> for more details.&lt;/li>
&lt;/ul>
&lt;p>Consider the following topology:&lt;/p>
&lt;p>&lt;img src="image06.png" alt="illustration: 6-node cockroachdb cluster distributed across 3 availability zones">&lt;/p>
&lt;p>These pods were created in order and they are spread across all availability zones in the cluster. When ordinals 5 through 3 are terminated, this cluster will lose its presence in zone C!&lt;/p>
&lt;p>&lt;img src="image07.png" alt="illustration: terminating 3 nodes in 6-node cluster spread across 3 availability zones, where 2/2 nodes in the same availability zone are terminated, knocking out that AZ">&lt;/p>
&lt;p>Worse yet, our automation, at the time, would remove Nodes A-2, B-2, and C-2. Leaving CRDB-1 in an unscheduled state as persistent volumes are only available in the zone they are initially created in.&lt;/p>
&lt;p>To correct the latter issue, we now employ a &amp;quot;hunt and peck&amp;quot; approach to removing machines from a cluster. Rather than blindly removing Kubernetes nodes from the cluster, only nodes without a CockroachDB pod would be removed. The much more daunting task was to wrangle the Kubernetes scheduler.&lt;/p>
&lt;h2 id="a-session-of-brainstorming-left-us-with-3-options">A session of brainstorming left us with 3 options:&lt;/h2>
&lt;h3 id="1-upgrade-to-kubernetes-1-18-and-make-use-of-pod-topology-spread-constraints">1. Upgrade to kubernetes 1.18 and make use of Pod Topology Spread Constraints&lt;/h3>
&lt;p>While this seems like it could have been the perfect solution, at the time of writing Kubernetes 1.18 was unavailable on the two most common managed Kubernetes services in public cloud, EKS and GKE.
Furthermore, &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">pod topology spread constraints&lt;/a> were still a &lt;a href="https://v1-18.docs.kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">beta feature in 1.18&lt;/a> which meant that it &lt;a href="https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters#kubernetes_feature_choices">wasn't guaranteed to be available in managed clusters&lt;/a> even when v1.18 became available.
The entire endeavour was concerningly reminiscent of checking &lt;a href="https://caniuse.com/">caniuse.com&lt;/a> when Internet Explorer 8 was still around.&lt;/p>
&lt;h3 id="2-deploy-a-statefulset-per-zone">2. Deploy a statefulset &lt;em>per zone&lt;/em>.&lt;/h3>
&lt;p>Rather than having one StatefulSet distributed across all availability zones, a single StatefulSet with node affinities per zone would allow manual control over our zonal topology.
Our team had considered this as an option in the past which made it particularly appealing.
Ultimately, we decided to forego this option as it would have required a massive overhaul to our codebase and performing the migration on existing customer clusters would have been an equally large undertaking.&lt;/p>
&lt;h3 id="3-write-a-custom-kubernetes-scheduler">3. Write a custom Kubernetes scheduler.&lt;/h3>
&lt;p>Thanks to an example from &lt;a href="https://github.com/kelseyhightower/scheduler">Kelsey Hightower&lt;/a> and a blog post from &lt;a href="https://banzaicloud.com/blog/k8s-custom-scheduler/">Banzai Cloud&lt;/a>, we decided to dive in head first and write our own &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/">custom Kubernetes scheduler&lt;/a>.
Once our proof-of-concept was deployed and running, we quickly discovered that the Kubernetes' scheduler is also responsible for mapping persistent volumes to the Pods that it schedules.
The output of &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/#verifying-that-the-pods-were-scheduled-using-the-desired-schedulers">&lt;code>kubectl get events&lt;/code>&lt;/a> had led us to believe there was another system at play.
In our journey to find the component responsible for storage claim mapping, we discovered the &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/">kube-scheduler plugin system&lt;/a>. Our next POC was a &lt;code>Filter&lt;/code> plugin that determined the appropriate availability zone by pod ordinal, and it worked flawlessly!&lt;/p>
&lt;p>Our &lt;a href="https://github.com/cockroachlabs/crl-scheduler">custom scheduler plugin&lt;/a> is open source and runs in all of our CockroachCloud clusters.
Having control over how our StatefulSet pods are being scheduled has let us scale out with confidence.
We may look into retiring our plugin once pod topology spread constraints are available in GKE and EKS, but the maintenance overhead has been surprisingly low.
Better still: the plugin's implementation is orthogonal to our business logic. Deploying it, or retiring it for that matter, is as simple as changing the &lt;code>schedulerName&lt;/code> field in our StatefulSet definitions.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/_ostriches">Chris Seto&lt;/a> is a software engineer at Cockroach Labs and works on their Kubernetes automation for &lt;a href="https://cockroachlabs.cloud">CockroachCloud&lt;/a>, CockroachDB.&lt;/em>&lt;/p></description></item><item><title>Blog: Kubernetes 1.20: Pod Impersonation and Short-lived Volumes in CSI Drivers</title><link>https://kubernetes.io/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Shihang Zhang (Google)&lt;/p>
&lt;p>Typically when a &lt;a href="https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md">CSI&lt;/a> driver mounts credentials such as secrets and certificates, it has to authenticate against storage providers to access the credentials. However, the access to those credentials are controlled on the basis of the pods' identities rather than the CSI driver's identity. CSI drivers, therefore, need some way to retrieve pod's service account token.&lt;/p>
&lt;p>Currently there are two suboptimal approaches to achieve this, either by granting CSI drivers the permission to use TokenRequest API or by reading tokens directly from the host filesystem.&lt;/p>
&lt;p>Both of them exhibit the following drawbacks:&lt;/p>
&lt;ul>
&lt;li>Violating the principle of least privilege&lt;/li>
&lt;li>Every CSI driver needs to re-implement the logic of getting the pod’s service account token&lt;/li>
&lt;/ul>
&lt;p>The second approach is more problematic due to:&lt;/p>
&lt;ul>
&lt;li>The audience of the token defaults to the kube-apiserver&lt;/li>
&lt;li>The token is not guaranteed to be available (e.g. &lt;code>AutomountServiceAccountToken=false&lt;/code>)&lt;/li>
&lt;li>The approach does not work for CSI drivers that run as a different (non-root) user from the pods. See &lt;a href="https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission">file permission section for service account token&lt;/a>&lt;/li>
&lt;li>The token might be legacy Kubernetes service account token which doesn’t expire if &lt;code>BoundServiceAccountTokenVolume=false&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Kubernetes 1.20 introduces an alpha feature, &lt;code>CSIServiceAccountToken&lt;/code>, to improve the security posture. The new feature allows CSI drivers to receive pods' &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md">bound service account tokens&lt;/a>.&lt;/p>
&lt;p>This feature also provides a knob to re-publish volumes so that short-lived volumes can be refreshed.&lt;/p>
&lt;h2 id="pod-impersonation">Pod Impersonation&lt;/h2>
&lt;h3 id="using-gcp-apis">Using GCP APIs&lt;/h3>
&lt;p>Using &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity&lt;/a>, a Kubernetes service account can authenticate as a Google service account when accessing Google Cloud APIs. If a CSI driver needs to access GCP APIs on behalf of the pods that it is mounting volumes for, it can use the pod's service account token to &lt;a href="https://cloud.google.com/iam/docs/reference/sts/rest">exchange for GCP tokens&lt;/a>. The pod's service account token is plumbed through the volume context in &lt;code>NodePublishVolume&lt;/code> RPC calls when the feature &lt;code>CSIServiceAccountToken&lt;/code> is enabled. For example: accessing &lt;a href="https://cloud.google.com/secret-manager/">Google Secret Manager&lt;/a> via a &lt;a href="https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp">secret store CSI driver&lt;/a>.&lt;/p>
&lt;h3 id="using-vault">Using Vault&lt;/h3>
&lt;p>If users configure &lt;a href="https://www.vaultproject.io/docs/auth/kubernetes">Kubernetes as an auth method&lt;/a>, Vault uses the &lt;code>TokenReview&lt;/code> API to validate the Kubernetes service account token. For CSI drivers using Vault as resources provider, they need to present the pod's service account to Vault. For example, &lt;a href="https://github.com/hashicorp/secrets-store-csi-driver-provider-vault">secrets store CSI driver&lt;/a> and &lt;a href="https://github.com/jetstack/cert-manager-csi">cert manager CSI driver&lt;/a>.&lt;/p>
&lt;h2 id="short-lived-volumes">Short-lived Volumes&lt;/h2>
&lt;p>To keep short-lived volumes such as certificates effective, CSI drivers can specify &lt;code>RequiresRepublish=true&lt;/code> in their&lt;code>CSIDriver&lt;/code> object to have the kubelet periodically call &lt;code>NodePublishVolume&lt;/code> on mounted volumes. These republishes allow CSI drivers to ensure that the volume content is up-to-date.&lt;/p>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>This feature is alpha and projected to move to beta in 1.21. See more in the following KEP and CSI documentation:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md">KEP-1855: Service Account Token for CSI Driver&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes-csi.github.io/docs/token-requests.html">Token Requests&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Your feedback is always welcome!&lt;/p>
&lt;ul>
&lt;li>SIG-Auth &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#meetings">meets regularly&lt;/a> and can be reached via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#contact">Slack and the mailing list&lt;/a>&lt;/li>
&lt;li>SIG-Storage &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">meets regularly&lt;/a> and can be reached via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#contact">Slack and the mailing list&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Third Party Device Metrics Reaches GA</title><link>https://kubernetes.io/blog/2020/12/16/third-party-device-metrics-reaches-ga/</link><pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/16/third-party-device-metrics-reaches-ga/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Renaud Gaubert (NVIDIA), David Ashpole (Google), and Pramod Ramarao (NVIDIA)&lt;/p>
&lt;p>With Kubernetes 1.20, infrastructure teams who manage large scale Kubernetes clusters, are seeing the graduation of two exciting and long awaited features:&lt;/p>
&lt;ul>
&lt;li>The Pod Resources API (introduced in 1.13) is finally graduating to GA. This allows Kubernetes plugins to obtain information about the node’s resource usage and assignment; for example: which pod/container consumes which device.&lt;/li>
&lt;li>The &lt;code>DisableAcceleratorMetrics&lt;/code> feature (introduced in 1.19) is graduating to beta and will be enabled by default. This removes device metrics reported by the kubelet in favor of the new plugin architecture.&lt;/li>
&lt;/ul>
&lt;p>Many of the features related to fundamental device support (device discovery, plugin, and monitoring) are reaching a strong level of stability.
Kubernetes users should see these features as stepping stones to enable more complex use cases (networking, scheduling, storage, etc.)!&lt;/p>
&lt;p>One such example is Non Uniform Memory Access (NUMA) placement where, when selecting a device, an application typically wants to ensure that data transfer between CPU Memory and Device Memory is as fast as possible. In some cases, incorrect NUMA placement can nullify the benefit of offloading compute to an external device.&lt;/p>
&lt;p>If these are topics of interest to you, consider joining the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">Kubernetes Node Special Insterest Group&lt;/a> (SIG) for all topics related to the Kubernetes node, the COD (container orchestrated device) workgroup for topics related to runtimes, or the resource management forum for topics related to resource management!&lt;/p>
&lt;h2 id="the-pod-resources-api-why-does-it-need-to-exist">The Pod Resources API - Why does it need to exist?&lt;/h2>
&lt;p>Kubernetes is a vendor neutral platform. If we want it to support device monitoring, adding vendor-specific code in the Kubernetes code base is not an ideal solution. Ultimately, devices are a domain where deep expertise is needed and the best people to add and maintain code in that area are the device vendors themselves.&lt;/p>
&lt;p>The Pod Resources API was built as a solution to this issue. Each vendor can build and maintain their own out-of-tree monitoring plugin. This monitoring plugin, often deployed as a separate pod within a cluster, can then associate the metrics a device emits with the associated pod that's using it.&lt;/p>
&lt;p>For example, use the NVIDIA GPU dcgm-exporter to scrape metrics in Prometheus format:&lt;/p>
&lt;pre>&lt;code>$ curl -sL http://127.0.01:8080/metrics
# HELP DCGM_FI_DEV_SM_CLOCK SM clock frequency (in MHz).
# TYPE DCGM_FI_DEV_SM_CLOCK gauge
# HELP DCGM_FI_DEV_MEM_CLOCK Memory clock frequency (in MHz).
# TYPE DCGM_FI_DEV_MEM_CLOCK gauge
# HELP DCGM_FI_DEV_MEMORY_TEMP Memory temperature (in C).
# TYPE DCGM_FI_DEV_MEMORY_TEMP gauge
...
DCGM_FI_DEV_SM_CLOCK{gpu=&amp;quot;0&amp;quot;, UUID=&amp;quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&amp;quot;,container=&amp;quot;foo&amp;quot;,namespace=&amp;quot;bar&amp;quot;,pod=&amp;quot;baz&amp;quot;} 139
DCGM_FI_DEV_MEM_CLOCK{gpu=&amp;quot;0&amp;quot;, UUID=&amp;quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&amp;quot;,container=&amp;quot;foo&amp;quot;,namespace=&amp;quot;bar&amp;quot;,pod=&amp;quot;baz&amp;quot;} 405
DCGM_FI_DEV_MEMORY_TEMP{gpu=&amp;quot;0&amp;quot;, UUID=&amp;quot;GPU-604ac76c-d9cf-fef3-62e9-d92044ab6e52&amp;quot;,container=&amp;quot;foo&amp;quot;,namespace=&amp;quot;bar&amp;quot;,pod=&amp;quot;baz&amp;quot;} 9223372036854775794
&lt;/code>&lt;/pre>&lt;p>Each agent is expected to adhere to the node monitoring guidelines. In other words, plugins are expected to generate metrics in Prometheus format, and new metrics should not have any dependency on the Kubernetes base directly.&lt;/p>
&lt;p>This allows consumers of the metrics to use a compatible monitoring pipeline to collect and analyze metrics from a variety of agents, even if they are maintained by different vendors.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-12-16-third-party-device-metrics-hits-ga/metrics-chart.png" alt="Device metrics flowchart">&lt;/p>
&lt;h2 id="nvidia-gpu-metrics-deprecated">Disabling the NVIDIA GPU metrics - Warning&lt;/h2>
&lt;p>With the graduation of the plugin monitoring system, Kubernetes is deprecating the NVIDIA GPU metrics that are being reported by the kubelet.&lt;/p>
&lt;p>With the &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/#disable-accelerator-metrics">DisableAcceleratorMetrics&lt;/a> feature being enabled by default in Kubernetes 1.20, NVIDIA GPUs are no longer special citizens in Kubernetes. This is a good thing in the spirit of being vendor-neutral, and enables the most suited people to maintain their plugin on their own release schedule!&lt;/p>
&lt;p>Users will now need to either install the &lt;a href="https://github.com/NVIDIA/gpu-monitoring-tools">NVIDIA GDGM exporter&lt;/a> or use &lt;a href="https://github.com/nvidia/go-nvml">bindings&lt;/a> to gather more accurate and complete metrics about NVIDIA GPUs. This deprecation means that you can no longer rely on metrics that were reported by kubelet, such as &lt;code>container_accelerator_duty_cycle&lt;/code> or &lt;code>container_accelerator_memory_used_bytes&lt;/code> which were used to gather NVIDIA GPU memory utilization.&lt;/p>
&lt;p>This means that users who used to rely on the NVIDIA GPU metrics reported by the kubelet, will need to update their reference and deploy the NVIDIA plugin. Namely the different metrics reported by Kubernetes map to the following metrics:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Kubernetes Metrics&lt;/th>
&lt;th>NVIDIA dcgm-exporter metric&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>container_accelerator_duty_cycle&lt;/code>&lt;/td>
&lt;td>&lt;code>DCGM_FI_DEV_GPU_UTIL&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>container_accelerator_memory_used_bytes&lt;/code>&lt;/td>
&lt;td>&lt;code>DCGM_FI_DEV_FB_USED&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>container_accelerator_memory_total_bytes&lt;/code>&lt;/td>
&lt;td>&lt;code>DCGM_FI_DEV_FB_FREE + DCGM_FI_DEV_FB_USED&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>You might also be interested in other metrics such as &lt;code>DCGM_FI_DEV_GPU_TEMP&lt;/code> (the GPU temperature) or DCGM_FI_DEV_POWER_USAGE (the power usage). The &lt;a href="https://github.com/NVIDIA/gpu-monitoring-tools/blob/d5c9bb55b4d1529ca07068b7f81e690921ce2b59/etc/dcgm-exporter/default-counters.csv">default set&lt;/a> is available in Nvidia's &lt;a href="https://docs.nvidia.com/datacenter/dcgm/latest/dcgm-api/group__dcgmFieldIdentifiers.html">Data Center GPU Manager documentation&lt;/a>.&lt;/p>
&lt;p>Note that for this release you can still set the &lt;code>DisableAcceleratorMetrics&lt;/code> &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a> to &lt;em>false&lt;/em>, effectively re-enabling the ability for the kubelet to report NVIDIA GPU metrics.&lt;/p>
&lt;p>Paired with the graduation of the Pod Resources API, these tools can be used to generate GPU telemetry &lt;a href="https://grafana.com/grafana/dashboards/12239">that can be used in visualization dashboards&lt;/a>, below is an example:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-12-16-third-party-device-metrics-hits-ga/grafana.png" alt="Grafana visualization of device metrics">&lt;/p>
&lt;h2 id="the-pod-resources-api-what-can-i-go-on-to-do-with-this">The Pod Resources API - What can I go on to do with this?&lt;/h2>
&lt;p>As soon as this interface was introduced, many vendors started using it for widely different use cases! To list a few examples:&lt;/p>
&lt;p>The &lt;a href="https://github.com/openstack/kuryr-kubernetes">kuryr-kubernetes&lt;/a> CNI plugin in tandem with &lt;a href="https://github.com/intel/sriov-network-device-plugin">intel-sriov-device-plugin&lt;/a>. This allowed the CNI plugin to know which allocation of SR-IOV Virtual Functions (VFs) the kubelet made and use that information to correctly setup the container network namespace and use a device with the appropriate NUMA node. We also expect this interface to be used to track the allocated and available resources with information about the NUMA topology of the worker node.&lt;/p>
&lt;p>Another use-case is GPU telemetry, where GPU metrics can be associated with the containers and pods that the GPU is assigned to. One such example is the NVIDIA &lt;code>dcgm-exporter&lt;/code>, but others can be easily built in the same paradigm.&lt;/p>
&lt;p>The Pod Resources API is a simple gRPC service which informs clients of the pods the kubelet knows. The information concerns the devices assignment the kubelet made and the assignment of CPUs. This information is obtained from the internal state of the kubelet's Device Manager and CPU Manager respectively.&lt;/p>
&lt;p>You can see below a sample example of the API and how a go client could use that information in a few lines:&lt;/p>
&lt;pre>&lt;code>service PodResourcesLister {
rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}
rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}
// Kubernetes 1.21
rpc Watch(WatchPodResourcesRequest) returns (stream WatchPodResourcesResponse) {}
}
&lt;/code>&lt;/pre>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">main&lt;/span>() {
ctx, cancel &lt;span style="color:#666">:=&lt;/span> context.&lt;span style="color:#00a000">WithTimeout&lt;/span>(context.&lt;span style="color:#00a000">Background&lt;/span>(), connectionTimeout)
&lt;span style="color:#a2f;font-weight:bold">defer&lt;/span> &lt;span style="color:#00a000">cancel&lt;/span>()
socket &lt;span style="color:#666">:=&lt;/span> &lt;span style="color:#b44">&amp;#34;/var/lib/kubelet/pod-resources/kubelet.sock&amp;#34;&lt;/span>
conn, err &lt;span style="color:#666">:=&lt;/span> grpc.&lt;span style="color:#00a000">DialContext&lt;/span>(ctx, socket, grpc.&lt;span style="color:#00a000">WithInsecure&lt;/span>(), grpc.&lt;span style="color:#00a000">WithBlock&lt;/span>(),
grpc.&lt;span style="color:#00a000">WithDialer&lt;/span>(&lt;span style="color:#a2f;font-weight:bold">func&lt;/span>(addr &lt;span style="color:#0b0;font-weight:bold">string&lt;/span>, timeout time.Duration) (net.Conn, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>) {
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> net.&lt;span style="color:#00a000">DialTimeout&lt;/span>(&lt;span style="color:#b44">&amp;#34;unix&amp;#34;&lt;/span>, addr, timeout)
}),
)
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#a2f">panic&lt;/span>(err)
}
client &lt;span style="color:#666">:=&lt;/span> podresourcesapi.&lt;span style="color:#00a000">NewPodResourcesListerClient&lt;/span>(conn)
resp, err &lt;span style="color:#666">:=&lt;/span> client.&lt;span style="color:#00a000">List&lt;/span>(ctx, &lt;span style="color:#666">&amp;amp;&lt;/span>podresourcesapi.ListPodResourcesRequest{})
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> err &lt;span style="color:#666">!=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#a2f">panic&lt;/span>(err)
}
net.&lt;span style="color:#00a000">Printf&lt;/span>(&lt;span style="color:#b44">&amp;#34;%+v\n&amp;#34;&lt;/span>, resp)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, note that you can watch the number of requests made to the Pod Resources endpoint by watching the new kubelet metric called &lt;code>pod_resources_endpoint_requests_total&lt;/code> on the kubelet's &lt;code>/metrics&lt;/code> endpoint.&lt;/p>
&lt;h2 id="is-device-monitoring-suitable-for-production-can-i-extend-it-can-i-contribute">Is device monitoring suitable for production? Can I extend it? Can I contribute?&lt;/h2>
&lt;p>Yes! This feature released in 1.13, almost 2 years ago, has seen broad adoption, is already used by different cloud managed services, and with its graduation to G.A in Kubernetes 1.20 is production ready!&lt;/p>
&lt;p>If you are a device vendor, you can start using it today! If you just want to monitor the devices in your cluster, go get the latest version of your monitoring plugin!&lt;/p>
&lt;p>If you feel passionate about that area, join the kubernetes community, help improve the API or contribute the device monitoring plugins!&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>We thank the members of the community who have contributed to this feature or given feedback including members of WG-Resource-Management, SIG-Node and the Resource management forum!&lt;/p></description></item><item><title>Blog: Kubernetes 1.20: Granular Control of Volume Permission Changes</title><link>https://kubernetes.io/blog/2020/12/14/kubernetes-release-1.20-fsgroupchangepolicy-fsgrouppolicy/</link><pubDate>Mon, 14 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/14/kubernetes-release-1.20-fsgroupchangepolicy-fsgrouppolicy/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Hemant Kumar, Red Hat &amp;amp; Christian Huffman, Red Hat&lt;/p>
&lt;p>Kubernetes 1.20 brings two important beta features, allowing Kubernetes admins and users alike to have more adequate control over how volume permissions are applied when a volume is mounted inside a Pod.&lt;/p>
&lt;h3 id="allow-users-to-skip-recursive-permission-changes-on-mount">Allow users to skip recursive permission changes on mount&lt;/h3>
&lt;p>Traditionally if your pod is running as a non-root user (&lt;a href="https://twitter.com/thockin/status/1333892204490735617">which you should&lt;/a>), you must specify a &lt;code>fsGroup&lt;/code> inside the pod’s security context so that the volume can be readable and writable by the Pod. This requirement is covered in more detail in &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">here&lt;/a>.&lt;/p>
&lt;p>But one side-effect of setting &lt;code>fsGroup&lt;/code> is that, each time a volume is mounted, Kubernetes must recursively &lt;code>chown()&lt;/code> and &lt;code>chmod()&lt;/code> all the files and directories inside the volume - with a few exceptions noted below. This happens even if group ownership of the volume already matches the requested &lt;code>fsGroup&lt;/code>, and can be pretty expensive for larger volumes with lots of small files, which causes pod startup to take a long time. This scenario has been a &lt;a href="https://github.com/kubernetes/kubernetes/issues/69699">known problem&lt;/a> for a while, and in Kubernetes 1.20 we are providing knobs to opt-out of recursive permission changes if the volume already has the correct permissions.&lt;/p>
&lt;p>When configuring a pod’s security context, set &lt;code>fsGroupChangePolicy&lt;/code> to &amp;quot;OnRootMismatch&amp;quot; so if the root of the volume already has the correct permissions, the recursive permission change can be skipped. Kubernetes ensures that permissions of the top-level directory are changed last the first time it applies permissions.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">runAsUser&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">runAsGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fsGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">2000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fsGroupChangePolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;OnRootMismatch&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can learn more about this in &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods">Configure volume permission and ownership change policy for Pods&lt;/a>.&lt;/p>
&lt;h3 id="allow-csi-drivers-to-declare-support-for-fsgroup-based-permissions">Allow CSI Drivers to declare support for fsGroup based permissions&lt;/h3>
&lt;p>Although the previous section implied that Kubernetes &lt;em>always&lt;/em> recursively changes permissions of a volume if a Pod has a &lt;code>fsGroup&lt;/code>, this is not strictly true. For certain multi-writer volume types, such as NFS or Gluster, the cluster doesn’t perform recursive permission changes even if the pod has a &lt;code>fsGroup&lt;/code>. Other volume types may not even support &lt;code>chown()&lt;/code>/&lt;code>chmod()&lt;/code>, which rely on Unix-style permission control primitives.&lt;/p>
&lt;p>So how do we know when to apply recursive permission changes and when we shouldn't? For in-tree storage drivers, this was relatively simple. For &lt;a href="https://kubernetes-csi.github.io/docs/introduction.html#introduction">CSI&lt;/a> drivers that could span a multitude of platforms and storage types, this problem can be a bigger challenge.&lt;/p>
&lt;p>Previously, whenever a CSI volume was mounted to a Pod, Kubernetes would attempt to automatically determine if the permissions and ownership should be modified. These methods were imprecise and could cause issues as we already mentioned, depending on the storage type.&lt;/p>
&lt;p>The CSIDriver custom resource now has a &lt;code>.spec.fsGroupPolicy&lt;/code> field, allowing storage drivers to explicitly opt in or out of these recursive modifications. By having the CSI driver specify a policy for the backing volumes, Kubernetes can avoid needless modification attempts. This optimization helps to reduce volume mount time and also cuts own reporting errors about modifications that would never succeed.&lt;/p>
&lt;h4 id="csidriver-fsgrouppolicy-api">CSIDriver FSGroupPolicy API&lt;/h4>
&lt;p>Three FSGroupPolicy values are available as of Kubernetes 1.20, with more planned for future releases.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ReadWriteOnceWithFSType&lt;/strong> - This is the default policy, applied if no &lt;code>fsGroupPolicy&lt;/code> is defined; this preserves the behavior from previous Kubernetes releases. Each volume is examined at mount time to determine if permissions should be recursively applied.&lt;/li>
&lt;li>&lt;strong>File&lt;/strong> - Always attempt to apply permission modifications, regardless of the filesystem type or PersistentVolumeClaim’s access mode.&lt;/li>
&lt;li>&lt;strong>None&lt;/strong> - Never apply permission modifications.&lt;/li>
&lt;/ul>
&lt;h4 id="how-do-i-use-it">How do I use it?&lt;/h4>
&lt;p>The only configuration needed is defining &lt;code>fsGroupPolicy&lt;/code> inside of the &lt;code>.spec&lt;/code> for a CSIDriver. Once that element is defined, any subsequently mounted volumes will automatically use the defined policy. There’s no additional deployment required!&lt;/p>
&lt;h4 id="what-s-next">What’s next?&lt;/h4>
&lt;p>Depending on feedback and adoption, the Kubernetes team plans to push these implementations to GA in either 1.21 or 1.22.&lt;/p>
&lt;h3 id="how-can-i-learn-more">How can I learn more?&lt;/h3>
&lt;p>This feature is explained in more detail in Kubernetes project documentation: &lt;a href="https://kubernetes-csi.github.io/docs/support-fsgroup.html">CSI Driver fsGroup Support&lt;/a> and &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods">Configure volume permission and ownership change policy for Pods &lt;/a>.&lt;/p>
&lt;h3 id="how-do-i-get-involved">How do I get involved?&lt;/h3>
&lt;p>The &lt;a href="https://kubernetes.slack.com/messages/csi">Kubernetes Slack channel #csi&lt;/a> and any of the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">standard SIG Storage communication channels&lt;/a> are great mediums to reach out to the SIG Storage and the CSI team.&lt;/p>
&lt;p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group (SIG)&lt;/a>. We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Kubernetes 1.20: Kubernetes Volume Snapshot Moves to GA</title><link>https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/</link><pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Xing Yang, VMware &amp;amp; Xiangqian Yu, Google&lt;/p>
&lt;p>The Kubernetes Volume Snapshot feature is now GA in Kubernetes v1.20. It was introduced as &lt;a href="https://kubernetes.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/">alpha&lt;/a> in Kubernetes v1.12, followed by a &lt;a href="https://kubernetes.io/blog/2019/01/17/update-on-volume-snapshot-alpha-for-kubernetes/">second alpha&lt;/a> with breaking changes in Kubernetes v1.13, and promotion to &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/">beta&lt;/a> in Kubernetes 1.17. This blog post summarizes the changes releasing the feature from beta to GA.&lt;/p>
&lt;h2 id="what-is-a-volume-snapshot">What is a volume snapshot?&lt;/h2>
&lt;p>Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to rehydrate a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).&lt;/p>
&lt;h2 id="why-add-volume-snapshots-to-kubernetes">Why add volume snapshots to Kubernetes?&lt;/h2>
&lt;p>Kubernetes aims to create an abstraction layer between distributed applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster-specific” knowledge.&lt;/p>
&lt;p>The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database’s volumes before starting a database operation.&lt;/p>
&lt;p>By providing a standard way to trigger volume snapshot operations in Kubernetes, this feature allows Kubernetes users to incorporate snapshot operations in a portable manner on any Kubernetes environment regardless of the underlying storage.&lt;/p>
&lt;p>Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced enterprise-grade storage administration features for Kubernetes, including application or cluster level backup solutions.&lt;/p>
&lt;h2 id="what-s-new-since-beta">What’s new since beta?&lt;/h2>
&lt;p>With the promotion of Volume Snapshot to GA, the feature is enabled by default on standard Kubernetes deployments and cannot be turned off.&lt;/p>
&lt;p>Many enhancements have been made to improve the quality of this feature and to make it production-grade.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The Volume Snapshot APIs and client library were moved to a separate Go module.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A snapshot validation webhook has been added to perform necessary validation on volume snapshot objects. More details can be found in the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1900-volume-snapshot-validation-webhook">Volume Snapshot Validation Webhook Kubernetes Enhancement Proposal&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Along with the validation webhook, the volume snapshot controller will start labeling invalid snapshot objects that already existed. This allows users to identify, remove any invalid objects, and correct their workflows. Once the API is switched to the v1 type, those invalid objects will not be deletable from the system.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>To provide better insights into how the snapshot feature is performing, an initial set of operation metrics has been added to the volume snapshot controller.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>There are more end-to-end tests, running on GCP, that validate the feature in a real Kubernetes cluster. Stress tests (based on Google Persistent Disk and &lt;code>hostPath&lt;/code> CSI Drivers) have been introduced to test the robustness of the system.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Other than introducing tightening validation, there is no difference between the v1beta1 and v1 Kubernetes volume snapshot API. In this release (with Kubernetes 1.20), both v1 and v1beta1 are served while the stored API version is still v1beta1. Future releases will switch the stored version to v1 and gradually remove v1beta1 support.&lt;/p>
&lt;h2 id="which-csi-drivers-support-volume-snapshots">Which CSI drivers support volume snapshots?&lt;/h2>
&lt;p>Snapshots are only supported for CSI drivers, not for in-tree or FlexVolume drivers. Ensure the deployed CSI driver on your cluster has implemented the snapshot interfaces. For more information, see &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface (CSI) for Kubernetes GA&lt;/a>.&lt;/p>
&lt;p>Currently more than &lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">50 CSI drivers&lt;/a> support the Volume Snapshot feature. The &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE Persistent Disk CSI Driver&lt;/a> has gone through the tests for upgrading from volume snapshots beta to GA. GA level support for other CSI drivers should be available soon.&lt;/p>
&lt;h2 id="who-builds-products-using-volume-snapshots">Who builds products using volume snapshots?&lt;/h2>
&lt;p>As of the publishing of this blog, the following participants from the &lt;a href="https://github.com/kubernetes/community/tree/master/wg-data-protection">Kubernetes Data Protection Working Group&lt;/a> are building products or have already built products using Kubernetes volume snapshots.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.delltechnologies.com/en-us/data-protection/powerprotect-data-manager.htm">Dell-EMC: PowerProtect&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.druva.com/">Druva&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kasten.io/">Kasten K10&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://cloud.netapp.com/project-astra">NetApp: Project Astra&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://portworx.com/products/px-backup/">Portworx (PX-Backup)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/purestorage/pso-csi">Pure Storage (Pure Service Orchestrator)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage">Red Hat OpenShift Container Storage&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://robin.io/storage/">Robin Cloud Native Storage&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.trilio.io/kubernetes/">TrilioVault for Kubernetes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/vmware-tanzu/velero-plugin-for-csi">Velero plugin for CSI&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-deploy-volume-snapshots">How to deploy volume snapshots?&lt;/h2>
&lt;p>Volume Snapshot feature contains the following components:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/client/config/crd">Kubernetes Volume Snapshot CRDs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/common-controller">Volume snapshot controller&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/validation-webhook">Snapshot validation webhook&lt;/a>&lt;/li>
&lt;li>CSI Driver along with &lt;a href="https://github.com/kubernetes-csi/external-snapshotter/tree/master/pkg/sidecar-controller">CSI Snapshotter sidecar&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>It is strongly recommended that Kubernetes distributors bundle and deploy the volume snapshot controller, CRDs, and validation webhook as part of their Kubernetes cluster management process (independent of any CSI Driver).&lt;/p>
&lt;div class="alert alert-danger warning callout" role="alert">
&lt;strong>Warning:&lt;/strong> The snapshot validation webhook serves as a critical component to transition smoothly from using v1beta1 to v1 API. Not installing the snapshot validation webhook makes prevention of invalid volume snapshot objects from creation/updating impossible, which in turn will block deletion of invalid volume snapshot objects in coming upgrades.
&lt;/div>
&lt;p>If your cluster does not come pre-installed with the correct components, you may manually install them. See the &lt;a href="https://github.com/kubernetes-csi/external-snapshotter#readme">CSI Snapshotter&lt;/a> README for details.&lt;/p>
&lt;h2 id="how-to-use-volume-snapshots">How to use volume snapshots?&lt;/h2>
&lt;p>Assuming all the required components (including CSI driver) have been already deployed and running on your cluster, you can create volume snapshots using the &lt;code>VolumeSnapshot&lt;/code> API object, or use an existing &lt;code>VolumeSnapshot&lt;/code> to restore a PVC by specifying the VolumeSnapshot data source on it. For more details, see the &lt;a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots/">volume snapshot documentation&lt;/a>.&lt;/p>
&lt;div class="alert alert-info note callout" role="alert">
&lt;strong>Note:&lt;/strong> The Kubernetes Snapshot API does not provide any application consistency guarantees. You have to prepare your application (pause application, freeze filesystem etc.) before taking the snapshot for data consistency either manually or using higher level APIs/controllers.
&lt;/div>
&lt;h3 id="dynamically-provision-a-volume-snapshot">Dynamically provision a volume snapshot&lt;/h3>
&lt;p>To dynamically provision a volume snapshot, create a &lt;code>VolumeSnapshotClass&lt;/code> API object first.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotClass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapclass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>testdriver.csi.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">deletionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi.storage.k8s.io/snapshotter-secret-name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mysecret&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi.storage.k8s.io/snapshotter-secret-namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mysecretnamespace&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then create a &lt;code>VolumeSnapshot&lt;/code> API object from a PVC by specifying the volume snapshot class.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapclass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">persistentVolumeClaimName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="importing-an-existing-volume-snapshot-with-kubernetes">Importing an existing volume snapshot with Kubernetes&lt;/h3>
&lt;p>To import a pre-existing volume snapshot into Kubernetes, manually create a &lt;code>VolumeSnapshotContent&lt;/code> object first.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotContent&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-content&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deletionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>testdriver.csi.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">snapshotHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>7bdd0de3-xxx&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then create a &lt;code>VolumeSnapshot&lt;/code> object pointing to the &lt;code>VolumeSnapshotContent&lt;/code> object.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotContentName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-content&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="rehydrate-volume-from-snapshot">Rehydrate volume from snapshot&lt;/h3>
&lt;p>A bound and ready &lt;code>VolumeSnapshot&lt;/code> object can be used to rehydrate a new volume with data pre-populated from snapshotted data as shown here:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pvc-restore&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>demo-namespace&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-storageclass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">dataSource&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-snapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshot&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="how-to-add-support-for-snapshots-in-a-csi-driver">How to add support for snapshots in a CSI driver?&lt;/h2>
&lt;p>See the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI spec&lt;/a> and the &lt;a href="https://kubernetes-csi.github.io/docs/snapshot-restore-feature.html">Kubernetes-CSI Driver Developer Guide&lt;/a> for more details on how to implement the snapshot feature in a CSI driver.&lt;/p>
&lt;h2 id="what-are-the-limitations">What are the limitations?&lt;/h2>
&lt;p>The GA implementation of volume snapshots for Kubernetes has the following limitations:&lt;/p>
&lt;ul>
&lt;li>Does not support reverting an existing PVC to an earlier state represented by a snapshot (only supports provisioning a new volume from a snapshot).&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-learn-more">How to learn more?&lt;/h3>
&lt;p>The code repository for snapshot APIs and controller is here: &lt;a href="https://github.com/kubernetes-csi/external-snapshotter">https://github.com/kubernetes-csi/external-snapshotter&lt;/a>&lt;/p>
&lt;p>Check out additional documentation on the snapshot feature here: &lt;a href="http://k8s.io/docs/concepts/storage/volume-snapshots">http://k8s.io/docs/concepts/storage/volume-snapshots&lt;/a> and &lt;a href="https://kubernetes-csi.github.io/docs/">https://kubernetes-csi.github.io/docs/&lt;/a>&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved?&lt;/h2>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p>
&lt;p>We offer a huge thank you to the contributors who stepped up these last few quarters to help the project reach GA. We want to thank Saad Ali, Michelle Au, Tim Hockin, and Jordan Liggitt for their insightful reviews and thorough consideration with the design, thank Andi Li for his work on adding the support of the snapshot validation webhook, thank Grant Griffiths on implementing metrics support in the snapshot controller and handling password rotation in the validation webhook, thank Chris Henzie, Raunak Shah, and Manohar Reddy for writing critical e2e tests to meet the scalability and stability requirements for graduation, thank Kartik Sharma for moving snapshot APIs and client lib to a separate go module, and thank Raunak Shah and Prafull Ladha for their help with upgrade testing from beta to GA.&lt;/p>
&lt;p>There are many more people who have helped to move the snapshot feature from beta to GA. We want to thank everyone who has contributed to this effort:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/AndiLi99">Andi Li&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/bswartz">Ben Swartzlander&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/chrishenzie">Chris Henzie&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/huffmanca">Christian Huffman&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ggriffiths">Grant Griffiths&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/humblec">Humble Devassy Chirammal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/jsafrane">Jan Šafránek&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Jiawei0227">Jiawei Wang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/jingxu97">Jing Xu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/liggitt">Jordan Liggitt&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Kartik494">Kartik Sharma&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Madhu-1">Madhu Rajanna&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/boddumanohar">Manohar Reddy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/msau42">Michelle Au&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/pohly">Patrick Ohly&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prafull01">Prafull Ladha&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prateekpandey14">Prateek Pandey&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/RaunakShah">Raunak Shah&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/saad-ali">Saad Ali&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/saikat-royc">Saikat Roychowdhury&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/thockin">Tim Hockin&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/yuxiangqian">Xiangqian Yu&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/xing-yang">Xing Yang&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/zhucan">Zhu Can&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>For those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group&lt;/a> (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p>
&lt;p>We also hold regular &lt;a href="https://docs.google.com/document/d/15tLCV3csvjHbKb16DVk-mfUmFry_Rlwo-2uG6KNGsfw/edit#">Data Protection Working Group meetings&lt;/a>. New attendees are welcome to join in discussions.&lt;/p></description></item><item><title>Blog: Kubernetes 1.20: The Raddest Release</title><link>https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.20/release_team.md">Kubernetes 1.20 Release Team&lt;/a>&lt;/p>
&lt;p>We’re pleased to announce the release of Kubernetes 1.20, our third and final release of 2020! This release consists of 42 enhancements: 11 enhancements have graduated to stable, 15 enhancements are moving to beta, and 16 enhancements are entering alpha.&lt;/p>
&lt;p>The 1.20 release cycle returned to its normal cadence of 11 weeks following the previous extended release cycle. This is one of the most feature dense releases in a while: the Kubernetes innovation cycle is still trending upward. This release has more alpha than stable enhancements, showing that there is still much to explore in the cloud native ecosystem.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="volume-snapshot-operations-goes-stable">Volume Snapshot Operations Goes Stable&lt;/h3>
&lt;p>This feature provides a standard way to trigger volume snapshot operations and allows users to incorporate snapshot operations in a portable manner on any Kubernetes environment and supported storage providers.&lt;/p>
&lt;p>Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise-grade, storage administration features for Kubernetes, including application or cluster level backup solutions.&lt;/p>
&lt;p>Note that snapshot support requires Kubernetes distributors to bundle the Snapshot controller, Snapshot CRDs, and validation webhook. A CSI driver supporting the snapshot functionality must also be deployed on the cluster.&lt;/p>
&lt;h3 id="kubectl-debug-graduates-to-beta">Kubectl Debug Graduates to Beta&lt;/h3>
&lt;p>The &lt;code>kubectl alpha debug&lt;/code> features graduates to beta in 1.20, becoming &lt;code>kubectl debug&lt;/code>. The feature provides support for common debugging workflows directly from kubectl. Troubleshooting scenarios supported in this release of kubectl include:&lt;/p>
&lt;ul>
&lt;li>Troubleshoot workloads that crash on startup by creating a copy of the pod that uses a different container image or command.&lt;/li>
&lt;li>Troubleshoot distroless containers by adding a new container with debugging tools, either in a new copy of the pod or using an ephemeral container. (Ephemeral containers are an alpha feature that are not enabled by default.)&lt;/li>
&lt;li>Troubleshoot on a node by creating a container running in the host namespaces and with access to the host’s filesystem.&lt;/li>
&lt;/ul>
&lt;p>Note that as a new built-in command, &lt;code>kubectl debug&lt;/code> takes priority over any kubectl plugin named “debug”. You must rename the affected plugin.&lt;/p>
&lt;p>Invocations using &lt;code>kubectl alpha debug&lt;/code> are now deprecated and will be removed in a subsequent release. Update your scripts to use &lt;code>kubectl debug&lt;/code>. For more information about &lt;code>kubectl debug&lt;/code>, see &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/">Debugging Running Pods&lt;/a>.&lt;/p>
&lt;h3 id="beta-api-priority-and-fairness">Beta: API Priority and Fairness&lt;/h3>
&lt;p>Introduced in 1.18, Kubernetes 1.20 now enables API Priority and Fairness (APF) by default. This allows &lt;code>kube-apiserver&lt;/code> to categorize incoming requests by priority levels.&lt;/p>
&lt;h3 id="alpha-with-updates-ipv4-ipv6">Alpha with updates: IPV4/IPV6&lt;/h3>
&lt;p>The IPv4/IPv6 dual stack has been reimplemented to support dual stack services based on user and community feedback. This allows both IPv4 and IPv6 service cluster IP addresses to be assigned to a single service, and also enables a service to be transitioned from single to dual IP stack and vice versa.&lt;/p>
&lt;h3 id="ga-process-pid-limiting-for-stability">GA: Process PID Limiting for Stability&lt;/h3>
&lt;p>Process IDs (pids) are a fundamental resource on Linux hosts. It is trivial to hit the task limit without hitting any other resource limits and cause instability to a host machine.&lt;/p>
&lt;p>Administrators require mechanisms to ensure that user pods cannot induce pid exhaustion that prevents host daemons (runtime, kubelet, etc) from running. In addition, it is important to ensure that pids are limited among pods in order to ensure they have limited impact to other workloads on the node.
After being enabled-by-default for a year, SIG Node graduates PID Limits to GA on both &lt;code>SupportNodePidsLimit&lt;/code> (node-to-pod PID isolation) and &lt;code>SupportPodPidsLimit&lt;/code> (ability to limit PIDs per pod).&lt;/p>
&lt;h3 id="alpha-graceful-node-shutdown">Alpha: Graceful node shutdown&lt;/h3>
&lt;p>Users and cluster administrators expect that pods will adhere to expected pod lifecycle including pod termination. Currently, when a node shuts down, pods do not follow the expected pod termination lifecycle and are not terminated gracefully which can cause issues for some workloads.
The &lt;code>GracefulNodeShutdown&lt;/code> feature is now in Alpha. &lt;code>GracefulNodeShutdown&lt;/code> makes the kubelet aware of node system shutdowns, enabling graceful termination of pods during a system shutdown.&lt;/p>
&lt;h2 id="major-changes">Major Changes&lt;/h2>
&lt;h3 id="dockershim-deprecation">Dockershim Deprecation&lt;/h3>
&lt;p>Dockershim, the container runtime interface (CRI) shim for Docker is being deprecated. Support for Docker is deprecated and will be removed in a future release. Docker-produced images will continue to work in your cluster with all CRI compliant runtimes as Docker images follow the Open Container Initiative (OCI) image specification.
The Kubernetes community has written a &lt;a href="https://blog.k8s.io/2020/12/02/dont-panic-kubernetes-and-docker/">detailed blog post about deprecation&lt;/a> with &lt;a href="https://blog.k8s.io/2020/12/02/dockershim-faq/">a dedicated FAQ page for it&lt;/a>.&lt;/p>
&lt;h3 id="exec-probe-timeout-handling">Exec Probe Timeout Handling&lt;/h3>
&lt;p>A longstanding bug regarding exec probe timeouts that may impact existing pod definitions has been fixed. Prior to this fix, the field &lt;code>timeoutSeconds&lt;/code> was not respected for exec probes. Instead, probes would run indefinitely, even past their configured deadline, until a result was returned. With this change, the default value of &lt;code>1 second&lt;/code> will be applied if a value is not specified and existing pod definitions may no longer be sufficient if a probe takes longer than one second. A feature gate, called &lt;code>ExecProbeTimeout&lt;/code>, has been added with this fix that enables cluster operators to revert to the previous behavior, but this will be locked and removed in subsequent releases. In order to revert to the previous behavior, cluster operators should set this feature gate to &lt;code>false&lt;/code>.&lt;/p>
&lt;p>Please review the updated documentation regarding &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes">configuring probes&lt;/a> for more details.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/585">RuntimeClass&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1929">Built-in API Types Defaults&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/950">Add Pod-Startup Liveness-Probe Holdoff&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1001">Support CRI-ContainerD On Windows&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/614">SCTP Support for Services&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">Adding AppProtocol To Services And Endpoints&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="notable-feature-updates">Notable Feature Updates&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/19">CronJobs&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="release-notes">Release notes&lt;/h1>
&lt;p>You can check out the full details of the 1.20 release in the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md">release notes&lt;/a>.&lt;/p>
&lt;h1 id="availability-of-release">Availability of release&lt;/h1>
&lt;p>Kubernetes 1.20 is available for &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.20.0">download on GitHub&lt;/a>. There are some great resources out there for getting started with Kubernetes. You can check out some &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> on the main Kubernetes site, or run a local cluster on your machine using Docker containers with &lt;a href="https://kind.sigs.k8s.io">kind&lt;/a>. If you’d like to try building a cluster from scratch, check out the &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes the Hard Way&lt;/a> tutorial by Kelsey Hightower.&lt;/p>
&lt;h1 id="release-team">Release Team&lt;/h1>
&lt;p>This release was made possible by a very dedicated group of individuals, who came together as a team in the midst of a lot of things happening out in the world. A huge thank you to the release lead Jeremy Rickard, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.20 release for the community.&lt;/p>
&lt;h1 id="release-logo">Release Logo&lt;/h1>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-12-08-kubernetes-1.20-release-announcement/laser.png" alt="Kubernetes 1.20 Release Logo">&lt;/p>
&lt;p>&lt;a href="https://www.dictionary.com/browse/rad">raddest&lt;/a>: &lt;em>adjective&lt;/em>, Slang. excellent; wonderful; cool:&lt;/p>
&lt;blockquote>
&lt;p>The Kubernetes 1.20 Release has been the raddest release yet.&lt;/p>
&lt;/blockquote>
&lt;p>2020 has been a challenging year for many of us, but Kubernetes contributors have delivered a record-breaking number of enhancements in this release. That is a great accomplishment, so the release lead wanted to end the year with a little bit of levity and pay homage to &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.14">Kubernetes 1.14 - Caturnetes&lt;/a> with a &amp;quot;rad&amp;quot; cat named Humphrey.&lt;/p>
&lt;p>Humphrey is the release lead's cat and has a permanent &lt;a href="https://www.inverse.com/article/42316-why-do-cats-blep-science-explains">&lt;code>blep&lt;/code>&lt;/a>. &lt;em>Rad&lt;/em> was pretty common slang in the 1990s in the United States, and so were laser backgrounds. Humphrey in a 1990s style school picture felt like a fun way to end the year. Hopefully, Humphrey and his &lt;em>blep&lt;/em> bring you a little joy at the end of 2020!&lt;/p>
&lt;p>The release logo was created by &lt;a href="https://www.instagram.com/robotdancebattle/">Henry Hsu - @robotdancebattle&lt;/a>.&lt;/p>
&lt;h1 id="user-highlights">User Highlights&lt;/h1>
&lt;ul>
&lt;li>Apple is operating multi-thousand node Kubernetes clusters in data centers all over the world. Watch &lt;a href="https://youtu.be/Tx8qXC-U3KM">Alena Prokharchyk's KubeCon NA Keynote&lt;/a> to learn more about their cloud native journey.&lt;/li>
&lt;/ul>
&lt;h1 id="project-velocity">Project Velocity&lt;/h1>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/">CNCF K8s DevStats project&lt;/a> aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is a neat illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p>
&lt;p>In the v1.20 release cycle, which ran for 11 weeks (September 25 to December 9), we saw contributions from &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions">967 companies&lt;/a> and &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All">1335 individuals&lt;/a> (&lt;a href="https://k8s.devstats.cncf.io/d/52/new-contributors?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-repogroup_name=Kubernetes">44 of whom&lt;/a> made their first Kubernetes contribution) from &lt;a href="https://k8s.devstats.cncf.io/d/50/countries-stats?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-period_name=Quarter&amp;amp;var-countries=All&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-metric=rcommitters&amp;amp;var-cum=countries">26 countries&lt;/a>.&lt;/p>
&lt;h1 id="ecosystem-updates">Ecosystem Updates&lt;/h1>
&lt;ul>
&lt;li>KubeCon North America just wrapped up three weeks ago, the second such event to be virtual! All talks are &lt;a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut">now available to all on-demand&lt;/a> for anyone still needing to catch up!&lt;/li>
&lt;li>In June, the Kubernetes community formed a new working group as a direct response to the Black Lives Matter protests occurring across America. WG Naming's goal is to remove harmful and unclear language in the Kubernetes project as completely as possible and to do so in a way that is portable to other CNCF projects. A great introductory talk on this important work and how it is conducted was given &lt;a href="https://sched.co/eukp">at KubeCon 2020 North America&lt;/a>, and the initial impact of this labor &lt;a href="https://github.com/kubernetes/enhancements/issues/2067">can actually be seen in the v1.20 release&lt;/a>.&lt;/li>
&lt;li>Previously announced this summer, &lt;a href="https://www.cncf.io/announcements/2020/11/17/kubernetes-security-specialist-certification-now-available/">The Certified Kubernetes Security Specialist (CKS) Certification&lt;/a> was released during Kubecon NA for immediate scheduling! Following the model of CKA and CKAD, the CKS is a performance-based exam, focused on security-themed competencies and domains. This exam is targeted at current CKA holders, particularly those who want to round out their baseline knowledge in securing cloud workloads (which is all of us, right?).&lt;/li>
&lt;/ul>
&lt;h1 id="event-updates">Event Updates&lt;/h1>
&lt;p>KubeCon + CloudNativeCon Europe 2021 will take place May 4 - 7, 2021! Registration will open on January 11. You can find more information about the conference &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">here&lt;/a>. Remember that &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/cfp/">the CFP&lt;/a> closes on Sunday, December 13, 11:59pm PST!&lt;/p>
&lt;h1 id="upcoming-release-webinar">Upcoming release webinar&lt;/h1>
&lt;p>Stay tuned for the upcoming release webinar happening this January.&lt;/p>
&lt;h1 id="get-involved">Get Involved&lt;/h1>
&lt;p>If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels:&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the new &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributor website&lt;/a>&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>