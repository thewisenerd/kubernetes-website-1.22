<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes – 生产级别的容器编排系统</title><link>https://kubernetes.io/zh/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/zh/</link></image><atom:link href="https://kubernetes.io/zh/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Kubernetes 1.20：CSI 驱动程序中的 Pod 身份假扮和短时卷</title><link>https://kubernetes.io/zh/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.20: Pod Impersonation and Short-lived Volumes in CSI Drivers'
date: 2020-12-18
slug: kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi
-->
&lt;!--
**Author**: Shihang Zhang (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Shihang Zhang（谷歌）&lt;/p>
&lt;!--
Typically when a [CSI](https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md) driver mounts credentials such as secrets and certificates, it has to authenticate against storage providers to access the credentials. However, the access to those credentials are controlled on the basis of the pods' identities rather than the CSI driver's identity. CSI drivers, therefore, need some way to retrieve pod's service account token.
-->
&lt;p>通常，当 &lt;a href="https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md">CSI&lt;/a> 驱动程序挂载
诸如 Secret 和证书之类的凭据时，它必须通过存储提供者的身份认证才能访问这些凭据。
然而，对这些凭据的访问是根据 Pod 的身份而不是 CSI 驱动程序的身份来控制的。
因此，CSI 驱动程序需要某种方法来取得 Pod 的服务帐户令牌。&lt;/p>
&lt;!--
Currently there are two suboptimal approaches to achieve this, either by granting CSI drivers the permission to use TokenRequest API or by reading tokens directly from the host filesystem.
-->
&lt;p>当前，有两种不是那么理想的方法来实现这一目的，要么通过授予 CSI 驱动程序使用 TokenRequest API 的权限，要么直接从主机文件系统中读取令牌。&lt;/p>
&lt;!--
Both of them exhibit the following drawbacks:
-->
&lt;p>两者都存在以下缺点：&lt;/p>
&lt;!--
- Violating the principle of least privilege
- Every CSI driver needs to re-implement the logic of getting the pod’s service account token
-->
&lt;ul>
&lt;li>违反最少特权原则&lt;/li>
&lt;li>每个 CSI 驱动程序都需要重新实现获取 Pod 的服务帐户令牌的逻辑&lt;/li>
&lt;/ul>
&lt;!--
The second approach is more problematic due to:
-->
&lt;p>第二种方式问题更多，因为：&lt;/p>
&lt;!--
- The audience of the token defaults to the kube-apiserver
- The token is not guaranteed to be available (e.g. `AutomountServiceAccountToken=false`)
- The approach does not work for CSI drivers that run as a different (non-root) user from the pods. See [file permission section for service account token](https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission)
- The token might be legacy Kubernetes service account token which doesn’t expire if `BoundServiceAccountTokenVolume=false`
-->
&lt;ul>
&lt;li>令牌的受众默认为 kube-apiserver&lt;/li>
&lt;li>该令牌不能保证可用（例如，&lt;code>AutomountServiceAccountToken=false&lt;/code>）&lt;/li>
&lt;li>该方法不适用于以与 Pod 不同的（非 root 用户）用户身份运行的 CSI 驱动程序。请参见
&lt;a href="https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission">服务帐户令牌的文件许可权部分&lt;/a>&lt;/li>
&lt;li>该令牌可能是旧的 Kubernetes 服务帐户令牌，如果 &lt;code>BoundServiceAccountTokenVolume=false&lt;/code>，该令牌不会过期。&lt;/li>
&lt;/ul>
&lt;!--
Kubernetes 1.20 introduces an alpha feature, `CSIServiceAccountToken`, to improve the security posture. The new feature allows CSI drivers to receive pods' [bound service account tokens](https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md).
-->
&lt;p>Kubernetes 1.20 引入了一个内测功能 &lt;code>CSIServiceAccountToken&lt;/code> 以改善安全状况。这项新功能允许 CSI 驱动程序接收 Pod 的&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md">绑定服务帐户令牌&lt;/a>。&lt;/p>
&lt;!--
This feature also provides a knob to re-publish volumes so that short-lived volumes can be refreshed.
-->
&lt;p>此功能还提供了一个重新发布卷的能力，以便可以刷新短时卷。&lt;/p>
&lt;!--
## Pod Impersonation
### Using GCP APIs
-->
&lt;h2 id="pod-身份假扮">Pod 身份假扮&lt;/h2>
&lt;h3 id="使用-gcp-apis">使用 GCP APIs&lt;/h3>
&lt;!--
Using [Workload Identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity), a Kubernetes service account can authenticate as a Google service account when accessing Google Cloud APIs. If a CSI driver needs to access GCP APIs on behalf of the pods that it is mounting volumes for, it can use the pod's service account token to [exchange for GCP tokens](https://cloud.google.com/iam/docs/reference/sts/rest). The pod's service account token is plumbed through the volume context in `NodePublishVolume` RPC calls when the feature `CSIServiceAccountToken` is enabled. For example: accessing [Google Secret Manager](https://cloud.google.com/secret-manager/) via a [secret store CSI driver](https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp).
-->
&lt;p>使用 &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity&lt;/a>，Kubernetes 服务帐户可以在访问 Google Cloud API 时验证为 Google 服务帐户。
如果 CSI 驱动程序要代表其为挂载卷的 Pod 访问 GCP API，则可以使用 Pod 的服务帐户令牌来
&lt;a href="https://cloud.google.com/iam/docs/reference/sts/rest">交换 GCP 令牌&lt;/a>。启用功能 &lt;code>CSIServiceAccountToken&lt;/code> 后，
可通过 &lt;code>NodePublishVolume&lt;/code> RPC 调用中的卷上下文来访问 Pod 的服务帐户令牌。例如：通过 &lt;a href="https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp">Secret 存储 CSI 驱动&lt;/a>
访问 &lt;a href="https://cloud.google.com/secret-manager/">Google Secret Manager&lt;/a>。&lt;/p>
&lt;!--
### Using Vault
If users configure [Kubernetes as an auth method](https://www.vaultproject.io/docs/auth/kubernetes), Vault uses the `TokenReview` API to validate the Kubernetes service account token. For CSI drivers using Vault as resources provider, they need to present the pod's service account to Vault. For example, [secrets store CSI driver](https://github.com/hashicorp/secrets-store-csi-driver-provider-vault) and [cert manager CSI driver](https://github.com/jetstack/cert-manager-csi).
-->
&lt;h3 id="使用vault">使用Vault&lt;/h3>
&lt;p>如果用户将 &lt;a href="https://www.vaultproject.io/docs/auth/kubernetes">Kubernetes 作为身份验证方法&lt;/a>配置，
则 Vault 使用 &lt;code>TokenReview&lt;/code> API 来验证 Kubernetes 服务帐户令牌。
对于使用 Vault 作为资源提供者的 CSI 驱动程序，它们需要将 Pod 的服务帐户提供给 Vault。
例如，&lt;a href="https://github.com/hashicorp/secrets-store-csi-driver-provider-vault">Secret 存储 CSI 驱动&lt;/a>和
&lt;a href="https://github.com/jetstack/cert-manager-csi">证书管理器 CSI 驱动&lt;/a>。&lt;/p>
&lt;!--
## Short-lived Volumes
To keep short-lived volumes such as certificates effective, CSI drivers can specify `RequiresRepublish=true` in their`CSIDriver` object to have the kubelet periodically call `NodePublishVolume` on mounted volumes. These republishes allow CSI drivers to ensure that the volume content is up-to-date.
-->
&lt;h2 id="短时卷">短时卷&lt;/h2>
&lt;p>为了使诸如证书之类的短时卷保持有效，CSI 驱动程序可以在其 &lt;code>CSIDriver&lt;/code> 对象中指定 &lt;code>RequiresRepublish=true&lt;/code>，
以使 kubelet 定期针对已挂载的卷调用 &lt;code>NodePublishVolume&lt;/code>。
这些重新发布操作使 CSI 驱动程序可以确保卷内容是最新的。&lt;/p>
&lt;!--
## Next steps
This feature is alpha and projected to move to beta in 1.21. See more in the following KEP and CSI documentation:
-->
&lt;h2 id="下一步">下一步&lt;/h2>
&lt;p>此功能是 Alpha 版，预计将在 1.21 版中移至 Beta 版。 请参阅以下 KEP 和 CSI 文档中的更多内容：&lt;/p>
&lt;!--
- [KEP-1855: Service Account Token for CSI Driver](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md)
- [Token Requests](https://kubernetes-csi.github.io/docs/token-requests.html)
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md">KEP-1855: CSI 驱动程序的服务帐户令牌&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes-csi.github.io/docs/token-requests.html">令牌请求&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Your feedback is always welcome!
- SIG-Auth [meets regularly](https://github.com/kubernetes/community/tree/master/sig-auth#meetings) and can be reached via [Slack and the mailing list](https://github.com/kubernetes/community/tree/master/sig-auth#contact)
- SIG-Storage [meets regularly](https://github.com/kubernetes/community/tree/master/sig-storage#meetings) and can be reached via [Slack and the mailing list](https://github.com/kubernetes/community/tree/master/sig-storage#contact).
-->
&lt;p>随时欢迎您提供反馈!&lt;/p>
&lt;ul>
&lt;li>SIG-Auth &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#meetings">定期开会&lt;/a>，可以通过 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#contact">Slack 和邮件列表&lt;/a>加入&lt;/li>
&lt;li>SIG-Storage &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">定期开会&lt;/a>，可以通过 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#contact">Slack 和邮件列表&lt;/a>加入&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.20: 最新版本</title><link>https://kubernetes.io/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/</link><pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/12/08/kubernetes-1-20-release-announcement/</guid><description>
&lt;!-- ---
layout: blog
title: 'Kubernetes 1.20: The Raddest Release'
date: 2020-12-08
slug: kubernetes-1-20-release-announcement
--- -->
&lt;p>&lt;strong>作者:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.20/release_team.md">Kubernetes 1.20 发布团队&lt;/a>&lt;/p>
&lt;!-- **Authors:** [Kubernetes 1.20 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.20/release_team.md) -->
&lt;p>我们很高兴地宣布 Kubernetes 1.20 的发布，这是我们 2020 年的第三个也是最后一个版本！此版本包含 42 项增强功能：11 项增强功能已升级到稳定版，15 项增强功能正在进入测试版，16 项增强功能正在进入 Alpha 版。&lt;/p>
&lt;!-- We’re pleased to announce the release of Kubernetes 1.20, our third and final release of 2020! This release consists of 42 enhancements: 11 enhancements have graduated to stable, 15 enhancements are moving to beta, and 16 enhancements are entering alpha. -->
&lt;p>1.20 发布周期在上一个延长的发布周期之后恢复到 11 周的正常节奏。这是一段时间以来功能最密集的版本之一：Kubernetes 创新周期仍呈上升趋势。此版本具有更多的 Alpha 而非稳定的增强功能，表明云原生生态系统仍有许多需要探索的地方。&lt;/p>
&lt;!-- The 1.20 release cycle returned to its normal cadence of 11 weeks following the previous extended release cycle. This is one of the most feature dense releases in a while: the Kubernetes innovation cycle is still trending upward. This release has more alpha than stable enhancements, showing that there is still much to explore in the cloud native ecosystem. -->
&lt;h2 id="major-themes">主题&lt;/h2>
&lt;!-- ## Major Themes -->
&lt;h3 id="volume-snapshot-operations-goes-stable">Volume 快照操作变得稳定&lt;/h3>
&lt;!-- This feature provides a standard way to trigger volume snapshot operations and allows users to incorporate snapshot operations in a portable manner on any Kubernetes environment and supported storage providers. -->
&lt;p>此功能提供了触发卷快照操作的标准方法，并允许用户以可移植的方式在任何 Kubernetes 环境和支持的存储提供程序上合并快照操作。&lt;/p>
&lt;!-- Additionally, these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise-grade, storage administration features for Kubernetes, including application or cluster level backup solutions. -->
&lt;p>此外，这些 Kubernetes 快照原语充当基本构建块，解锁为 Kubernetes 开发高级企业级存储管理功能的能力，包括应用程序或集群级备份解决方案。&lt;/p>
&lt;!-- Note that snapshot support requires Kubernetes distributors to bundle the Snapshot controller, Snapshot CRDs, and validation webhook. A CSI driver supporting the snapshot functionality must also be deployed on the cluster. -->
&lt;p>请注意，快照支持要求 Kubernetes 分销商捆绑 Snapshot 控制器、Snapshot CRD 和验证 webhook。还必须在集群上部署支持快照功能的 CSI 驱动程序。&lt;/p>
&lt;!-- ### Kubectl Debug Graduates to Beta -->
&lt;h3 id="kubectl-debug-graduates-to-beta">Kubectl Debug 功能升级到 Beta&lt;/h3>
&lt;!-- The `kubectl alpha debug` features graduates to beta in 1.20, becoming `kubectl debug`. The feature provides support for common debugging workflows directly from kubectl. Troubleshooting scenarios supported in this release of kubectl include: -->
&lt;p>&lt;code>kubectl alpha debug&lt;/code> 功能在 1.20 中升级到测试版，成为 &lt;code>kubectl debug&lt;/code>. 该功能直接从 kubectl 提供对常见调试工作流的支持。此版本的 kubectl 支持的故障排除场景包括：&lt;/p>
&lt;!-- * Troubleshoot workloads that crash on startup by creating a copy of the pod that uses a different container image or command.
* Troubleshoot distroless containers by adding a new container with debugging tools, either in a new copy of the pod or using an ephemeral container. (Ephemeral containers are an alpha feature that are not enabled by default.)
* Troubleshoot on a node by creating a container running in the host namespaces and with access to the host’s filesystem. -->
&lt;ul>
&lt;li>通过创建使用不同容器映像或命令的 pod 副本，对在启动时崩溃的工作负载进行故障排除。&lt;/li>
&lt;li>通过在 pod 的新副本或使用临时容器中添加带有调试工具的新容器来对 distroless 容器进行故障排除。（临时容器是默认未启用的 alpha 功能。）&lt;/li>
&lt;li>通过创建在主机命名空间中运行并可以访问主机文件系统的容器来对节点进行故障排除。&lt;/li>
&lt;/ul>
&lt;!-- Note that as a new built-in command, `kubectl debug` takes priority over any kubectl plugin named “debug”. You must rename the affected plugin. -->
&lt;p>请注意，作为新的内置命令，&lt;code>kubectl debug&lt;/code> 优先于任何名为 “debug” 的 kubectl 插件。你必须重命名受影响的插件。&lt;/p>
&lt;!-- Invocations using `kubectl alpha debug` are now deprecated and will be removed in a subsequent release. Update your scripts to use `kubectl debug`. For more information about `kubectl debug`, see [Debugging Running Pods](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/). -->
&lt;p>&lt;code>kubectl alpha debug&lt;/code> 现在不推荐使用，并将在后续版本中删除。更新你的脚本以使用 &lt;code>kubectl debug&lt;/code>。 有关更多信息 &lt;code>kubectl debug&lt;/code>，请参阅[调试正在运行的 Pod]((&lt;a href="https://kubernetes.io/zh/docs/tasks/debug-application-cluster/debug-running-pod/">https://kubernetes.io/zh/docs/tasks/debug-application-cluster/debug-running-pod/&lt;/a>)。&lt;/p>
&lt;!-- ### Beta: API Priority and Fairness -->
&lt;h3 id="测试版-api-优先级和公平性-beta-api-priority-and-fairness">测试版：API 优先级和公平性 {#beta-api-priority-and-fairness)&lt;/h3>
&lt;!-- Introduced in 1.18, Kubernetes 1.20 now enables API Priority and Fairness (APF) by default. This allows `kube-apiserver` to categorize incoming requests by priority levels. -->
&lt;p>Kubernetes 1.20 由 1.18 引入，现在默认启用 API 优先级和公平性 (APF)。这允许 &lt;code>kube-apiserver&lt;/code> 按优先级对传入请求进行分类。&lt;/p>
&lt;!-- ### Alpha with updates: IPV4/IPV6 -->
&lt;h3 id="alpha-with-updates-ipv4-ipv6">Alpha 更新：IPV4/IPV6&lt;/h3>
&lt;!-- The IPv4/IPv6 dual stack has been reimplemented to support dual stack services based on user and community feedback. This allows both IPv4 and IPv6 service cluster IP addresses to be assigned to a single service, and also enables a service to be transitioned from single to dual IP stack and vice versa. -->
&lt;p>基于用户和社区反馈，重新实现了 IPv4/IPv6 双栈以支持双栈服务。
这允许将 IPv4 和 IPv6 服务集群 IP 地址分配给单个服务，还允许服务从单 IP 堆栈转换为双 IP 堆栈，反之亦然。&lt;/p>
&lt;!-- ### GA: Process PID Limiting for Stability -->
&lt;h3 id="ga-process-pid-limiting-for-stability">GA：进程 PID 稳定性限制&lt;/h3>
&lt;!-- Process IDs (pids) are a fundamental resource on Linux hosts. It is trivial to hit the task limit without hitting any other resource limits and cause instability to a host machine. -->
&lt;p>进程 ID (pid) 是 Linux 主机上的基本资源。达到任务限制而不达到任何其他资源限制并导致主机不稳定是很可能发生的。&lt;/p>
&lt;!-- Administrators require mechanisms to ensure that user pods cannot induce pid exhaustion that prevents host daemons (runtime, kubelet, etc) from running. In addition, it is important to ensure that pids are limited among pods in order to ensure they have limited impact to other workloads on the node. -->
&lt;!-- After being enabled-by-default for a year, SIG Node graduates PID Limits to GA on both `SupportNodePidsLimit` (node-to-pod PID isolation) and `SupportPodPidsLimit` (ability to limit PIDs per pod). -->
&lt;p>管理员需要机制来确保用户 pod 不会导致 pid 耗尽，从而阻止主机守护程序（运行时、kubelet 等）运行。此外，重要的是要确保 pod 之间的 pid 受到限制，以确保它们对节点上的其他工作负载的影响有限。
默认启用一年后，SIG Node 在 &lt;code>SupportNodePidsLimit&lt;/code>（节点到 Pod PID 隔离）和 &lt;code>SupportPodPidsLimit&lt;/code>（限制每个 Pod 的 PID 的能力）上都将 PID 限制升级为 GA。&lt;/p>
&lt;!-- ### Alpha: Graceful node shutdown -->
&lt;h3 id="alpha-graceful-node-shutdown">Alpha：节点体面地关闭&lt;/h3>
&lt;!-- Users and cluster administrators expect that pods will adhere to expected pod lifecycle including pod termination. Currently, when a node shuts down, pods do not follow the expected pod termination lifecycle and are not terminated gracefully which can cause issues for some workloads.
The `GracefulNodeShutdown` feature is now in Alpha. `GracefulNodeShutdown` makes the kubelet aware of node system shutdowns, enabling graceful termination of pods during a system shutdown. -->
&lt;p>用户和集群管理员希望 Pod 遵守预期的 Pod 生命周期，包括 Pod 终止。目前，当一个节点关闭时，Pod 不会遵循预期的 Pod 终止生命周期，也不会正常终止，这可能会导致某些工作负载出现问题。
该 &lt;code>GracefulNodeShutdown&lt;/code> 功能现在处于 Alpha 阶段。&lt;code>GracefulNodeShutdown&lt;/code> 使 kubelet 知道节点系统关闭，从而在系统关闭期间正常终止 pod。&lt;/p>
&lt;!-- ## Major Changes -->
&lt;h2 id="major-changes">主要变化&lt;/h2>
&lt;!-- ### Dockershim Deprecation -->
&lt;h3 id="dockershim-deprecation">Dockershim 弃用&lt;/h3>
&lt;!-- Dockershim, the container runtime interface (CRI) shim for Docker is being deprecated. Support for Docker is deprecated and will be removed in a future release. Docker-produced images will continue to work in your cluster with all CRI compliant runtimes as Docker images follow the Open Container Initiative (OCI) image specification.
The Kubernetes community has written a [detailed blog post about deprecation](https://blog.k8s.io/2020/12/02/dont-panic-kubernetes-and-docker/) with [a dedicated FAQ page for it](https://blog.k8s.io/2020/12/02/dockershim-faq/). -->
&lt;p>Dockershim，Docker 的容器运行时接口 (CRI) shim 已被弃用。不推荐使用对 Docker 的支持，并将在未来版本中删除。由于 Docker 映像遵循开放容器计划 (OCI) 映像规范，因此 Docker 生成的映像将继续在具有所有 CRI 兼容运行时的集群中工作。
Kubernetes 社区写了一篇关于弃用的详细&lt;a href="https://blog.k8s.io/2020/12/02/dont-panic-kubernetes-and-docker/">博客文章&lt;/a>，并为其提供了一个专门的常见问题&lt;a href="https://blog.k8s.io/2020/12/02/dockershim-faq/">解答页面&lt;/a>。&lt;/p>
&lt;!-- ### Exec Probe Timeout Handling -->
&lt;h3 id="exec-probe-timeout-handling">Exec 探测超时处理&lt;/h3>
&lt;!-- A longstanding bug regarding exec probe timeouts that may impact existing pod definitions has been fixed. Prior to this fix, the field `timeoutSeconds` was not respected for exec probes. Instead, probes would run indefinitely, even past their configured deadline, until a result was returned. With this change, the default value of `1 second` will be applied if a value is not specified and existing pod definitions may no longer be sufficient if a probe takes longer than one second. A feature gate, called `ExecProbeTimeout`, has been added with this fix that enables cluster operators to revert to the previous behavior, but this will be locked and removed in subsequent releases. In order to revert to the previous behavior, cluster operators should set this feature gate to `false`. -->
&lt;p>一个关于 exec 探测超时的长期错误可能会影响现有的 pod 定义，已得到修复。在此修复之前，exec 探测器不考虑 &lt;code>timeoutSeconds&lt;/code> 字段。相反，探测将无限期运行，甚至超过其配置的截止日期，直到返回结果。
通过此更改，如果未指定值，将应用默认值 &lt;code>1 second&lt;/code>，并且如果探测时间超过一秒，现有 pod 定义可能不再足够。
新引入的 &lt;code>ExecProbeTimeout&lt;/code> 特性门控所提供的修复使集群操作员能够恢复到以前的行为，但这种行为将在后续版本中锁定并删除。为了恢复到以前的行为，集群运营商应该将此特性门控设置为 &lt;code>false&lt;/code>。&lt;/p>
&lt;!-- Please review the updated documentation regarding [configuring probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes) for more details. -->
&lt;p>有关更多详细信息，请查看有关配置探针的&lt;a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes">更新文档&lt;/a>。&lt;/p>
&lt;!-- ## Other Updates -->
&lt;h2 id="other-updates">其他更新&lt;/h2>
&lt;!-- ### Graduated to Stable -->
&lt;h3 id="graduated-to-stable">稳定版&lt;/h3>
&lt;!-- * [RuntimeClass](https://github.com/kubernetes/enhancements/issues/585)
* [Built-in API Types Defaults](https://github.com/kubernetes/enhancements/issues/1929)
* [Add Pod-Startup Liveness-Probe Holdoff](https://github.com/kubernetes/enhancements/issues/950)
* [Support CRI-ContainerD On Windows](https://github.com/kubernetes/enhancements/issues/1001)
* [SCTP Support for Services](https://github.com/kubernetes/enhancements/issues/614)
* [Adding AppProtocol To Services And Endpoints](https://github.com/kubernetes/enhancements/issues/1507) -->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/585">RuntimeClass&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1929">内置 API 类型默认值&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/950">添加了对 Pod 层面启动探针和活跃性探针的扼制&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1001">在 Windows 上支持 CRI-ContainerD&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/614">SCTP 对 Services 的支持&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">将 AppProtocol 添加到 Services 和 Endpoints 上&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- ### Notable Feature Updates -->
&lt;h3 id="notable-feature-updates">值得注意的功能更新&lt;/h3>
&lt;!-- * [CronJobs](https://github.com/kubernetes/enhancements/issues/19) -->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/19">CronJobs&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- # Release notes -->
&lt;h1 id="release-notes">发行说明&lt;/h1>
&lt;!-- You can check out the full details of the 1.20 release in the [release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md). -->
&lt;p>你可以在&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md">发行说明&lt;/a>中查看 1.20 发行版的完整详细信息。&lt;/p>
&lt;!-- # Availability of release -->
&lt;h1 id="availability-of-release">可用的发布&lt;/h1>
&lt;!-- Kubernetes 1.20 is available for [download on GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.20.0). There are some great resources out there for getting started with Kubernetes. You can check out some [interactive tutorials](https://kubernetes.io/docs/tutorials/) on the main Kubernetes site, or run a local cluster on your machine using Docker containers with [kind](https://kind.sigs.k8s.io). If you’d like to try building a cluster from scratch, check out the [Kubernetes the Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way) tutorial by Kelsey Hightower. -->
&lt;p>Kubernetes 1.20 可在 &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.20.0">GitHub&lt;/a> 上下载。有一些很棒的资源可以帮助你开始使用 Kubernetes。你可以在 Kubernetes 主站点上查看一些&lt;a href="https://kubernetes.io/docs/tutorials/">交互式教程&lt;/a>，或者使用 &lt;a href="https://kind.sigs.k8s.io">kind&lt;/a> 的 Docker 容器在你的机器上运行本地集群。如果你想尝试从头开始构建集群，请查看 Kelsey Hightower 的 &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes the Hard Way&lt;/a> 教程。&lt;/p>
&lt;!-- # Release Team -->
&lt;h1 id="release-team">发布团队&lt;/h1>
&lt;!-- This release was made possible by a very dedicated group of individuals, who came together as a team in the midst of a lot of things happening out in the world. A huge thank you to the release lead Jeremy Rickard, and to everyone else on the release team for supporting each other, and working so hard to deliver the 1.20 release for the community. -->
&lt;p>这个版本是由一群非常敬业的人促成的，他们在世界上发生的许多事情的时段作为一个团队走到了一起。
非常感谢发布负责人 Jeremy Rickard 以及发布团队中的其他所有人，感谢他们相互支持，并努力为社区发布 1.20 版本。&lt;/p>
&lt;!-- # Release Logo -->
&lt;h1 id="release-logo">发布 Logo&lt;/h1>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-12-08-kubernetes-1.20-release-announcement/laser.png" alt="Kubernetes 1.20 Release Logo">&lt;/p>
&lt;p>&lt;a href="https://www.dictionary.com/browse/rad">raddest&lt;/a>: &lt;em>adjective&lt;/em>, Slang. excellent; wonderful; cool:&lt;/p>
&lt;!-- > The Kubernetes 1.20 Release has been the raddest release yet. -->
&lt;blockquote>
&lt;p>Kubernetes 1.20 版本是迄今为止最激动人心的版本。&lt;/p>
&lt;/blockquote>
&lt;!-- 2020 has been a challenging year for many of us, but Kubernetes contributors have delivered a record-breaking number of enhancements in this release. That is a great accomplishment, so the release lead wanted to end the year with a little bit of levity and pay homage to [Kubernetes 1.14 - Caturnetes](https://github.com/kubernetes/sig-release/tree/master/releases/release-1.14) with a "rad" cat named Humphrey. -->
&lt;p>2020 年对我们中的许多人来说都是充满挑战的一年，但 Kubernetes 贡献者在此版本中提供了创纪录的增强功能。这是一项了不起的成就，因此发布负责人希望以一点轻松的方式结束这一年，并向 &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.14">Kubernetes 1.14 - Caturnetes&lt;/a> 和一只名叫 Humphrey 的 “rad” 猫致敬。&lt;/p>
&lt;!-- Humphrey is the release lead's cat and has a permanent [`blep`](https://www.inverse.com/article/42316-why-do-cats-blep-science-explains). *Rad* was pretty common slang in the 1990s in the United States, and so were laser backgrounds. Humphrey in a 1990s style school picture felt like a fun way to end the year. Hopefully, Humphrey and his *blep* bring you a little joy at the end of 2020! -->
&lt;p>Humphrey是发布负责人的猫，有一个永久的 &lt;code>blep&lt;/code>. 在 1990 年代，&lt;em>Rad&lt;/em> 是美国非常普遍的俚语，激光背景也是如此。Humphrey 在 1990 年代风格的学校照片中感觉像是结束这一年的有趣方式。希望 Humphrey 和它的 &lt;em>blep&lt;/em> 在 2020 年底给你带来一点快乐！&lt;/p>
&lt;!-- The release logo was created by [Henry Hsu - @robotdancebattle](https://www.instagram.com/robotdancebattle/). -->
&lt;p>发布标志由 &lt;a href="https://www.instagram.com/robotdancebattle/">Henry Hsu - @robotdancebattle&lt;/a> 创建。&lt;/p>
&lt;!-- # User Highlights -->
&lt;h1 id="user-highlights">用户亮点&lt;/h1>
&lt;!-- - Apple is operating multi-thousand node Kubernetes clusters in data centers all over the world. Watch [Alena Prokharchyk's KubeCon NA Keynote](https://youtu.be/Tx8qXC-U3KM) to learn more about their cloud native journey. -->
&lt;ul>
&lt;li>Apple 正在世界各地的数据中心运行数千个节点的 Kubernetes 集群。观看 &lt;a href="https://youtu.be/Tx8qXC-U3KM">Alena Prokarchyk&lt;/a> 的 KubeCon NA 主题演讲，了解有关他们的云原生之旅的更多信息。&lt;/li>
&lt;/ul>
&lt;!-- # Project Velocity -->
&lt;h1 id="project-velocity">项目速度&lt;/h1>
&lt;!-- The [CNCF K8s DevStats project](https://k8s.devstats.cncf.io/) aggregates a number of interesting data points related to the velocity of Kubernetes and various sub-projects. This includes everything from individual contributions to the number of companies that are contributing, and is a neat illustration of the depth and breadth of effort that goes into evolving this ecosystem. -->
&lt;p>&lt;a href="https://k8s.devstats.cncf.io/">CNCF K8S DevStats 项目&lt;/a>聚集了许多有关Kubernetes和各分项目的速度有趣的数据点。这包括从个人贡献到做出贡献的公司数量的所有内容，并且清楚地说明了为发展这个生态系统所做的努力的深度和广度。&lt;/p>
&lt;!-- In the v1.20 release cycle, which ran for 11 weeks (September 25 to December 9), we saw contributions from [967 companies](https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.19.0%20-%20now&amp;var-metric=contributions) and [1335 individuals](https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.19.0%20-%20now&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-country_name=All&amp;var-companies=All) ([44 of whom](https://k8s.devstats.cncf.io/d/52/new-contributors?orgId=1&amp;from=1601006400000&amp;to=1607576399000&amp;var-repogroup_name=Kubernetes) made their first Kubernetes contribution) from [26 countries](https://k8s.devstats.cncf.io/d/50/countries-stats?orgId=1&amp;from=1601006400000&amp;to=1607576399000&amp;var-period_name=Quarter&amp;var-countries=All&amp;var-repogroup_name=Kubernetes&amp;var-metric=rcommitters&amp;var-cum=countries). -->
&lt;p>在持续 11 周（9 月 25 日至 12 月 9 日）的 v1.20 发布周期中，我们看到了来自 &lt;a href="https://k8s.devstats.cncf.io/d/50/countries-stats?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-period_name=Quarter&amp;amp;var-countries=All&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-metric=rcommitters&amp;amp;var-cum=countries">26 个国家/地区&lt;/a> 的 &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions">967 家公司&lt;/a> 和 &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.19.0%20-%20now&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All">1335 名个人&lt;/a>（其中 &lt;a href="https://k8s.devstats.cncf.io/d/52/new-contributors?orgId=1&amp;amp;from=1601006400000&amp;amp;to=1607576399000&amp;amp;var-repogroup_name=Kubernetes">44 人&lt;/a>首次为 Kubernetes 做出贡献）的贡献。&lt;/p>
&lt;!-- # Ecosystem Updates -->
&lt;h1 id="ecosystem-updates">生态系统更新&lt;/h1>
&lt;!-- - KubeCon North America just wrapped up three weeks ago, the second such event to be virtual! All talks are [now available to all on-demand](https://www.youtube.com/playlist?list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut) for anyone still needing to catch up!
- In June, the Kubernetes community formed a new working group as a direct response to the Black Lives Matter protests occurring across America. WG Naming's goal is to remove harmful and unclear language in the Kubernetes project as completely as possible and to do so in a way that is portable to other CNCF projects. A great introductory talk on this important work and how it is conducted was given [at KubeCon 2020 North America](https://sched.co/eukp), and the initial impact of this labor [can actually be seen in the v1.20 release](https://github.com/kubernetes/enhancements/issues/2067).
- Previously announced this summer, [The Certified Kubernetes Security Specialist (CKS) Certification](https://www.cncf.io/announcements/2020/11/17/kubernetes-security-specialist-certification-now-available/) was released during Kubecon NA for immediate scheduling! Following the model of CKA and CKAD, the CKS is a performance-based exam, focused on security-themed competencies and domains. This exam is targeted at current CKA holders, particularly those who want to round out their baseline knowledge in securing cloud workloads (which is all of us, right?). -->
&lt;ul>
&lt;li>KubeCon North America 三周前刚刚结束，这是第二个虚拟的此类活动！现在所有演讲都可以&lt;a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2Pn8RxfLh2qrXBDftr6Qjut">点播&lt;/a>，供任何需要赶上的人使用！&lt;/li>
&lt;li>6 月，Kubernetes 社区成立了一个新的工作组，作为对美国各地发生的 Black Lives Matter 抗议活动的直接回应。WG Naming 的目标是尽可能彻底地删除 Kubernetes 项目中有害和不清楚的语言，并以可移植到其他 CNCF 项目的方式进行。在 &lt;a href="https://sched.co/eukp">KubeCon 2020 North America&lt;/a> 上就这项重要工作及其如何进行进行了精彩的介绍性演讲，这项工作的初步影响&lt;a href="https://github.com/kubernetes/enhancements/issues/2067">实际上可以在 v1.20 版本中看到&lt;/a>。&lt;/li>
&lt;li>此前于今年夏天宣布，在 Kubecon NA 期间发布了经认证的 &lt;a href="https://www.cncf.io/announcements/2020/11/17/kubernetes-security-specialist-certification-now-available/">Kubernetes 安全专家 (CKS) 认证&lt;/a> ，以便立即安排！遵循 CKA 和 CKAD 的模型，CKS 是一项基于性能的考试，侧重于以安全为主题的能力和领域。该考试面向当前的 CKA 持有者，尤其是那些想要完善其在保护云工作负载方面的基础知识的人（这是我们所有人，对吧？）。&lt;/li>
&lt;/ul>
&lt;!-- # Event Updates -->
&lt;h1 id="event-updates">活动更新&lt;/h1>
&lt;!-- KubeCon + CloudNativeCon Europe 2021 will take place May 4 - 7, 2021! Registration will open on January 11. You can find more information about the conference [here](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/). Remember that [the CFP](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/cfp/) closes on Sunday, December 13, 11:59pm PST! -->
&lt;p>KubeCon + CloudNativeCon Europe 2021 将于 2021 年 5 月 4 日至 7 日举行！注册将于 1 月 11 日开放。你可以在&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">此处&lt;/a>找到有关会议的更多信息。
请记住，&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/program/cfp/">CFP&lt;/a> 将于太平洋标准时间 12 月 13 日星期日晚上 11:59 关闭！&lt;/p>
&lt;!-- # Upcoming release webinar -->
&lt;h1 id="upcoming-release-webinar">即将发布的网络研讨会&lt;/h1>
&lt;!-- Stay tuned for the upcoming release webinar happening this January. -->
&lt;p>请继续关注今年 1 月即将举行的发布网络研讨会。&lt;/p>
&lt;!-- # Get Involved -->
&lt;h1 id="get-involved">参与其中&lt;/h1>
&lt;!-- If you’re interested in contributing to the Kubernetes community, Special Interest Groups (SIGs) are a great starting point. Many of them may align with your interests! If there are things you’d like to share with the community, you can join the weekly community meeting, or use any of the following channels: -->
&lt;p>如果你有兴趣为 Kubernetes 社区做出贡献，那么特别兴趣小组 (SIG) 是一个很好的起点。其中许多可能符合你的兴趣！如果你有什么想与社区分享的内容，你可以参加每周的社区会议，或使用以下任一渠道：&lt;/p>
&lt;!-- * Find out more about contributing to Kubernetes at the new [Kubernetes Contributor website](https://www.kubernetes.dev/)
* Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
* Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
* Join the community on [Slack](http://slack.k8s.io/)
* Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
* Read more about what’s happening with Kubernetes on the [blog](https://kubernetes.io/blog/)
* Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team) -->
&lt;ul>
&lt;li>在新的 &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributor 网站&lt;/a>上了解更多关于为Kubernetes 做出贡献的信息&lt;/li>
&lt;li>在 Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> 上关注我们以获取最新更新&lt;/li>
&lt;li>加入关于讨论的&lt;a href="https://discuss.kubernetes.io/">社区&lt;/a>讨论&lt;/li>
&lt;li>加入 &lt;a href="http://slack.k8s.io/">Slack 社区&lt;/a>&lt;/li>
&lt;li>分享你的 &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">Kubernetes 故事&lt;/a>&lt;/li>
&lt;li>在&lt;a href="https://kubernetes.io/blog/">博客&lt;/a>上阅读更多关于 Kubernetes 发生的事情&lt;/li>
&lt;li>了解有关 &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes 发布团队&lt;/a>的更多信息&lt;/li>
&lt;/ul></description></item><item><title>Blog: 别慌: Kubernetes 和 Docker</title><link>https://kubernetes.io/zh/blog/2020/12/02/dont-panic-kubernetes-and-docker/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/12/02/dont-panic-kubernetes-and-docker/</guid><description>
&lt;!--
layout: blog
title: "Don't Panic: Kubernetes and Docker"
date: 2020-12-02
slug: dont-panic-kubernetes-and-docker
-->
&lt;p>&lt;strong>作者：&lt;/strong> Jorge Castro, Duffie Cooley, Kat Cosgrove, Justin Garrison, Noah Kantrowitz, Bob Killen, Rey Lejano, Dan “POP” Papandrea, Jeffrey Sica, Davanum “Dims” Srinivas&lt;/p>
&lt;!--
Kubernetes is [deprecating
Docker](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation)
as a container runtime after v1.20.
-->
&lt;p>Kubernetes 从版本 v1.20 之后，&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">弃用 Docker&lt;/a>
这个容器运行时。&lt;/p>
&lt;!--
**You do not need to panic. It’s not as dramatic as it sounds.**
-->
&lt;p>&lt;strong>不必慌张，这件事并没有听起来那么吓人。&lt;/strong>&lt;/p>
&lt;!--
TL;DR Docker as an underlying runtime is being deprecated in favor of runtimes
that use the [Container Runtime Interface (CRI)](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/)
created for Kubernetes. Docker-produced images will continue to work in your
cluster with all runtimes, as they always have.
-->
&lt;p>弃用 Docker 这个底层运行时，转而支持符合为 Kubernetes 创建的容器运行接口
&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface (CRI)&lt;/a>
的运行时。
Docker 构建的镜像，将在你的集群的所有运行时中继续工作，一如既往。&lt;/p>
&lt;!--
If you’re an end-user of Kubernetes, not a whole lot will be changing for you.
This doesn’t mean the death of Docker, and it doesn’t mean you can’t, or
shouldn’t, use Docker as a development tool anymore. Docker is still a useful
tool for building containers, and the images that result from running `docker
build` can still run in your Kubernetes cluster.
-->
&lt;p>如果你是 Kubernetes 的终端用户，这对你不会有太大影响。
这事并不意味着 Docker 已死、也不意味着你不能或不该继续把 Docker 用作开发工具。
Docker 仍然是构建容器的利器，使用命令 &lt;code>docker build&lt;/code> 构建的镜像在 Kubernetes 集群中仍然可以运行。&lt;/p>
&lt;!--
If you’re using a managed Kubernetes service like GKE, EKS, or AKS (which [defaults to containerd](https://github.com/Azure/AKS/releases/tag/2020-11-16)) you will need to
make sure your worker nodes are using a supported container runtime before
Docker support is removed in a future version of Kubernetes. If you have node
customizations you may need to update them based on your environment and runtime
requirements. Please work with your service provider to ensure proper upgrade
testing and planning.
-->
&lt;p>如果你正在使用 GKE、EKS、或 AKS
(&lt;a href="https://github.com/Azure/AKS/releases/tag/2020-11-16">默认使用 containerd&lt;/a>)&lt;br>
这类托管 Kubernetes 服务，你需要在 Kubernetes 后续版本移除对 Docker 支持之前，
确认工作节点使用了被支持的容器运行时。
如果你的节点被定制过，你可能需要根据你自己的环境和运行时需求更新它们。
请与你的服务供应商协作，确保做出适当的升级测试和计划。&lt;/p>
&lt;!--
If you’re rolling your own clusters, you will also need to make changes to avoid
your clusters breaking. At v1.20, you will get a deprecation warning for Docker.
When Docker runtime support is removed in a future release (currently planned
for the 1.22 release in late 2021) of Kubernetes it will no longer be supported
and you will need to switch to one of the other compliant container runtimes,
like containerd or CRI-O. Just make sure that the runtime you choose supports
the docker daemon configurations you currently use (e.g. logging).
-->
&lt;p>如果你正在运营你自己的集群，那还应该做些工作，以避免集群中断。
在 v1.20 版中，你仅会得到一个 Docker 的弃用警告。
当对 Docker 运行时的支持在 Kubernetes 某个后续发行版（目前的计划是 2021 年晚些时候的 1.22 版）中被移除时，
你需要切换到 containerd 或 CRI-O 等兼容的容器运行时。
只要确保你选择的运行时支持你当前使用的 Docker 守护进程配置（例如 logging）。&lt;/p>
&lt;!--
## So why the confusion and what is everyone freaking out about?
-->
&lt;h2 id="so-why-the-confusion-and-what-is-everyone-freaking-out-about">那为什么会有这样的困惑，为什么每个人要害怕呢？&lt;/h2>
&lt;!--
We’re talking about two different environments here, and that’s creating
confusion. Inside of your Kubernetes cluster, there’s a thing called a container
runtime that’s responsible for pulling and running your container images. Docker
is a popular choice for that runtime (other common options include containerd
and CRI-O), but Docker was not designed to be embedded inside Kubernetes, and
that causes a problem.
-->
&lt;p>我们在这里讨论的是两套不同的环境，这就是造成困惑的根源。
在你的 Kubernetes 集群中，有一个叫做容器运行时的东西，它负责拉取并运行容器镜像。
Docker 对于运行时来说是一个流行的选择（其他常见的选择包括 containerd 和 CRI-O），
但 Docker 并非设计用来嵌入到 Kubernetes，这就是问题所在。&lt;/p>
&lt;!--
You see, the thing we call “Docker” isn’t actually one thing&amp;mdash;it’s an entire
tech stack, and one part of it is a thing called “containerd,” which is a
high-level container runtime by itself. Docker is cool and useful because it has
a lot of UX enhancements that make it really easy for humans to interact with
while we’re doing development work, but those UX enhancements aren’t necessary
for Kubernetes, because it isn’t a human.
-->
&lt;p>你看，我们称之为 “Docker” 的物件实际上并不是一个物件——它是一个完整的技术堆栈，
它其中一个叫做 “containerd” 的部件本身，才是一个高级容器运行时。
Docker 既酷炫又实用，因为它提供了很多用户体验增强功能，而这简化了我们做开发工作时的操作，
Kubernetes 用不到这些增强的用户体验，毕竟它并非人类。&lt;/p>
&lt;!--
As a result of this human-friendly abstraction layer, your Kubernetes cluster
has to use another tool called Dockershim to get at what it really needs, which
is containerd. That’s not great, because it gives us another thing that has to
be maintained and can possibly break. What’s actually happening here is that
Dockershim is being removed from Kubelet as early as v1.23 release, which
removes support for Docker as a container runtime as a result. You might be
thinking to yourself, but if containerd is included in the Docker stack, why
does Kubernetes need the Dockershim?
-->
&lt;p>因为这个用户友好的抽象层，Kubernetes 集群不得不引入一个叫做 Dockershim 的工具来访问它真正需要的 containerd。
这不是一件好事，因为这引入了额外的运维工作量，而且还可能出错。
实际上正在发生的事情就是：Dockershim 将在不早于 v1.23 版中从 kubelet 中被移除，也就取消对 Docker 容器运行时的支持。
你心里可能会想，如果 containerd 已经包含在 Docker 堆栈中，为什么 Kubernetes 需要 Dockershim。&lt;/p>
&lt;!--
Docker isn’t compliant with CRI, the [Container Runtime Interface](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/).
If it were, we wouldn’t need the shim, and this wouldn’t be a thing. But it’s
not the end of the world, and you don’t need to panic&amp;mdash;you just need to change
your container runtime from Docker to another supported container runtime.
-->
&lt;p>Docker 不兼容 CRI，
&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">容器运行时接口&lt;/a>。
如果支持，我们就不需要这个 shim 了，也就没问题了。
但这也不是世界末日，你也不需要恐慌——你唯一要做的就是把你的容器运行时从 Docker 切换到其他受支持的容器运行时。&lt;/p>
&lt;!--
One thing to note: If you are relying on the underlying docker socket
(`/var/run/docker.sock`) as part of a workflow within your cluster today, moving
to a different runtime will break your ability to use it. This pattern is often
called Docker in Docker. There are lots of options out there for this specific
use case including things like
[kaniko](https://github.com/GoogleContainerTools/kaniko),
[img](https://github.com/genuinetools/img), and
[buildah](https://github.com/containers/buildah).
-->
&lt;p>要注意一点：如果你依赖底层的 Docker 套接字(&lt;code>/var/run/docker.sock&lt;/code>)，作为你集群中工作流的一部分，
切换到不同的运行时会导致你无法使用它。
这种模式经常被称之为嵌套 Docker（Docker in Docker）。
对于这种特殊的场景，有很多选项，比如：
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>、
&lt;a href="https://github.com/genuinetools/img">img&lt;/a>、和
&lt;a href="https://github.com/containers/buildah">buildah&lt;/a>。&lt;/p>
&lt;!--
## What does this change mean for developers, though? Do we still write Dockerfiles? Do we still build things with Docker?
-->
&lt;h2 id="what-does-this-change-mean-for-developers">那么，这一改变对开发人员意味着什么？我们还要写 Dockerfile 吗？还能用 Docker 构建镜像吗？&lt;/h2>
&lt;!--
This change addresses a different environment than most folks use to interact
with Docker. The Docker installation you’re using in development is unrelated to
the Docker runtime inside your Kubernetes cluster. It’s confusing, we understand.
As a developer, Docker is still useful to you in all the ways it was before this
change was announced. The image that Docker produces isn’t really a
Docker-specific image&amp;mdash;it’s an OCI ([Open Container Initiative](https://opencontainers.org/)) image.
Any OCI-compliant image, regardless of the tool you use to build it, will look
the same to Kubernetes. Both [containerd](https://containerd.io/) and
[CRI-O](https://cri-o.io/) know how to pull those images and run them. This is
why we have a standard for what containers should look like.
-->
&lt;p>此次改变带来了一个不同的环境，这不同于我们常用的 Docker 交互方式。
你在开发环境中用的 Docker 和你 Kubernetes 集群中的 Docker 运行时无关。
我们知道这听起来让人困惑。
对于开发人员，Docker 从所有角度来看仍然有用，就跟这次改变之前一样。
Docker 构建的镜像并不是 Docker 特有的镜像——它是一个
OCI（&lt;a href="https://opencontainers.org/">开放容器标准&lt;/a>）镜像。
任一 OCI 兼容的镜像，不管它是用什么工具构建的，在 Kubernetes 的角度来看都是一样的。
&lt;a href="https://containerd.io/">containerd&lt;/a> 和
&lt;a href="https://cri-o.io/">CRI-O&lt;/a>
两者都知道怎么拉取并运行这些镜像。
这就是我们制定容器标准的原因。&lt;/p>
&lt;!--
So, this change is coming. It’s going to cause issues for some, but it isn’t
catastrophic, and generally it’s a good thing. Depending on how you interact
with Kubernetes, this could mean nothing to you, or it could mean a bit of work.
In the long run, it’s going to make things easier. If this is still confusing
for you, that’s okay&amp;mdash;there’s a lot going on here; Kubernetes has a lot of
moving parts, and nobody is an expert in 100% of it. We encourage any and all
questions regardless of experience level or complexity! Our goal is to make sure
everyone is educated as much as possible on the upcoming changes. We hope
this has answered most of your questions and soothed some anxieties! ❤️
-->
&lt;p>所以，改变已经发生。
它确实带来了一些问题，但这不是一个灾难，总的说来，这还是一件好事。
根据你操作 Kubernetes 的方式的不同，这可能对你不构成任何问题，或者也只是意味着一点点的工作量。
从一个长远的角度看，它使得事情更简单。
如果你还在困惑，也没问题——这里还有很多事情；
Kubernetes 有很多变化中的功能，没有人是100%的专家。
我们鼓励你提出任何问题，无论水平高低、问题难易。
我们的目标是确保所有人都能在即将到来的改变中获得足够的了解。
我们希望这已经回答了你的大部分问题，并缓解了一些焦虑！❤️&lt;/p>
&lt;!--
Looking for more answers? Check out our accompanying [Dockershim Deprecation FAQ](/blog/2020/12/02/dockershim-faq/).
-->
&lt;p>还在寻求更多答案吗？请参考我们附带的
&lt;a href="https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/">弃用 Dockershim 的常见问题&lt;/a>。&lt;/p></description></item><item><title>Blog: 弃用 Dockershim 的常见问题</title><link>https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/12/02/dockershim-faq/</guid><description>
&lt;!--
layout: blog
title: "Dockershim Deprecation FAQ"
date: 2020-12-02
slug: dockershim-faq
aliases: [ '/dockershim' ]
-->
&lt;!--
This document goes over some frequently asked questions regarding the Dockershim
deprecation announced as a part of the Kubernetes v1.20 release. For more detail
on the deprecation of Docker as a container runtime for Kubernetes kubelets, and
what that means, check out the blog post
[Don't Panic: Kubernetes and Docker](/blog/2020/12/02/dont-panic-kubernetes-and-docker/).
-->
&lt;p>本文回顾了自 Kubernetes v1.20 版宣布弃用 Dockershim 以来所引发的一些常见问题。
关于 Kubernetes kubelets 从容器运行时的角度弃用 Docker 的细节以及这些细节背后的含义，请参考博文
&lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">别慌: Kubernetes 和 Docker&lt;/a>。&lt;/p>
&lt;!--
### Why is dockershim being deprecated?
-->
&lt;h3 id="why-is-dockershim-being-deprecated">为什么弃用 dockershim&lt;/h3>
&lt;!--
Maintaining dockershim has become a heavy burden on the Kubernetes maintainers.
The CRI standard was created to reduce this burden and allow smooth interoperability
of different container runtimes. Docker itself doesn't currently implement CRI,
thus the problem.
-->
&lt;p>维护 dockershim 已经成为 Kubernetes 维护者肩头一个沉重的负担。
创建 CRI 标准就是为了减轻这个负担，同时也可以增加不同容器运行时之间平滑的互操作性。
但反观 Docker 却至今也没有实现 CRI，所以麻烦就来了。&lt;/p>
&lt;!--
Dockershim was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
[Dockershim Removal Kubernetes Enhancement Proposal][drkep].
-->
&lt;p>Dockershim 向来都是一个临时解决方案（因此得名：shim）。
你可以进一步阅读
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">移除 Kubernetes 增强方案 Dockershim&lt;/a>
以了解相关的社区讨论和计划。&lt;/p>
&lt;!--
Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing support for the dockershim will allow further development in
those areas.
-->
&lt;p>此外，与 dockershim 不兼容的一些特性，例如：控制组（cgoups）v2 和用户名字空间（user namespace），已经在新的 CRI 运行时中被实现。
移除对 dockershim 的支持将加速这些领域的发展。&lt;/p>
&lt;!--
### Can I still use Docker in Kubernetes 1.20?
-->
&lt;h3 id="can-I-still-use-docker-in-kubernetes-1.20">在 Kubernetes 1.20 版本中，我还可以用 Docker 吗？&lt;/h3>
&lt;!--
Yes, the only thing changing in 1.20 is a single warning log printed at [kubelet]
startup if using Docker as the runtime.
-->
&lt;p>当然可以，在 1.20 版本中仅有的改变就是：如果使用 Docker 运行时，启动
&lt;a href="https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet/">kubelet&lt;/a>
的过程中将打印一条警告日志。&lt;/p>
&lt;!--
### When will dockershim be removed?
-->
&lt;h3 id="when-will-dockershim-be-removed">什么时候移除 dockershim&lt;/h3>
&lt;!--
Given the impact of this change, we are using an extended deprecation timeline.
It will not be removed before Kubernetes 1.22, meaning the earliest release without
dockershim would be 1.23 in late 2021.
_Update_: removal of dockershim is scheduled for Kubernetes v1.24, see
[Dockershim Removal Kubernetes Enhancement Proposal][drkep].
We will be working closely with vendors and other ecosystem groups to ensure a smooth transition and will evaluate
things as the situation evolves.
-->
&lt;p>考虑到此改变带来的影响，我们使用了一个加长的废弃时间表。
在 Kubernetes 1.22 版之前，它不会被彻底移除；换句话说，dockershim 被移除的最早版本会是 2021 年底发布的 1.23 版。
&lt;em>更新&lt;/em>：dockershim 计划在 Kubernetes 1.24 版被移除，
请参阅&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">移除 Kubernetes 增强方案 Dockershim&lt;/a>。
我们将与供应商以及其他生态团队紧密合作，确保顺利过渡，并将依据事态的发展评估后续事项。&lt;/p>
&lt;!--
### Will my existing Docker images still work?
-->
&lt;h3 id="will-my-existing-docker-image-still-work">我现有的 Docker 镜像还能正常工作吗？&lt;/h3>
&lt;!--
Yes, the images produced from `docker build` will work with all CRI implementations.
All your existing images will still work exactly the same.
-->
&lt;p>当然可以，&lt;code>docker build&lt;/code> 创建的镜像适用于任何 CRI 实现。
所有你的现有镜像将和往常一样工作。&lt;/p>
&lt;!--
### What about private images?
-->
&lt;h3 id="what-about-private-images">私有镜像呢？&lt;/h3>
&lt;!--
Yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.
-->
&lt;p>当然可以。所有 CRI 运行时均支持 Kubernetes 中相同的拉取（pull）Secret 配置，
不管是通过 PodSpec 还是通过 ServiceAccount 均可。&lt;/p>
&lt;!--
### Are Docker and containers the same thing?
-->
&lt;h3 id="are-docker-and-containers-the-same-thing">Docker 和容器是一回事吗？&lt;/h3>
&lt;!--
Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.
-->
&lt;p>虽然 Linux 的容器技术已经存在了很久，
但 Docker 普及了 Linux 容器这种技术模式，并在开发底层技术方面发挥了重要作用。
容器的生态相比于单纯的 Docker，已经进化到了一个更宽广的领域。
像 OCI 和 CRI 这类标准帮助许多工具在我们的生态中成长和繁荣，
其中一些工具替代了 Docker 的某些部分，另一些增强了现有功能。&lt;/p>
&lt;!--
### Are there examples of folks using other runtimes in production today?
-->
&lt;h3 id="are-there-example-of-folks-using-other-runtimes-in-production-today">现在是否有在生产系统中使用其他运行时的例子？&lt;/h3>
&lt;!--
All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.
-->
&lt;p>Kubernetes 所有项目在所有版本中出产的工件（Kubernetes 二进制文件）都经过了验证。&lt;/p>
&lt;!--
Additionally, the [kind] project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the [CRI-O] runtime in production since June 2019.
-->
&lt;p>此外，&lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> 项目使用 containerd 已经有年头了，
并且在这个场景中，稳定性还明显得到提升。
Kind 和 containerd 每天都会做多次协调，以验证对 Kubernetes 代码库的所有更改。
其他相关项目也遵循同样的模式，从而展示了其他容器运行时的稳定性和可用性。
例如，OpenShift 4.x 从 2019 年 6 月以来，就一直在生产环境中使用 &lt;a href="https://cri-o.io/">CRI-O&lt;/a> 运行时。&lt;/p>
&lt;!--
For other examples and references you can look at the adopters of containerd and
CRI-O, two container runtimes under the Cloud Native Computing Foundation ([CNCF]).
- [containerd](https://github.com/containerd/containerd/blob/master/ADOPTERS.md)
- [CRI-O](https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md)
-->
&lt;p>至于其他示例和参考资料，你可以查看 containerd 和 CRI-O 的使用者列表，
这两个容器运行时是云原生基金会（[CNCF]）下的项目。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containerd/containerd/blob/master/ADOPTERS.md">containerd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md">CRI-O&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### People keep referencing OCI, what is that?
-->
&lt;h3 id="people-keep-referenceing-oci-what-is-that">人们总在谈论 OCI，那是什么？&lt;/h3>
&lt;!--
OCI stands for the [Open Container Initiative], which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of [runc], which is the underlying default runtime for both
[containerd] and [CRI-O]. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.
-->
&lt;p>OCI 代表&lt;a href="https://opencontainers.org/about/overview/">开放容器标准&lt;/a>，
它标准化了容器工具和底层实现（technologies）之间的大量接口。
他们维护了打包容器镜像（OCI image-spec）和运行容器（OCI runtime-spec）的标准规范。
他们还以 &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a>
的形式维护了一个 runtime-spec 的真实实现，
这也是 &lt;a href="https://containerd.io/">containerd&lt;/a> 和 &lt;a href="https://cri-o.io/">CRI-O&lt;/a> 依赖的默认运行时。
CRI 建立在这些底层规范之上，为管理容器提供端到端的标准。&lt;/p>
&lt;!--
### Which CRI implementation should I use?
-->
&lt;h3 id="which-cri-implementation-should-I-use">我应该用哪个 CRI 实现？&lt;/h3>
&lt;!--
That’s a complex question and it depends on a lot of factors. If Docker is
working for you, moving to containerd should be a relatively easy swap and
will have strictly better performance and less overhead. However, we encourage you
to explore all the options from the [CNCF landscape] in case another would be an
even better fit for your environment.
-->
&lt;p>这是一个复杂的问题，依赖于许多因素。
在 Docker 工作良好的情况下，迁移到 containerd 是一个相对容易的转换，并将获得更好的性能和更少的开销。
然而，我们建议你先探索 &lt;a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category">CNCF 全景图&lt;/a>
提供的所有选项，以做出更适合你的环境的选择。&lt;/p>
&lt;!--
### What should I look out for when changing CRI implementations?
-->
&lt;h3 id="what-should-I-look-out-for-when-changing-CRI-implementation">当切换 CRI 底层实现时，我应该注意什么？&lt;/h3>
&lt;!--
While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:
-->
&lt;p>Docker 和大多数 CRI（包括 containerd）的底层容器化代码是相同的，但其周边部分却存在一些不同。
迁移时一些常见的关注点是：&lt;/p>
&lt;!--
- Logging configuration
- Runtime resource limitations
- Node provisioning scripts that call docker or use docker via it's control socket
- Kubectl plugins that require docker CLI or the control socket
- Kubernetes tools that require direct access to Docker (e.g. kube-imagepuller)
- Configuration of functionality like `registry-mirrors` and insecure registries
- Other support scripts or daemons that expect Docker to be available and are run
outside of Kubernetes (e.g. monitoring or security agents)
- GPUs or special hardware and how they integrate with your runtime and Kubernetes
-->
&lt;ul>
&lt;li>日志配置&lt;/li>
&lt;li>运行时的资源限制&lt;/li>
&lt;li>直接访问 docker 命令或通过控制套接字调用 Docker 的节点供应脚本&lt;/li>
&lt;li>需要访问 docker 命令或控制套接字的 kubectl 插件&lt;/li>
&lt;li>需要直接访问 Docker 的 Kubernetes 工具（例如：kube-imagepuller）&lt;/li>
&lt;li>像 &lt;code>registry-mirrors&lt;/code> 和不安全的注册表这类功能的配置&lt;/li>
&lt;li>需要 Docker 保持可用、且运行在 Kubernetes 之外的，其他支持脚本或守护进程（例如：监视或安全代理）&lt;/li>
&lt;li>GPU 或特殊硬件，以及它们如何与你的运行时和 Kubernetes 集成&lt;/li>
&lt;/ul>
&lt;!--
If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if you’ve customized
your dockerd configuration, you’ll need to adapt that for your new container
runtime where possible.
-->
&lt;p>如果你只是用了 Kubernetes 资源请求/限制或基于文件的日志收集 DaemonSet，它们将继续稳定工作，
但是如果你用了自定义了 dockerd 配置，则可能需要为新容器运行时做一些适配工作。&lt;/p>
&lt;!--
Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the [`crictl`][cr] tool as a drop-in replacement (see [mapping from docker cli to crictl](https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/#mapping-from-docker-cli-to-crictl)) and for the
latter you can use newer container build options like [img], [buildah],
[kaniko], or [buildkit-cli-for-kubectl] that don’t require Docker.
-->
&lt;p>另外还有一个需要关注的点，那就是当创建镜像时，系统维护或嵌入容器方面的任务将无法工作。
对于前者，可以用 &lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> 工具作为临时替代方案
(参见 &lt;a href="https://kubernetes.io/zh/docs/tasks/debug-application-cluster/crictl/#mapping-from-docker-cli-to-crictl">从 docker 命令映射到 crictl&lt;/a>)；
对于后者，可以用新的容器创建选项，比如
&lt;a href="https://github.com/genuinetools/img">img&lt;/a>、
&lt;a href="https://github.com/containers/buildah">buildah&lt;/a>、
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>、或
&lt;a href="https://github.com/vmware-tanzu/buildkit-cli-for-kubectl">buildkit-cli-for-kubectl&lt;/a>，
他们均不需要访问 Docker。&lt;/p>
&lt;!--
For containerd, you can start with their [documentation] to see what configuration
options are available as you migrate things over.
-->
&lt;p>对于 containerd，你可以从它们的
&lt;a href="https://github.com/containerd/cri/blob/master/docs/registry.md">文档&lt;/a>
开始，看看在迁移过程中有哪些配置选项可用。&lt;/p>
&lt;!--
For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on [Container Runtimes]
-->
&lt;p>对于如何协同 Kubernetes 使用 containerd 和 CRI-O 的说明，参见 Kubernetes 文档中这部分：
&lt;a href="https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes">容器运行时&lt;/a>。&lt;/p>
&lt;!--
### What if I have more questions?
-->
&lt;h3 id="what-if-I-have-more-question">我还有问题怎么办？&lt;/h3>
&lt;!--
If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: https://discuss.kubernetes.io/.
-->
&lt;p>如果你使用了一个有供应商支持的 Kubernetes 发行版，你可以咨询供应商他们产品的升级计划。
对于最终用户的问题，请把问题发到我们的最终用户社区的论坛：https://discuss.kubernetes.io/。&lt;/p>
&lt;!--
You can also check out the excellent blog post
[Wait, Docker is deprecated in Kubernetes now?][dep] a more in-depth technical
discussion of the changes.
-->
&lt;p>你也可以看看这篇优秀的博文：
&lt;a href="https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m">等等，Docker 刚刚被 Kubernetes 废掉了？&lt;/a>
一个对此变化更深入的技术讨论。&lt;/p>
&lt;!--
### Can I have a hug?
-->
&lt;h3 id="can-I-have-a-hug">我可以加入吗？&lt;/h3>
&lt;!--
Always and whenever you want! 🤗🤗
-->
&lt;p>只要你愿意，随时随地欢迎加入！&lt;/p></description></item><item><title>Blog: 为开发指南做贡献</title><link>https://kubernetes.io/zh/blog/2020/10/01/%E4%B8%BA%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E5%81%9A%E8%B4%A1%E7%8C%AE/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/10/01/%E4%B8%BA%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E5%81%9A%E8%B4%A1%E7%8C%AE/</guid><description>
&lt;!--
---
title: "Contributing to the Development Guide"
linkTitle: "Contributing to the Development Guide"
Author: Erik L. Arneson
Description: "A new contributor describes the experience of writing and submitting changes to the Kubernetes Development Guide."
date: 2020-10-01
canonicalUrl: https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/
resources:
- src: "jorge-castro-code-of-conduct.jpg"
title: "Jorge Castro announcing the Kubernetes Code of Conduct during a weekly SIG ContribEx meeting."
---
-->
&lt;!--
When most people think of contributing to an open source project, I suspect they probably think of
contributing code changes, new features, and bug fixes. As a software engineer and a long-time open
source user and contributor, that's certainly what I thought. Although I have written a good quantity
of documentation in different workflows, the massive size of the Kubernetes community was a new kind
of "client." I just didn't know what to expect when Google asked my compatriots and me at
[Lion's Way](https://lionswaycontent.com/) to make much-needed updates to the Kubernetes Development Guide.
*This article originally appeared on the [Kubernetes Contributor Community blog](https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/).*
-->
&lt;p>当大多数人想到为一个开源项目做贡献时，我猜想他们可能想到的是贡献代码修改、新功能和错误修复。作为一个软件工程师和一个长期的开源用户和贡献者，这也正是我的想法。
虽然我已经在不同的工作流中写了不少文档，但规模庞大的 Kubernetes 社区是一种新型 &amp;quot;客户&amp;quot;。我只是不知道当 Google 要求我和 &lt;a href="https://lionswaycontent.com/">Lion's Way&lt;/a> 的同胞们对 Kubernetes 开发指南进行必要更新时会发生什么。&lt;/p>
&lt;p>&lt;em>本文最初出现在 &lt;a href="https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/">Kubernetes Contributor Community blog&lt;/a>。&lt;/em>&lt;/p>
&lt;!--
## The Delights of Working With a Community
As professional writers, we are used to being hired to write very specific pieces. We specialize in
marketing, training, and documentation for technical services and products, which can range anywhere from relatively fluffy marketing emails to deeply technical white papers targeted at IT and developers. With
this kind of professional service, every deliverable tends to have a measurable return on investment.
I knew this metric wouldn't be present when working on open source documentation, but I couldn't
predict how it would change my relationship with the project.
-->
&lt;h2 id="与社区合作的乐趣">与社区合作的乐趣&lt;/h2>
&lt;p>作为专业的写手，我们习惯了受雇于他人去书写非常具体的项目。我们专注于技术服务，产品营销，技术培训以及文档编制，范围从相对宽松的营销邮件到针对 IT 和开发人员的深层技术白皮书。
在这种专业服务下，每一个可交付的项目往往都有可衡量的投资回报。我知道在从事开源文档工作时不会出现这个指标，但我不确定它将如何改变我与项目的关系。&lt;/p>
&lt;!--
One of the primary traits of the relationship between our writing and our traditional clients is that we
always have one or two primary points of contact inside a company. These contacts are responsible
for reviewing our writing and making sure it matches the voice of the company and targets the
audience they're looking for. It can be stressful -- which is why I'm so glad that my writing
partner, eagle-eyed reviewer, and bloodthirsty editor [Joel](https://twitter.com/JoelByronBarker)
handles most of the client contact.
-->
&lt;p>我们的写作和传统客户之间的关系有一个主要的特点，就是我们在一个公司里面总是有一两个主要的对接人。他们负责审查我们的文稿，并确保文稿内容符合公司的声明且对标于他们正在寻找的受众。
这随之而来的压力--正好解释了为什么我很高兴我的写作伙伴、鹰眼审稿人同时也是嗜血编辑的 &lt;a href="https://twitter.com/JoelByronBarker">Joel&lt;/a> 处理了大部分的客户联系。&lt;/p>
&lt;!--
I was surprised and delighted that all of the stress of client contact went out the window when
working with the Kubernetes community.
-->
&lt;p>在与 Kubernetes 社区合作时，所有与客户接触的压力都消失了，这让我感到惊讶和高兴。&lt;/p>
&lt;!--
"How delicate do I have to be? What if I screw up? What if I make a developer angry? What if I make
enemies?" These were all questions that raced through my mind and made me feel like I was
approaching a field of eggshells when I first joined the `#sig-contribex` channel on the Kubernetes
Slack and announced that I would be working on the
[Development Guide](https://github.com/kubernetes/community/blob/master/contributors/devel/development.md).
-->
&lt;p>&amp;quot;我必须得多仔细？如果我搞砸了怎么办？如果我让开发商生气了怎么办？如果我树敌了怎么办？&amp;quot;。
当我第一次加入 Kubernetes Slack 上的 &amp;quot;#sig-contribex &amp;quot; 频道并宣布我将编写 &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/development.md">开发指南&lt;/a> 时，这些问题都在我脑海中奔腾，让我感觉如履薄冰。&lt;/p>
&lt;!--
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 810px">
&lt;img class="card-img-top" src="https://kubernetes.io/zh/blog/2020/10/01/%E4%B8%BA%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E5%81%9A%E8%B4%A1%E7%8C%AE/jorge-castro-code-of-conduct_hu5bc3c30874931ced96ecf71d135c93d2_143155_800x450_fit_q75_catmullrom.jpg" width="800" height="450">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
"The Kubernetes Code of Conduct is in effect, so please be excellent to each other." &amp;mdash; Jorge
Castro, SIG ContribEx co-chair
&lt;/p>
&lt;/div>
&lt;/div>
-->
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 810px">
&lt;img class="card-img-top" src="https://kubernetes.io/zh/blog/2020/10/01/%E4%B8%BA%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97%E5%81%9A%E8%B4%A1%E7%8C%AE/jorge-castro-code-of-conduct_hu5bc3c30874931ced96ecf71d135c93d2_143155_800x450_fit_q75_catmullrom.jpg" width="800" height="450">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
"Kubernetes 编码准则已经生效，让我们共同勉励。" &amp;mdash; Jorge
Castro, SIG ContribEx co-chair
&lt;/p>
&lt;/div>
&lt;/div>
&lt;!--
My fears were unfounded. Immediately, I felt welcome. I like to think this isn't just because I was
working on a much needed task, but rather because the Kubernetes community is filled
with friendly, welcoming people. During the weekly SIG ContribEx meetings, our reports on progress
with the Development Guide were included immediately. In addition, the leader of the meeting would
always stress that the [Kubernetes Code of Conduct](https://www.kubernetes.dev/resources/code-of-conduct/) was in
effect, and that we should, like Bill and Ted, be excellent to each other.
-->
&lt;p>事实上我的担心是多虑的。很快，我就感觉到自己是被欢迎的。我倾向于认为这不仅仅是因为我正在从事一项急需的任务，而是因为 Kubernetes 社区充满了友好、热情的人们。
在每周的 SIG ContribEx 会议上，我们关于开发指南进展情况的报告会被立即纳入其中。此外，会议的领导会一直强调 &lt;a href="https://www.kubernetes.dev/resources/code-of-conduct/">Kubernetes&lt;/a> 编码准则，我们应该像 Bill 和 Ted 一样，相互进步。&lt;/p>
&lt;!--
## This Doesn't Mean It's All Easy
The Development Guide needed a pretty serious overhaul. When we got our hands on it, it was already
packed with information and lots of steps for new developers to go through, but it was getting dusty
with age and neglect. Documentation can really require a global look, not just point fixes.
As a result, I ended up submitting a gargantuan pull request to the
[Community repo](https://github.com/kubernetes/community): 267 additions and 88 deletions.
-->
&lt;h2 id="这并不意味着这一切都很简单">这并不意味着这一切都很简单&lt;/h2>
&lt;p>开发指南需要一次全面检查。当我们拿到它的时候，它已经捆绑了大量的信息和很多新开发者需要经历的步骤，但随着时间的推移和被忽视，它变得相当陈旧。
文档的确需要全局观，而不仅仅是点与点的修复。结果，最终我向这个项目提交了一个巨大的 pull 请求。&lt;a href="https://github.com/kubernetes/community">社区仓库&lt;/a>：新增 267 行，删除 88 行。&lt;/p>
&lt;!--
The life cycle of a pull request requires a certain number of Kubernetes organization members to review and approve changes
before they can be merged. This is a great practice, as it keeps both documentation and code in
pretty good shape, but it can be tough to cajole the right people into taking the time for such a hefty
review. As a result, that massive PR took 26 days from my first submission to final merge. But in
the end, [it was successful](https://github.com/kubernetes/community/pull/5003).
-->
&lt;p>pull 请求的周期需要一定数量的 Kubernetes 组织成员审查和批准更改后才能合并。这是一个很好的做法，因为它使文档和代码都保持在相当不错的状态，
但要哄骗合适的人花时间来做这样一个赫赫有名的审查是很难的。
因此，那次大规模的 PR 从我第一次提交到最后合并，用了 26 天。 但最终，&lt;a href="https://github.com/kubernetes/community/pull/5003">它是成功的&lt;/a>.&lt;/p>
&lt;!--
Since Kubernetes is a pretty fast-moving project, and since developers typically aren't really
excited about writing documentation, I also ran into the problem that sometimes, the secret jewels
that describe the workings of a Kubernetes subsystem are buried deep within the [labyrinthine mind of
a brilliant engineer](https://github.com/amwat), and not in plain English in a Markdown file. I ran headlong into this issue
when it came time to update the getting started documentation for end-to-end (e2e) testing.
-->
&lt;p>由于 Kubernetes 是一个发展相当迅速的项目，而且开发人员通常对编写文档并不十分感兴趣，所以我也遇到了一个问题，那就是有时候，
描述 Kubernetes 子系统工作原理的秘密珍宝被深埋在 &lt;a href="https://github.com/amwat">天才工程师的迷宫式思维&lt;/a> 中，而不是用单纯的英文写在 Markdown 文件中。
当我要更新端到端（e2e）测试的入门文档时，就一头撞上了这个问题。&lt;/p>
&lt;!--
This portion of my journey took me out of documentation-writing territory and into the role of a
brand new user of some unfinished software. I ended up working with one of the developers of the new
[`kubetest2` framework](https://github.com/kubernetes-sigs/kubetest2) to document the latest process of
getting up-and-running for e2e testing, but it required a lot of head scratching on my part. You can
judge the results for yourself by checking out my
[completed pull request](https://github.com/kubernetes/community/pull/5045).
-->
&lt;p>这段旅程将我带出了编写文档的领域，进入到一些未完成软件的全新用户角色。最终我花了很多心思与新的 &lt;a href="https://github.com/kubernetes-sigs/kubetest2">kubetest2`框架&lt;/a> 的开发者之一合作，
记录了最新 e2e 测试的启动和运行过程。
你可以通过查看我的 &lt;a href="https://github.com/kubernetes/community/pull/5045">已完成的 pull request&lt;/a> 来自己判断结果。&lt;/p>
&lt;!--
## Nobody Is the Boss, and Everybody Gives Feedback
But while I secretly expected chaos, the process of contributing to the Kubernetes Development Guide
and interacting with the amazing Kubernetes community went incredibly smoothly. There was no
contention. I made no enemies. Everybody was incredibly friendly and welcoming. It was *enjoyable*.
-->
&lt;h2 id="没有人是老板-每个人都给出反馈">没有人是老板，每个人都给出反馈。&lt;/h2>
&lt;p>但当我暗自期待混乱的时候，为 Kubernetes 开发指南做贡献以及与神奇的 Kubernetes 社区互动的过程却非常顺利。
没有争执，我也没有树敌。每个人都非常友好和热情。这是令人&lt;em>愉快的&lt;/em>。&lt;/p>
&lt;!--
With an open source project, there is no one boss. The Kubernetes project, which approaches being
gargantuan, is split into many different special interest groups (SIGs), working groups, and
communities. Each has its own regularly scheduled meetings, assigned duties, and elected
chairpersons. My work intersected with the efforts of both SIG ContribEx (who watch over and seek to
improve the contributor experience) and SIG Testing (who are in charge of testing). Both of these
SIGs proved easy to work with, eager for contributions, and populated with incredibly friendly and
welcoming people.
-->
&lt;p>对于一个开源项目，没人是老板。Kubernetes 项目，一个近乎巨大的项目，被分割成许多不同的特殊兴趣小组（SIG）、工作组和社区。
每个小组都有自己的定期会议、职责分配和主席推选。我的工作与 SIG ContribEx（负责监督并寻求改善贡献者体验）和 SIG Testing（负责测试）的工作有交集。
事实证明，这两个 SIG 都很容易合作，他们渴望贡献，而且都是非常友好和热情的人。&lt;/p>
&lt;!--
In an active, living project like Kubernetes, documentation continues to need maintenance, revision,
and testing alongside the code base. The Development Guide will continue to be crucial to onboarding
new contributors to the Kubernetes code base, and as our efforts have shown, it is important that
this guide keeps pace with the evolution of the Kubernetes project.
-->
&lt;p>在 Kubernetes 这样一个活跃的、有生命力的项目中，文档仍然需要与代码库一起进行维护、修订和测试。
开发指南将继续对 Kubernetes 代码库的新贡献者起到至关重要的作用，正如我们的努力所显示的那样，该指南必须与 Kubernetes 项目的发展保持同步。&lt;/p>
&lt;!--
Joel and I really enjoy interacting with the Kubernetes community and contributing to
the Development Guide. I really look forward to continuing to not only contributing more, but to
continuing to build the new friendships I've made in this vast open source community over the past
few months.
-->
&lt;p>Joel 和我非常喜欢与 Kubernetes 社区互动并为开发指南做出贡献。我真的很期待，不仅能继续做出更多贡献，还能继续与过去几个月在这个庞大的开源社区中结识的新朋友进行合作。&lt;/p></description></item><item><title>Blog: 结构化日志介绍</title><link>https://kubernetes.io/zh/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</guid><description>
&lt;!--
layout: blog
title: 'Introducing Structured Logs'
date: 2020-09-04
slug: kubernetes-1-19-Introducing-Structured-Logs
-->
&lt;!--
**Authors:** Marek Siarkowicz (Google), Nathan Beach (Google)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Marek Siarkowicz（谷歌），Nathan Beach（谷歌）&lt;/p>
&lt;!--
Logs are an essential aspect of observability and a critical tool for debugging. But Kubernetes logs have traditionally been unstructured strings, making any automated parsing difficult and any downstream processing, analysis, or querying challenging to do reliably.
-->
&lt;p>日志是可观察性的一个重要方面，也是调试的重要工具。 但是Kubernetes日志传统上是非结构化的字符串，因此很难进行自动解析，以及任何可靠的后续处理、分析或查询。&lt;/p>
&lt;!--
In Kubernetes 1.19, we are adding support for structured logs, which natively support (key, value) pairs and object references. We have also updated many logging calls such that over 99% of logging volume in a typical deployment are now migrated to the structured format.
-->
&lt;p>在Kubernetes 1.19中，我们添加结构化日志的支持，该日志本身支持（键，值）对和对象引用。 我们还更新了许多日志记录调用，以便现在将典型部署中超过99％的日志记录量迁移为结构化格式。&lt;/p>
&lt;!--
To maintain backwards compatibility, structured logs will still be outputted as a string where the string contains representations of those "key"="value" pairs. Starting in alpha in 1.19, logs can also be outputted in JSON format using the `--logging-format=json` flag.
-->
&lt;p>为了保持向后兼容性，结构化日志仍将作为字符串输出，其中该字符串包含这些“键” =“值”对的表示。 从1.19的Alpha版本开始，日志也可以使用&lt;code>--logging-format = json&lt;/code>标志以JSON格式输出。&lt;/p>
&lt;h2 id="使用结构化日志">使用结构化日志&lt;/h2>
&lt;!--
We've added two new methods to the klog library: InfoS and ErrorS. For example, this invocation of InfoS:
-->
&lt;p>我们在klog库中添加了两个新方法：InfoS和ErrorS。 例如，InfoS的此调用：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-golang" data-lang="golang">klog.&lt;span style="color:#00a000">InfoS&lt;/span>(&lt;span style="color:#b44">&amp;#34;Pod status updated&amp;#34;&lt;/span>, &lt;span style="color:#b44">&amp;#34;pod&amp;#34;&lt;/span>, klog.&lt;span style="color:#00a000">KObj&lt;/span>(pod), &lt;span style="color:#b44">&amp;#34;status&amp;#34;&lt;/span>, status)
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
will result in this log:
-->
&lt;p>将得到下面的日志输出：&lt;/p>
&lt;pre>&lt;code>I1025 00:15:15.525108 1 controller_utils.go:116] &amp;quot;Pod status updated&amp;quot; pod=&amp;quot;kube-system/kubedns&amp;quot; status=&amp;quot;ready&amp;quot;
&lt;/code>&lt;/pre>&lt;!--
Or, if the --logging-format=json flag is set, it will result in this output:
-->
&lt;p>或者, 如果 --logging-format=json 模式被设置, 将会产生如下结果:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;ts&amp;#34;&lt;/span>: &lt;span style="color:#666">1580306777.04728&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;msg&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;Pod status updated&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;pod&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;coredns&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kube-system&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ready&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
This means downstream logging tools can easily ingest structured logging data and instead of using regular expressions to parse unstructured strings. This also makes processing logs easier, querying logs more robust, and analyzing logs much faster.
-->
&lt;p>这意味着下游日志记录工具可以轻松地获取结构化日志数据，而无需使用正则表达式来解析非结构化字符串。这也使处理日志更容易，查询日志更健壮，并且分析日志更快。&lt;/p>
&lt;!--
With structured logs, all references to Kubernetes objects are structured the same way, so you can filter the output and only log entries referencing the particular pod. You can also find logs indicating how the scheduler was scheduling the pod, how the pod was created, the health probes of the pod, and all other changes in the lifecycle of the pod.
-->
&lt;p>使用结构化日志，所有对Kubernetes对象的引用都以相同的方式进行结构化，因此您可以过滤输出并且仅引用特定Pod的日志条目。您还可以发现指示调度程序如何调度Pod，如何创建Pod，监测Pod的运行状况以及Pod生命周期中的所有其他更改的日志。&lt;/p>
&lt;!--
Suppose you are debugging an issue with a pod. With structured logs, you can filter to only those log entries referencing the pod of interest, rather than needing to scan through potentially thousands of log lines to find the relevant ones.
-->
&lt;p>假设您正在调试Pod的问题。使用结构化日志，您可以只过滤查看感兴趣的Pod的日志条目，而无需扫描可能成千上万条日志行以找到相关的日志行。&lt;/p>
&lt;!--
Not only are structured logs more useful when manual debugging of issues, they also enable richer features like automated pattern recognition within logs or tighter correlation of log and trace data.
-->
&lt;p>结构化日志不仅在手动调试问题时更有用，而且还启用了更丰富的功能，例如日志的自动模式识别或日志和所跟踪数据的更紧密关联性（分析）。&lt;/p>
&lt;!--
Finally, structured logs can help reduce storage costs for logs because most storage systems are more efficiently able to compress structured key=value data than unstructured strings.
-->
&lt;p>最后，结构化日志可以帮助降低日志的存储成本，因为大多数存储系统比非结构化字符串更有效地压缩结构化键值数据。&lt;/p>
&lt;h2 id="参与其中">参与其中&lt;/h2>
&lt;!--
While we have updated over 99% of the log entries by log volume in a typical deployment, there are still thousands of logs to be updated. Pick a file or directory that you would like to improve and [migrate existing log calls to use structured logs](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md). It's a great and easy way to make your first contribution to Kubernetes!
-->
&lt;p>虽然在典型部署中，我们已按日志量更新了99％以上的日志条目，但仍有数千个日志需要更新。 选择一个您要改进的文件或目录，然后[迁移现有的日志调用以使用结构化日志]（https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md）。这是对Kubernetes做出第一笔贡献的好方法!&lt;/p></description></item><item><title>Blog: Kubernetes 1.18: Fit &amp; Finish</title><link>https://kubernetes.io/zh/blog/2020/03/25/kubernetes-1-18-release-announcement/</link><pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/03/25/kubernetes-1-18-release-announcement/</guid><description>
&lt;!--
**Authors:** [Kubernetes 1.18 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md)
-->
&lt;p>&lt;strong>作者:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">Kubernetes 1.18 发布团队&lt;/a>&lt;/p>
&lt;!--
We're pleased to announce the delivery of Kubernetes 1.18, our first release of 2020! Kubernetes 1.18 consists of 38 enhancements: 15 enhancements are moving to stable, 11 enhancements in beta, and 12 enhancements in alpha.
-->
&lt;p>我们很高兴宣布 Kubernetes 1.18 版本的交付，这是我们 2020 年的第一版！ Kubernetes 1.18 包含 38 个增强功能：15 项增强功能已转为稳定版，11 项增强功能处于 beta 阶段，12 项增强功能处于 alpha 阶段。&lt;/p>
&lt;!--
Kubernetes 1.18 is a "fit and finish" release. Significant work has gone into improving beta and stable features to ensure users have a better experience. An equal effort has gone into adding new developments and exciting new features that promise to enhance the user experience even more.
-->
&lt;p>Kubernetes 1.18 是一个近乎 “完美” 的版本。 为了改善 beta 和稳定的特性，已进行了大量工作，以确保用户获得更好的体验。 我们在增强现有功能的同时也增加了令人兴奋的新特性，这些有望进一步增强用户体验。&lt;/p>
&lt;!--
Having almost as many enhancements in alpha, beta, and stable is a great achievement. It shows the tremendous effort made by the community on improving the reliability of Kubernetes as well as continuing to expand its existing functionality.
-->
&lt;p>对 alpha，beta 和稳定版进行几乎同等程度的增强是一项伟大的成就。 它展现了社区在提高 Kubernetes 的可靠性以及继续扩展其现有功能方面所做的巨大努力。&lt;/p>
&lt;!--
## Major Themes
-->
&lt;h2 id="主要内容">主要内容&lt;/h2>
&lt;!--
### Kubernetes Topology Manager Moves to Beta - Align Up!
-->
&lt;h3 id="kubernetes-拓扑管理器-topology-manager-进入-beta-阶段-对齐">Kubernetes 拓扑管理器（Topology Manager）进入 Beta 阶段 - 对齐！&lt;/h3>
&lt;!--
A beta feature of Kubernetes in release 1.18, the [Topology Manager feature](https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md) enables NUMA alignment of CPU and devices (such as SR-IOV VFs) that will allow your workload to run in an environment optimized for low-latency. Prior to the introduction of the Topology Manager, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications.
-->
&lt;p>Kubernetes 在 1.18 版中的 Beta 阶段功能 &lt;a href="https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md">拓扑管理器特性&lt;/a> 启用 CPU 和设备（例如 SR-IOV VF）的 NUMA 对齐，这将使您的工作负载在针对低延迟而优化的环境中运行。在引入拓扑管理器之前，CPU 和设备管理器将做出彼此独立的资源分配决策。 这可能会导致在多处理器系统上非预期的资源分配结果，从而导致对延迟敏感的应用程序的性能下降。&lt;/p>
&lt;!--
### Serverside Apply Introduces Beta 2
-->
&lt;h3 id="serverside-apply-推出beta-2">Serverside Apply 推出Beta 2&lt;/h3>
&lt;!--
Server-side Apply was promoted to Beta in 1.16, but is now introducing a second Beta in 1.18. This new version will track and manage changes to fields of all new Kubernetes objects, allowing you to know what changed your resources and when.
-->
&lt;p>Serverside Apply 在1.16 中进入 Beta 阶段，但现在在 1.18 中进入了第二个 Beta 阶段。 这个新版本将跟踪和管理所有新 Kubernetes 对象的字段更改，从而使您知道什么更改了资源以及何时发生了更改。&lt;/p>
&lt;!--
### Extending Ingress with and replacing a deprecated annotation with IngressClass
-->
&lt;h3 id="使用-ingressclass-扩展-ingress-并用-ingressclass-替换已弃用的注释">使用 IngressClass 扩展 Ingress 并用 IngressClass 替换已弃用的注释&lt;/h3>
&lt;!--
In Kubernetes 1.18, there are two significant additions to Ingress: A new `pathType` field and a new `IngressClass` resource. The `pathType` field allows specifying how paths should be matched. In addition to the default `ImplementationSpecific` type, there are new `Exact` and `Prefix` path types.
-->
&lt;p>在 Kubernetes 1.18 中，Ingress 有两个重要的补充：一个新的 &lt;code>pathType&lt;/code> 字段和一个新的 &lt;code>IngressClass&lt;/code> 资源。&lt;code>pathType&lt;/code> 字段允许指定路径的匹配方式。 除了默认的&lt;code>ImplementationSpecific&lt;/code>类型外，还有新的 &lt;code>Exact&lt;/code>和&lt;code>Prefix&lt;/code> 路径类型。&lt;/p>
&lt;!--
The `IngressClass` resource is used to describe a type of Ingress within a Kubernetes cluster. Ingresses can specify the class they are associated with by using a new `ingressClassName` field on Ingresses. This new resource and field replace the deprecated `kubernetes.io/ingress.class` annotation.
-->
&lt;p>&lt;code>IngressClass&lt;/code> 资源用于描述 Kubernetes 集群中 Ingress 的类型。 Ingress 对象可以通过在Ingress 资源类型上使用新的&lt;code>ingressClassName&lt;/code> 字段来指定与它们关联的类。 这个新的资源和字段替换了不再建议使用的 &lt;code>kubernetes.io/ingress.class&lt;/code> 注解。&lt;/p>
&lt;!--
### SIG-CLI introduces kubectl alpha debug
-->
&lt;h3 id="sig-cli-引入了-kubectl-alpha-debug">SIG-CLI 引入了 kubectl alpha debug&lt;/h3>
&lt;!--
SIG-CLI was debating the need for a debug utility for quite some time already. With the development of [ephemeral containers](https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/), it became more obvious how we can support developers with tooling built on top of `kubectl exec`. The addition of the [`kubectl alpha debug` command](https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md) (it is alpha but your feedback is more than welcome), allows developers to easily debug their Pods inside the cluster. We think this addition is invaluable. This command allows one to create a temporary container which runs next to the Pod one is trying to examine, but also attaches to the console for interactive troubleshooting.
-->
&lt;p>SIG-CLI 一直在争论着调试工具的必要性。随着 &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">临时容器&lt;/a> 的发展，我们如何使用基于 &lt;code>kubectl exec&lt;/code> 的工具来支持开发人员的必要性变得越来越明显。 &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md">&lt;code>kubectl alpha debug&lt;/code> 命令&lt;/a> 的增加，（由于是 alpha 阶段，非常欢迎您反馈意见），使开发人员可以轻松地在集群中调试 Pod。我们认为这个功能的价值非常高。 此命令允许创建一个临时容器，该容器在要尝试检查的 Pod 旁边运行，并且还附加到控制台以进行交互式故障排除。&lt;/p>
&lt;!--
### Introducing Windows CSI support alpha for Kubernetes
-->
&lt;h3 id="为-kubernetes-引入-windows-csi-支持-alpha">为 Kubernetes 引入 Windows CSI 支持（Alpha）&lt;/h3>
&lt;!--
The alpha version of CSI Proxy for Windows is being released with Kubernetes 1.18. CSI proxy enables CSI Drivers on Windows by allowing containers in Windows to perform privileged storage operations.
-->
&lt;p>用于 Windows 的 CSI 代理的 Alpha 版本随 Kubernetes 1.18 一起发布。 CSI 代理通过允许Windows 中的容器执行特权存储操作来启用 Windows 上的 CSI 驱动程序。&lt;/p>
&lt;!--
## Other Updates
-->
&lt;h2 id="其它更新">其它更新&lt;/h2>
&lt;!--
### Graduated to Stable 💯
-->
&lt;h3 id="毕业转为稳定版">毕业转为稳定版&lt;/h3>
&lt;!--
- [Taint Based Eviction](https://github.com/kubernetes/enhancements/issues/166)
- [`kubectl diff`](https://github.com/kubernetes/enhancements/issues/491)
- [CSI Block storage support](https://github.com/kubernetes/enhancements/issues/565)
- [API Server dry run](https://github.com/kubernetes/enhancements/issues/576)
- [Pass Pod information in CSI calls](https://github.com/kubernetes/enhancements/issues/603)
- [Support Out-of-Tree vSphere Cloud Provider](https://github.com/kubernetes/enhancements/issues/670)
- [Support GMSA for Windows workloads](https://github.com/kubernetes/enhancements/issues/689)
- [Skip attach for non-attachable CSI volumes](https://github.com/kubernetes/enhancements/issues/770)
- [PVC cloning](https://github.com/kubernetes/enhancements/issues/989)
- [Moving kubectl package code to staging](https://github.com/kubernetes/enhancements/issues/1020)
- [RunAsUserName for Windows](https://github.com/kubernetes/enhancements/issues/1043)
- [AppProtocol for Services and Endpoints](https://github.com/kubernetes/enhancements/issues/1507)
- [Extending Hugepage Feature](https://github.com/kubernetes/enhancements/issues/1539)
- [client-go signature refactor to standardize options and context handling](https://github.com/kubernetes/enhancements/issues/1601)
- [Node-local DNS cache](https://github.com/kubernetes/enhancements/issues/1024)
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/166">基于污点的逐出操作&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/491">&lt;code>kubectl diff&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/565">CSI 块存储支持&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/576">API 服务器 dry run&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/603">在 CSI 调用中传递 Pod 信息&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/670">支持树外 vSphere 云驱动&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/689">对 Windows 负载支持 GMSA&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/770">对不可挂载的CSI卷跳过挂载&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/989">PVC 克隆&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">移动 kubectl 包代码到 staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1043">Windows 的 RunAsUserName&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">服务和端点的 AppProtocol&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">扩展 Hugepage 特性&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go signature refactor to standardize options and context handling&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1024">Node-local DNS cache&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### Major Changes
-->
&lt;h3 id="主要变化">主要变化&lt;/h3>
&lt;!--
- [EndpointSlice API](https://github.com/kubernetes/enhancements/issues/752)
- [Moving kubectl package code to staging](https://github.com/kubernetes/enhancements/issues/1020)
- [CertificateSigningRequest API](https://github.com/kubernetes/enhancements/issues/1513)
- [Extending Hugepage Feature](https://github.com/kubernetes/enhancements/issues/1539)
- [client-go signature refactor to standardize options and context handling](https://github.com/kubernetes/enhancements/issues/1601)
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/752">EndpointSlice API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">Moving kubectl package code to staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">Extending Hugepage Feature&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go 的调用规范重构来标准化选项和管理上下文&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### Release Notes
-->
&lt;h3 id="发布说明">发布说明&lt;/h3>
&lt;!--
Check out the full details of the Kubernetes 1.18 release in our [release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md).
-->
&lt;p>在我们的 &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md">发布文档&lt;/a>中查看 Kubernetes 1.18 发行版的完整详细信息。&lt;/p>
&lt;!--
### Availability
-->
&lt;h3 id="下载安装">下载安装&lt;/h3>
&lt;!--
Kubernetes 1.18 is available for download on [GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.18.0). To get started with Kubernetes, check out these [interactive tutorials](https://kubernetes.io/docs/tutorials/) or run local Kubernetes clusters using Docker container “nodes” with [kind](https://kind.sigs.k8s.io/). You can also easily install 1.18 using [kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/).
-->
&lt;p>Kubernetes 1.18 可以在 &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.18.0">GitHub&lt;/a> 上下载。 要开始使用Kubernetes，请查看这些 &lt;a href="https://kubernetes.io/docs/tutorials/">交互教程&lt;/a> 或通过&lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> 使用 Docker 容器运行本地 kubernetes 集群。您还可以使用&lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>轻松安装 1.18。&lt;/p>
&lt;!--
### Release Team
-->
&lt;h3 id="发布团队">发布团队&lt;/h3>
&lt;!--
This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the [release team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md) led by Jorge Alarcon Ochoa, Site Reliability Engineer at Searchable AI. The 34 release team members coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.
-->
&lt;p>通过数百位贡献了技术和非技术内容的个人的努力，使本次发行成为可能。 特别感谢由 Searchable AI 的网站可靠性工程师 Jorge Alarcon Ochoa 领导的&lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">发布团队&lt;/a>。 34 位发布团队成员协调了发布的各个方面，从文档到测试、验证和功能完整性。&lt;/p>
&lt;!--
As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over [40,000 individual contributors](https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1) to date and an active community of more than 3,000 people.
-->
&lt;p>随着 Kubernetes 社区的发展壮大，我们的发布过程很好地展示了开源软件开发中的协作。 Kubernetes 继续快速获取新用户。 这种增长创造了一个积极的反馈回路，其中有更多的贡献者提交了代码，从而创建了更加活跃的生态系统。 迄今为止，Kubernetes 已有 &lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">40,000 独立贡献者&lt;/a> 和一个超过3000人的活跃社区。&lt;/p>
&lt;!--
### Release Logo
-->
&lt;h3 id="发布-logo">发布 logo&lt;/h3>
&lt;!--
![Kubernetes 1.18 Release Logo](/images/blog/2020-03-25-kubernetes-1.18-release-announcement/release-logo.png)
-->
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/release-logo.png" alt="Kubernetes 1.18 发布图标">&lt;/p>
&lt;!--
#### Why the LHC?
-->
&lt;h4 id="为什么是-lhc">为什么是 LHC&lt;/h4>
&lt;!--
The LHC is the world’s largest and most powerful particle accelerator. It is the result of the collaboration of thousands of scientists from around the world, all for the advancement of science. In a similar manner, Kubernetes has been a project that has united thousands of contributors from hundreds of organizations – all to work towards the same goal of improving cloud computing in all aspects! "A Bit Quarky" as the release name is meant to remind us that unconventional ideas can bring about great change and keeping an open mind to diversity will lead help us innovate.
-->
&lt;p>LHC 是世界上最大，功能最强大的粒子加速器。它是由来自世界各地成千上万科学家合作的结果，所有这些合作都是为了促进科学的发展。以类似的方式，Kubernetes 已经成为一个聚集了来自数百个组织的数千名贡献者–所有人都朝着在各个方面改善云计算的相同目标努力的项目！ 发布名称“ A Bit Quarky” 的意思是提醒我们，非常规的想法可以带来巨大的变化，对开放性保持开放态度将有助于我们进行创新。&lt;/p>
&lt;!--
#### About the designer
-->
&lt;h4 id="关于设计者">关于设计者&lt;/h4>
&lt;!--
Maru Lango is a designer currently based in Mexico City. While her area of expertise is Product Design, she also enjoys branding, illustration and visual experiments using CSS + JS and contributing to diversity efforts within the tech and design communities. You may find her in most social media as @marulango or check her website: https://marulango.com
-->
&lt;p>Maru Lango 是目前居住在墨西哥城的设计师。她的专长是产品设计，她还喜欢使用 CSS + JS 进行品牌、插图和视觉实验，为技术和设计社区的多样性做贡献。您可能会在大多数社交媒体上以 @marulango 的身份找到她，或查看她的网站： &lt;a href="https://marulango.com">https://marulango.com&lt;/a>&lt;/p>
&lt;!--
### User Highlights
-->
&lt;h3 id="高光用户">高光用户&lt;/h3>
&lt;!--
- Ericsson is using Kubernetes and other cloud native technology to deliver a [highly demanding 5G network](https://www.cncf.io/case-study/ericsson/) that resulted in up to 90 percent CI/CD savings.
- Zendesk is using Kubernetes to [run around 70% of its existing applications](https://www.cncf.io/case-study/zendesk/). It’s also building all new applications to also run on Kubernetes, which has brought time savings, greater flexibility, and increased velocity to its application development.
- LifeMiles has [reduced infrastructure spending by 50%](https://www.cncf.io/case-study/lifemiles/) because of its move to Kubernetes. It has also allowed them to double its available resource capacity.
-->
&lt;ul>
&lt;li>爱立信正在使用 Kubernetes 和其他云原生技术来交付&lt;a href="https://www.cncf.io/case-study/ericsson/">高标准的 5G 网络&lt;/a>，这可以在 CI/CD 上节省多达 90％ 的支出。&lt;/li>
&lt;li>Zendesk 正在使用 Kubernetes &lt;a href="https://www.cncf.io/case-study/zendesk/">运行其现有应用程序的约 70％&lt;/a>。它还正在使所构建的所有新应用都可以在 Kubernetes 上运行，从而节省时间、提高灵活性并加快其应用程序开发的速度。&lt;/li>
&lt;li>LifeMiles 因迁移到 Kubernetes 而&lt;a href="https://www.cncf.io/case-study/lifemiles/">降低了 50% 的基础设施开支&lt;/a>。Kubernetes 还使他们可以将其可用资源容量增加一倍。&lt;/li>
&lt;/ul>
&lt;!--
### Ecosystem Updates
-->
&lt;h3 id="生态系统更新">生态系统更新&lt;/h3>
&lt;!--
- The CNCF published the results of its [annual survey](https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/) showing that Kubernetes usage in production is skyrocketing. The survey found that 78% of respondents are using Kubernetes in production compared to 58% last year.
- The “Introduction to Kubernetes” course hosted by the CNCF [surpassed 100,000 registrations](https://www.cncf.io/announcement/2020/01/28/cloud-native-computing-foundation-announces-introduction-to-kubernetes-course-surpasses-100000-registrations/).
-->
&lt;ul>
&lt;li>CNCF发布了&lt;a href="https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/">年度调查&lt;/a> 的结果，表明 Kubernetes 在生产中的使用正在飞速增长。调查发现，有78％的受访者在生产中使用Kubernetes，而去年这一比例为 58％。&lt;/li>
&lt;li>CNCF 举办的 “Kubernetes入门” 课程有&lt;a href="https://www.cncf.io/announcement/2020/01/28/cloud-native-computing-foundation-announces-introduction-to-kubernetes-course-surpasses-100000-registrations/">超过 100,000 人注册&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
### Project Velocity
-->
&lt;h3 id="项目速度">项目速度&lt;/h3>
&lt;!--
The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. [K8s DevStats](https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1) illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times.
-->
&lt;p>CNCF 继续完善 DevStats。这是一个雄心勃勃的项目，旨在对项目中的无数贡献数据进行可视化展示。&lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1">K8s DevStats&lt;/a> 展示了主要公司贡献者的贡献细目，以及一系列令人印象深刻的预定义的报告，涉及从贡献者个人的各方面到 PR 生命周期的各个方面。&lt;/p>
&lt;!--
This past quarter, 641 different companies and over 6,409 individuals contributed to Kubernetes. [Check out DevStats](https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;var-period=m&amp;var-repogroup_name=All) to learn more about the overall velocity of the Kubernetes project and community.
-->
&lt;p>在过去的一个季度中，641 家不同的公司和超过 6,409 个个人为 Kubernetes 作出贡献。 &lt;a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All">查看 DevStats&lt;/a> 以了解有关 Kubernetes 项目和社区发展速度的信息。&lt;/p>
&lt;!--
### Event Update
-->
&lt;h3 id="活动信息">活动信息&lt;/h3>
&lt;!--
Kubecon + CloudNativeCon EU 2020 is being pushed back – for the more most up-to-date information, please check the [Novel Coronavirus Update page](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/).
-->
&lt;p>Kubecon + CloudNativeCon EU 2020 已经推迟 - 有关最新信息，请查看&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/">新型肺炎发布页面&lt;/a>。&lt;/p>
&lt;!--
### Upcoming Release Webinar
-->
&lt;h3 id="即将到来的发布的线上会议">即将到来的发布的线上会议&lt;/h3>
&lt;!--
Join members of the Kubernetes 1.18 release team on April 23rd, 2020 to learn about the major features in this release including kubectl debug, Topography Manager, Ingress to V1 graduation, and client-go. Register here: https://www.cncf.io/webinars/kubernetes-1-18/.
-->
&lt;p>在 2020 年 4 月 23 日，和 Kubernetes 1.18 版本团队一起了解此版本的主要功能，包括 kubectl debug、拓扑管理器、Ingress 毕业为 V1 版本以及 client-go。 在此处注册：https://www.cncf.io/webinars/kubernetes-1-18/ 。&lt;/p>
&lt;!--
### Get Involved
-->
&lt;h3 id="如何参与">如何参与&lt;/h3>
&lt;!--
The simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.
-->
&lt;p>参与 Kubernetes 的最简单方法是加入众多与您的兴趣相关的 &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">特别兴趣小组&lt;/a> (SIGs) 之一。 您有什么想向 Kubernetes 社区发布的内容吗？ 参与我们的每周 &lt;a href="https://github.com/kubernetes/community/tree/master/communication">社区会议&lt;/a>，并通过以下渠道分享您的声音。 感谢您一直以来的反馈和支持。&lt;/p>
&lt;!--
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
- Join the community on [Slack](http://slack.k8s.io/)
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
- Read more about what’s happening with Kubernetes on the [blog](https://kubernetes.io/blog/)
- Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team)
-->
&lt;ul>
&lt;li>在 Twitter 上关注我们 &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a>，了解最新动态&lt;/li>
&lt;li>在 &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a> 上参与社区讨论&lt;/li>
&lt;li>加入 &lt;a href="http://slack.k8s.io/">Slack&lt;/a> 上的社区&lt;/li>
&lt;li>在&lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>提问（或回答）&lt;/li>
&lt;li>分享您的 Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">故事&lt;/a>&lt;/li>
&lt;li>通过 &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>了解更多关于 Kubernetes 的新鲜事&lt;/li>
&lt;li>了解更多关于 &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes 发布团队&lt;/a> 的信息&lt;/li>
&lt;/ul></description></item><item><title>Blog: 基于 MIPS 架构的 Kubernetes 方案</title><link>https://kubernetes.io/zh/blog/2020/01/15/kubernetes-on-mips/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/01/15/kubernetes-on-mips/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes on MIPS"
date: 2020-01-15
slug: Kubernetes-on-MIPS
-->
&lt;!--
**Authors:** TimYin Shi, Dominic Yin, Wang Zhan, Jessica Jiang, Will Cai, Jeffrey Gao, Simon Sun (Inspur)
-->
&lt;p>&lt;strong>作者:&lt;/strong> 石光银，尹东超，展望，江燕，蔡卫卫，高传集，孙思清（浪潮）&lt;/p>
&lt;!--
## Background
-->
&lt;h2 id="背景">背景&lt;/h2>
&lt;!--
[MIPS](https://en.wikipedia.org/wiki/MIPS_architecture) (Microprocessor without Interlocked Pipelined Stages) is a reduced instruction set computer (RISC) instruction set architecture (ISA), appeared in 1981 and developed by MIPS Technologies. Now MIPS architecture is widely used in many electronic products.
-->
&lt;p>&lt;a href="https://zh.wikipedia.org/wiki/MIPS%E6%9E%B6%E6%A7%8B">MIPS&lt;/a> (Microprocessor without Interlocked Pipelined Stages) 是一种采取精简指令集（RISC）的处理器架构 (ISA)，出现于 1981 年，由 MIPS 科技公司开发。如今 MIPS 架构被广泛应用于许多电子产品上。&lt;/p>
&lt;!--
[Kubernetes](https://kubernetes.io) has officially supported a variety of CPU architectures such as x86, arm/arm64, ppc64le, s390x. However, it's a pity that Kubernetes doesn't support MIPS. With the widespread use of cloud native technology, users under MIPS architecture also have an urgent demand for Kubernetes on MIPS.
-->
&lt;p>&lt;a href="https://kubernetes.io">Kubernetes&lt;/a> 官方目前支持众多 CPU 架构诸如 x86, arm/arm64, ppc64le, s390x 等。然而目前还不支持 MIPS 架构，始终是一个遗憾。随着云原生技术的广泛应用，MIPS 架构下的用户始终对 Kubernetes on MIPS 有着迫切的需求。&lt;/p>
&lt;!--
## Achievements
-->
&lt;h2 id="成果">成果&lt;/h2>
&lt;!--
For many years, to enrich the ecology of the open-source community, we have been working on adjusting MIPS architecture for Kubernetes use cases. With the continuous iterative optimization and the performance improvement of the MIPS CPU, we have made some breakthrough progresses on the mips64el platform.
-->
&lt;p>多年来，为了丰富开源社区的生态，我们一直致力于在 MIPS 架构下适配 Kubernetes。随着 MIPS CPU 的不断迭代优化和性能的提升，我们在 mips64el 平台上取得了一些突破性的进展。&lt;/p>
&lt;!--
Over the years, we have been actively participating in the Kubernetes community and have rich experience in the using and optimization of Kubernetes technology. Recently, we tried to adapt the MIPS architecture platform for Kubernetes and achieved a new a stage on that journey. The team has completed migration and adaptation of Kubernetes and related components, built not only a stable and highly available MIPS cluster but also completed the conformance test for Kubernetes v1.16.2.
-->
&lt;p>多年来，我们一直积极投入 Kubernetes 社区，在 Kubernetes 技术应用和优化方面具备了丰富的经验。最近，我们在研发过程中尝试将 Kubernetes 适配到 MIPS 架构平台，并取得了阶段性成果。成功完成了 Kubernetes 以及相关组件的迁移适配，不仅搭建出稳定高可用的 MIPS 集群，同时完成了 Kubernetes v1.16.2 版本的一致性测试。&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/kubernetes-on-mips.png" alt="Kubernetes on MIPS">&lt;/p>
&lt;!--
_Figure 1 Kubernetes on MIPS_
-->
&lt;p>&lt;em>图一 Kubernetes on MIPS&lt;/em>&lt;/p>
&lt;!--
## K8S-MIPS component build
-->
&lt;h2 id="k8s-mips-组件构建">K8S-MIPS 组件构建&lt;/h2>
&lt;!--
Almost all native cloud components related to Kubernetes do not provide a MIPS version installation package or image. The prerequisite of deploying Kubernetes on the MIPS platform is to compile and build all required components on the mips64el platform. These components include:
-->
&lt;p>几乎所有的 Kubernetes 相关的云原生组件都没有提供 MIPS 版本的安装包或镜像，在 MIPS 平台上部署 Kubernetes 的前提是自行编译构建出全部所需组件。这些组件主要包括：&lt;/p>
&lt;ul>
&lt;li>golang&lt;/li>
&lt;li>docker-ce&lt;/li>
&lt;li>hyperkube&lt;/li>
&lt;li>pause&lt;/li>
&lt;li>etcd&lt;/li>
&lt;li>calico&lt;/li>
&lt;li>coredns&lt;/li>
&lt;li>metrics-server&lt;/li>
&lt;/ul>
&lt;!--
Thanks to the excellent design of Golang and its good support for the MIPS platform, the compilation processes of the above cloud native components are greatly simplified. First of all, we compiled Golang on the latest stable version for the mips64el platform, and then we compiled most of the above components with source code.
-->
&lt;p>得益于 Golang 优秀的设计以及对于 MIPS 平台的良好支持，极大地简化了上述云原生组件的编译过程。首先，我们在 mips64el 平台编译出了最新稳定的 golang, 然后通过源码构建的方式编译完成了上述大部分组件。&lt;/p>
&lt;!--
During the compilation processes, we inevitably encountered many platform compatibility problems, such as a Golang system call compatibility problem (syscall), typecasting of syscall. Stat_t from uint32 to uint64, patching for EpollEvent, and so on.
-->
&lt;p>在编译过程中，我们不可避免地遇到了很多平台兼容性的问题，比如关于 golang 系统调用 (syscall) 的兼容性问题, syscall.Stat_t 32 位 与 64 位类型转换，EpollEvent 修正位缺失等等。&lt;/p>
&lt;!--
To build K8S-MIPS components, we used cross-compilation technology. Our process involved integrating a QEMU tool to translate MIPS CPU instructions and modifying the build script of Kubernetes and E2E image script of Kubernetes, Hyperkube, and E2E test images on MIPS architecture.
-->
&lt;p>构建 K8S-MIPS 组件主要使用了交叉编译技术。构建过程包括集成 QEMU 工具来实现 MIPS CPU 指令的转换。同时修改 Kubernetes 和 E2E 镜像的构建脚本，构建了 Hyperkube 和 MIPS 架构的 E2E 测试镜像。&lt;/p>
&lt;!--
After successfully building the above components, we use tools such as kubespray and kubeadm to complete kubernetes cluster construction.
-->
&lt;p>成功构建出以上组件后，我们使用工具完成 Kubernetes 集群的搭建，比如 kubespray、kubeadm 等。&lt;/p>
&lt;!--
| Name | Version | MIPS Repository |
|--------------------------------|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| golang on MIPS | 1.12.5 | - |
| docker-ce on MIPS | 18.09.8 | - |
| metrics-server for CKE on MIPS | 0.3.2 | `registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2` |
| etcd for CKE on MIPS | 3.2.26 | `registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26` |
| pause for CKE on MIPS | 3.1 | `registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1` |
| hyperkube for CKE on MIPS | 1.14.3 | `registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3` |
| coredns for CKE on MIPS | 1.6.5 | `registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5` |
| calico for CKE on MIPS | 3.8.0 | `registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0` |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>名称&lt;/th>
&lt;th>版本&lt;/th>
&lt;th>MIPS 镜像仓库&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MIPS 版本 golang&lt;/td>
&lt;td>1.12.5&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 docker-ce&lt;/td>
&lt;td>18.09.8&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 CKE 构建 metrics-server&lt;/td>
&lt;td>0.3.2&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 CKE 构建 etcd&lt;/td>
&lt;td>3.2.26&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 CKE 构建 pause&lt;/td>
&lt;td>3.1&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 CKE 构建 hyperkube&lt;/td>
&lt;td>1.14.3&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 CKE 构建 coredns&lt;/td>
&lt;td>1.6.5&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS 版本 CKE 构建 calico&lt;/td>
&lt;td>3.8.0&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!--
**Note**: CKE is a Kubernetes-based cloud container engine launched by Inspur
-->
&lt;p>&lt;strong>注&lt;/strong>: CKE 是浪潮推出的一款基于 Kubernetes 的容器云服务引擎&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/k8s-mips-cluster-components.png" alt="K8S-MIPS Cluster Components">&lt;/p>
&lt;!--
_Figure 2 K8S-MIPS Cluster Components_
-->
&lt;p>&lt;em>图二 K8S-MIPS 集群组件&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cpu-architecture.png" alt="CPU Architecture">&lt;/p>
&lt;!--
_Figure 3 CPU Architecture_
-->
&lt;p>&lt;em>图三 CPU 架构&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cluster-node-information.png" alt="Cluster Node Information">&lt;/p>
&lt;!--
_Figure 4 Cluster Node Information_
-->
&lt;p>&lt;em>图四 集群节点信息&lt;/em>&lt;/p>
&lt;!--
## Run K8S Conformance Test
-->
&lt;h2 id="运行-k8s-一致性测试">运行 K8S 一致性测试&lt;/h2>
&lt;!--
The most straightforward way to verify the stability and availability of the K8S-MIPS cluster is to run a Kubernetes [conformance test](https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md).
-->
&lt;p>验证 K8S-MIP 集群稳定性和可用性最简单直接的方式是运行 Kubernetes 的 &lt;a href="https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md">一致性测试&lt;/a>。&lt;/p>
&lt;!--
Conformance is a standalone container to launch Kubernetes end-to-end tests for conformance testing.
-->
&lt;p>一致性测试是一个独立的容器，用于启动 Kubernetes 端到端的一致性测试。&lt;/p>
&lt;!--
Once the test has started, it launches several pods for various end-to-end tests. The source code of those images used by these pods is mostly from `kubernetes/test/images`, and the built images are at `gcr.io/kubernetes-e2e-test-images`. Since there are no MIPS images in the repository, we must first build all needed images to run the test.
-->
&lt;p>当执行一致性测试时，测试程序会启动许多 Pod 进行各种端到端的行为测试，这些 Pod 使用的镜像源码大部分来自于 &lt;code>kubernetes/test/images&lt;/code> 目录下，构建的镜像位于 &lt;code>gcr.io/kubernetes-e2e-test-images/&lt;/code>。由于镜像仓库中目前并不存在 MIPS 架构的镜像，我们要想运行 E2E 测试，必须首先构建出测试所需的全部镜像。&lt;/p>
&lt;!--
### Build needed images for test
-->
&lt;h3 id="构建测试所需镜像">构建测试所需镜像&lt;/h3>
&lt;!--
The first step is to find all needed images for the test. We can run `sonobuoy images-p e2e` command to list all images, or we can find those images in [/test/utils/image/manifest.go](https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go). Although Kubernetes officially has a complete Makefile and shell-script that provides commands for building test images, there are still a number of architecture-related issues that have not been resolved, such as the incompatibilities of base images and dependencies. So we cannot directly build mips64el architecture images by executing these commands.
-->
&lt;p>第一步是找到测试所需的所有镜像。我们可以执行 &lt;code>sonobuoy images-p e2e&lt;/code> 命令来列出所有镜像，或者我们可以在 &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go">/test/utils/image/manifest.go&lt;/a> 中找到这些镜像。尽管 Kubernetes 官方提供了完整的 Makefile 和 shell 脚本，为构建测试映像提供了命令，但是仍然有许多与体系结构相关的问题未能解决，比如基础映像和依赖包的不兼容问题。因此，我们无法通过直接执行这些构建命令来制作 mips64el 架构镜像。&lt;/p>
&lt;!--
Most test images are in golang, then compiled into binaries and built as Docker image based on the corresponding Dockerfile. These images are easy to build. But note that most images are using alpine as their base image, which does not officially support mips64el architecture for now. For this moment, we are unable to make mips64el version of [alpine](https://www.alpinelinux.org/), so we have to replace the alpine to existing MIPS images, such as Debian-stretch, fedora, ubuntu. Replacing the base image also requires replacing the command to install the dependencies, even the version of these dependencies.
-->
&lt;p>多数测试镜像都是使用 golang 编写，然后编译出二进制文件，并基于相应的 Dockerfile 制作出镜像。这些镜像对我们来说可以轻松地制作出来。但是需要注意一点：测试镜像默认使用的基础镜像大多是 alpine, 目前 &lt;a href="https://www.alpinelinux.org/">Alpine&lt;/a> 官方并不支持 mips64el 架构，我们暂时未能自己制作出 mips64el 版本的 alpine 础镜像，只能将基础镜像替换为我们目前已有的 mips64el 基础镜像，比如 debian-stretch,fedora, ubuntu 等。替换基础镜像的同时也需要替换安装依赖包的命令，甚至依赖包的版本等。&lt;/p>
&lt;!--
Some images are not in `kubernetes/test/images`, such as `gcr.io/google-samples/gb-frontend:v6`. There is no clear documentation explaining where these images are locaated, though we found the source code in repository [github.com/GoogleCloudPlatform/kubernetes-engine-samples](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples). We soon ran into new problems: to build these google sample images, we have to build the base image it uses, even the base image of the base images, such as `php:5-apache`, `redis`, and `perl`.
-->
&lt;p>有些测试所需镜像的源码并不在 &lt;code>kubernetes/test/images&lt;/code> 下,比如 &lt;code>gcr.io/google-samples/gb-frontend:v6&lt;/code> 等，没有明确的文档说明这类镜像来自于何方，最终还是在 &lt;a href="github.com/GoogleCloudPlatform/kubernetes-engine-samples">github.com/GoogleCloudPlatform/kubernetes-engine-samples&lt;/a> 这个仓库找到了原始的镜像源代码。但是很快我们遇到了新的问题，为了制作这些镜像，还要制作它依赖的基础镜像，甚至基础镜像的基础镜像，比如 &lt;code>php:5-apache&lt;/code>、&lt;code>redis&lt;/code>、&lt;code>perl&lt;/code> 等等。&lt;/p>
&lt;!--
After a long process of building an image, we finished with about four dozen images, including the images used by the test pod, and the base images. The last step before we run the tests is to place all those images into every node in the cluster and make sure the Pod image pull policy is `imagePullPolicy: ifNotPresent`.
-->
&lt;p>经过漫长庞杂的的镜像重制工作，我们完成了总计约 40 个镜像的制作 ，包括测试镜像以及直接和间接依赖的基础镜像。
最终我们将所有镜像在集群内准备妥当，并确保测试用例内所有 Pod 的镜像拉取策略设置为 &lt;code>imagePullPolicy: ifNotPresent&lt;/code>。&lt;/p>
&lt;!--
Here are some of the images we built
-->
&lt;p>这是我们构建出的部分镜像列表：&lt;/p>
&lt;ul>
&lt;li>&lt;code>docker.io/library/busybox:1.29&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/nginx:1.14-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/nginx:1.15-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/perl:5.26&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/httpd:2.4.38-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/redis:5.0.5-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-containers/conformance:v1.16.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-containers/hyperkube:v1.16.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-samples/gb-frontend:v6&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/agnhost:2.6&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/dnsutils:1.1&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/echoserver:2.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/kitten:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/mounttest:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nautilus:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nonroot:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/resource-consumer-controller:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.10&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/test-webserver:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1&lt;/code>&lt;/li>
&lt;li>&lt;code>k8s.gcr.io/etcd:3.3.15&lt;/code>&lt;/li>
&lt;li>&lt;code>k8s.gcr.io/pause:3.1&lt;/code>&lt;/li>
&lt;/ul>
&lt;!--
Finally, we ran the tests and got the test result, include `e2e.log`, which showed that all test cases passed. Additionally, we submitted our test result to [k8s-conformance](https://github.com/cncf/k8s-conformance) as a [pull request](https://github.com/cncf/k8s-conformance/pull/779).
-->
&lt;p>最终我们执行一致性测试并且得到了测试报告，包括 &lt;code>e2e.log&lt;/code>，显示我们通过了全部的测试用例。此外，我们将测试结果以 &lt;a href="https://github.com/cncf/k8s-conformance/pull/779">pull request&lt;/a> 的形式提交给了 &lt;a href="https://github.com/cncf/k8s-conformance">k8s-conformance&lt;/a> 。&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/pull-request-for-conformance-test-results.png" alt="Pull request for conformance test results">&lt;/p>
&lt;!--
_Figure 5 Pull request for conformance test results_
-->
&lt;p>&lt;em>图五 一致性测试结果的 PR&lt;/em>&lt;/p>
&lt;!--
## What's next
-->
&lt;h2 id="后续计划">后续计划&lt;/h2>
&lt;!--
We built the kubernetes-MIPS component manually and finished the conformance test, which verified the feasibility of Kubernetes On the MIPS platform and greatly enhanced our confidence in promoting the support of the MIPS architecture by Kubernetes.
-->
&lt;p>我们手动构建了 K8S-MIPS 组件以及执行了 E2E 测试，验证了 Kubernetes on MIPS 的可行性，极大的增强了我们对于推进 Kubernetes 支持 MIPS 架构的信心。&lt;/p>
&lt;!--
In the future, we plan to actively contribute our experience and achievements to the community, submit PR, and patch for MIPS. We hope that more developers and companies in the community join us and promote Kubernetes on MIPS.
-->
&lt;p>后续，我们将积极地向社区贡献我们的工作经验以及成果，提交 PR 以及 Patch For MIPS 等， 希望能够有更多的来自社区的力量加入进来，共同推进 Kubernetes for MIPS 的进程。&lt;/p>
&lt;!--
Contribution plan：
-->
&lt;p>后续开源贡献计划：&lt;/p>
&lt;!--
- contribute the source of e2e test images for MIPS
- contribute the source of hyperkube for MIPS
- contribute the source of deploy tools like kubeadm for MIPS
-->
&lt;ul>
&lt;li>贡献构建 E2E 测试镜像代码&lt;/li>
&lt;li>贡献构建 MIPS 版本 hyperkube 代码&lt;/li>
&lt;li>贡献构建 MIPS 版本 kubeadm 等集群部署工具&lt;/li>
&lt;/ul>
&lt;hr></description></item><item><title>Blog: Kubernetes 1.17：稳定</title><link>https://kubernetes.io/zh/blog/2019/12/09/kubernetes-1-17-release-announcement/</link><pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/12/09/kubernetes-1-17-release-announcement/</guid><description>
&lt;!-- ---
layout: blog
title: "Kubernetes 1.17: Stability"
date: 2019-12-09T13:00:00-08:00
slug: kubernetes-1-17-release-announcement
--- -->
&lt;p>&lt;strong>作者:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md">Kubernetes 1.17发布团队&lt;/a>&lt;/p>
&lt;!--
**Authors:** [Kubernetes 1.17 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md)
-->
&lt;p>我们高兴的宣布Kubernetes 1.17版本的交付，它是我们2019年的第四个也是最后一个发布版本。Kubernetes v1.17包含22个增强功能：有14个增强已经逐步稳定(stable)，4个增强功能已经进入公开测试版(beta)，4个增强功能刚刚进入内部测试版(alpha)。&lt;/p>
&lt;!--
We’re pleased to announce the delivery of Kubernetes 1.17, our fourth and final release of 2019! Kubernetes v1.17 consists of 22 enhancements: 14 enhancements have graduated to stable, 4 enhancements are moving to beta, and 4 enhancements are entering alpha.
-->
&lt;h2 id="主要的主题">主要的主题&lt;/h2>
&lt;!--
## Major Themes
-->
&lt;h3 id="云服务提供商标签基本可用">云服务提供商标签基本可用&lt;/h3>
&lt;!--
### Cloud Provider Labels reach General Availability
-->
&lt;p>作为公开测试版特性添加到 v1.2 ，v1.17 中可以看到云提供商标签达到基本可用。&lt;/p>
&lt;!--
Added as a beta feature way back in v1.2, v1.17 sees the general availability of cloud provider labels.
-->
&lt;h3 id="卷快照进入公开测试版">卷快照进入公开测试版&lt;/h3>
&lt;!--
### Volume Snapshot Moves to Beta
-->
&lt;p>在 v1.17 中，Kubernetes卷快照特性是公开测试版。这个特性是在 v1.12 中以内部测试版引入的，第二个有重大变化的内部测试版是 v1.13 。&lt;/p>
&lt;!--
The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13.
-->
&lt;h2 id="容器存储接口迁移公开测试版">容器存储接口迁移公开测试版&lt;/h2>
&lt;!--
### CSI Migration Beta
-->
&lt;p>在 v1.17 中，Kubernetes树内存储插件到容器存储接口(CSI)的迁移基础架构是公开测试版。容器存储接口迁移最初是在Kubernetes v1.14 中以内部测试版引入的。&lt;/p>
&lt;!--
The Kubernetes in-tree storage plugin to Container Storage Interface (CSI) migration infrastructure is now beta in Kubernetes v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.
-->
&lt;h2 id="云服务提供商标签基本可用-1">云服务提供商标签基本可用&lt;/h2>
&lt;!--
## Cloud Provider Labels reach General Availability
-->
&lt;p>当节点和卷被创建，会基于基础云提供商的Kubernetes集群打上一系列标准标签。节点会获得一个实例类型标签。节点和卷都会得到两个描述资源在云提供商拓扑的位置标签,通常是以区域和地区的方式组织。&lt;/p>
&lt;!--
When nodes and volumes are created, a set of standard labels are applied based on the underlying cloud provider of the Kubernetes cluster. Nodes get a label for the instance type. Both nodes and volumes get two labels describing the location of the resource in the cloud provider topology, usually organized in zones and regions.
-->
&lt;p>Kubernetes组件使用标准标签来支持一些特性。例如，调度者会保证pods和它们所声明的卷放置在相同的区域；当调度部署的pods时，调度器会优先将它们分布在不同的区域。你还可以在自己的pods标准中利用标签来配置，如节点亲和性，之类的事。标准标签使得你写的pod规范在不同的云提供商之间是可移植的。&lt;/p>
&lt;!--
Standard labels are used by Kubernetes components to support some features. For example, the scheduler would ensure that pods are placed on the same zone as the volumes they claim; and when scheduling pods belonging to a deployment, the scheduler would prioritize spreading them across zones. You can also use the labels in your pod specs to configure things as such node affinity. Standard labels allow you to write pod specs that are portable among different cloud providers.
-->
&lt;p>在这个版本中，标签已经达到基本可用。Kubernetes组件都已经更新，可以填充基本可用和公开测试版标签，并对两者做出反应。然而，如果你的pod规范或自定义的控制器正在使用公开测试版标签，如节点亲和性，我们建议你可以将它们迁移到新的基本可用标签中。你可以从如下地方找到新标签的文档：&lt;/p>
&lt;!--
The labels are reaching general availability in this release. Kubernetes components have been updated to populate the GA and beta labels and to react to both. However, if you are using the beta labels in your pod specs for features such as node affinity, or in your custom controllers, we recommend that you start migrating them to the new GA labels. You can find the documentation for the new labels here:
-->
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#nodekubernetesioinstance-type">实例类型&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesioregion">地区&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone">区域&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [node.kubernetes.io/instance-type](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#nodekubernetesioinstance-type)
- [topology.kubernetes.io/region](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesioregion)
- [topology.kubernetes.io/zone](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone)
-->
&lt;h2 id="卷快照进入公开测试版-1">卷快照进入公开测试版&lt;/h2>
&lt;!--
## Volume Snapshot Moves to Beta
-->
&lt;p>在 v1.17 中，Kubernetes卷快照是是公开测试版。最初是在 v1.12 中以内部测试版引入的，第二个有重大变化的内部测试版是 v1.13 。这篇文章总结它在公开版本中的变化。&lt;/p>
&lt;!--
The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13. This post summarizes the changes in the beta release.
-->
&lt;h3 id="卷快照是什么">卷快照是什么？&lt;/h3>
&lt;!-- ### What is a Volume Snapshot? -->
&lt;p>许多的存储系统(如谷歌云持久化磁盘，亚马逊弹性块存储和许多的内部存储系统)支持为持久卷创建快照。快照代表卷在一个时间点的复制。它可用于配置新卷(使用快照数据提前填充)或恢复卷到一个之前的状态(用快照表示)。&lt;/p>
&lt;!--
Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a “snapshot” of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).
-->
&lt;h3 id="为什么给kubernetes加入卷快照">为什么给Kubernetes加入卷快照？&lt;/h3>
&lt;!--
### Why add Volume Snapshots to Kubernetes?
-->
&lt;p>Kubernetes卷插件系统已经提供了功能强大的抽象用于自动配置、附加和挂载块文件系统。&lt;/p>
&lt;!--
The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.
-->
&lt;p>支持所有这些特性是Kubernets负载可移植的目标：Kubernetes旨在分布式系统应用和底层集群之间创建一个抽象层,使得应用可以不感知其运行集群的具体信息并且部署也不需特定集群的知识。&lt;/p>
&lt;!--
Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster specific” knowledge.
-->
&lt;p>Kubernetes存储特别兴趣组(SIG)将快照操作确定为对很多有状态负载的关键功能。如数据库管理员希望在操作数据库前保存数据库卷快照。&lt;/p>
&lt;!--
The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.
-->
&lt;p>在Kubernetes接口中提供一种标准的方式触发快照操作，Kubernetes用户可以处理这种用户场景，而不必使用Kubernetes API(并手动执行存储系统的具体操作)。&lt;/p>
&lt;!--
By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).
-->
&lt;p>取而代之的是，Kubernetes用户现在被授权以与集群无关的方式将快照操作放进他们的工具和策略中，并且确信它将对任意的Kubernetes集群有效，而与底层存储无关。&lt;/p>
&lt;!--
Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.
-->
&lt;p>此外，Kubernetes 快照原语作为基础构建能力解锁了为Kubernetes开发高级、企业级、存储管理特性的能力:包括应用或集群级别的备份方案。&lt;/p>
&lt;!--
Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: including application or cluster level backup solutions.
-->
&lt;p>你可以阅读更多关于&lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/">发布容器存储接口卷快照公开测试版&lt;/a>&lt;/p>
&lt;!--
You can read more in the blog entry about [releasing CSI volume snapshots to beta](https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/).
-->
&lt;h2 id="容器存储接口迁移公测版">容器存储接口迁移公测版&lt;/h2>
&lt;!--
## CSI Migration Beta
-->
&lt;h3 id="为什么我们迁移内建树插件到容器存储接口">为什么我们迁移内建树插件到容器存储接口？&lt;/h3>
&lt;!--
### Why are we migrating in-tree plugins to CSI?
-->
&lt;p>在容器存储接口之前，Kubernetes提供功能强大的卷插件系统。这些卷插件是树内的意味着它们的代码是核心Kubernetes代码的一部分并附带在核心Kubernetes二进制中。然而，为Kubernetes添加插件支持新卷是非常有挑战的。希望在Kubernetes上为自己存储系统添加支持(或修复现有卷插件的bug)的供应商被迫与Kubernetes发行进程对齐。此外，第三方存储代码在核心Kubernetes二进制中会造成可靠性和安全问题，并且这些代码对于Kubernetes的维护者来说是难以(一些场景是不可能)测试和维护的。在Kubernetes上采用容器存储接口可以解决大部分问题。&lt;/p>
&lt;!--
Prior to CSI, Kubernetes provided a powerful volume plugin system. These volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries. However, adding support for new volume plugins to Kubernetes was challenging. Vendors that wanted to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain. Using the Container Storage Interface in Kubernetes resolves these major issues.
-->
&lt;p>随着更多容器存储接口驱动变成生产环境可用，我们希望所有的Kubernetes用户从容器存储接口模型中获益。然而，我们不希望强制用户以破坏现有基本可用的存储接口的方式去改变负载和配置。道路很明确，我们将不得不用CSI替换树内插件接口。什么是容器存储接口迁移？&lt;/p>
&lt;!--
As more CSI Drivers were created and became production ready, we wanted all Kubernetes users to reap the benefits of the CSI model. However, we did not want to force users into making workload/configuration changes by breaking the existing generally available storage APIs. The way forward was clear - we would have to replace the backend of the “in-tree plugin” APIs with CSI.What is CSI migration?
-->
&lt;p>在容器存储接口迁移上所做的努力使得替换现有的树内存储插件，如&lt;code>kubernetes.io/gce-pd&lt;/code>或&lt;code>kubernetes.io/aws-ebs&lt;/code>，为相应的容器存储接口驱动成为可能。如果容器存储接口迁移正常工作，Kubernetes终端用户不会注意到任何差别。迁移过后，Kubernetes用户可以继续使用现有接口来依赖树内存储插件的功能。&lt;/p>
&lt;!--
The CSI migration effort enables the replacement of existing in-tree storage plugins such as `kubernetes.io/gce-pd` or `kubernetes.io/aws-ebs` with a corresponding CSI driver. If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. After migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.
-->
&lt;p>当Kubernetes集群管理者更新集群使得CSI迁移可用，现有的有状态部署和工作负载照常工作；然而，在幕后Kubernetes将存储管理操作交给了(以前是交给树内驱动)CSI驱动。&lt;/p>
&lt;!--
When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing stateful deployments and workloads continue to function as they always have; however, behind the scenes Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.
-->
&lt;p>Kubernetes组非常努力地保证存储接口的稳定性和平滑升级体验的承诺。这需要细致的考虑现有特性和行为来确保后向兼容和接口稳定性。你可以想像成在加速行驶的直线上给赛车换轮胎。&lt;/p>
&lt;!--
The Kubernetes team has worked hard to ensure the stability of storage APIs and for the promise of a smooth upgrade experience. This involves meticulous accounting of all existing features and behaviors to ensure backwards compatibility and API stability. You can think of it like changing the wheels on a racecar while it’s speeding down the straightaway.
-->
&lt;p>你可以在这篇博客中阅读更多关于&lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/">容器存储接口迁移成为公开测试版&lt;/a>.&lt;/p>
&lt;!--
You can read more in the blog entry about [CSI migration going to beta](https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/). -->
&lt;h2 id="其它更新">其它更新&lt;/h2>
&lt;!--
## Other Updates
-->
&lt;h3 id="稳定">稳定💯&lt;/h3>
&lt;!--
### Graduated to Stable 💯
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/382">按条件污染节点&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/495">可配置的Pod进程共享命名空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/548">采用kube-scheduler调度DaemonSet Pods&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/554">动态卷最大值&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/557">Kubernetes容器存储接口支持拓扑&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/559">在SubPath挂载提供环境变量扩展&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/575">为Custom Resources提供默认值&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/589">从频繁的Kublet心跳到租约接口&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/714">拆分Kubernetes测试Tarball&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/956">添加Watch书签支持&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/960">行为驱动一致性测试&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/980">服务负载均衡终结保护&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1152">避免每一个Watcher独立序列化相同的对象&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [Taint Node by Condition](https://github.com/kubernetes/enhancements/issues/382)
- [Configurable Pod Process Namespace Sharing](https://github.com/kubernetes/enhancements/issues/495)
- [Schedule DaemonSet Pods by kube-scheduler](https://github.com/kubernetes/enhancements/issues/548)
- [Dynamic Maximum Volume Count](https://github.com/kubernetes/enhancements/issues/554)
- [Kubernetes CSI Topology Support](https://github.com/kubernetes/enhancements/issues/557)
- [Provide Environment Variables Expansion in SubPath Mount](https://github.com/kubernetes/enhancements/issues/559)
- [Defaulting of Custom Resources](https://github.com/kubernetes/enhancements/issues/575)
- [Move Frequent Kubelet Heartbeats To Lease Api](https://github.com/kubernetes/enhancements/issues/589)
- [Break Apart The Kubernetes Test Tarball](https://github.com/kubernetes/enhancements/issues/714)
- [Add Watch Bookmarks Support](https://github.com/kubernetes/enhancements/issues/956)
- [Behavior-Driven Conformance Testing](https://github.com/kubernetes/enhancements/issues/960)
- [Finalizer Protection For Service Loadbalancers](https://github.com/kubernetes/enhancements/issues/980)
- [Avoid Serializing The Same Object Independently For Every Watcher](https://github.com/kubernetes/enhancements/issues/1152)
-->
&lt;h3 id="主要变化">主要变化&lt;/h3>
&lt;!--
### Major Changes
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/563">添加IPv4/IPv6双栈支持&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [Add IPv4/IPv6 Dual Stack Support](https://github.com/kubernetes/enhancements/issues/563)
-->
&lt;h3 id="其它显著特性">其它显著特性&lt;/h3>
&lt;!--
### Other Notable Features
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/536">拓扑感知路由服务(内部测试版)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1043">为Windows添加RunAsUserName&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [Topology Aware Routing of Services (Alpha)](https://github.com/kubernetes/enhancements/issues/536)
- [RunAsUserName for Windows](https://github.com/kubernetes/enhancements/issues/1043)
-->
&lt;h3 id="可用性">可用性&lt;/h3>
&lt;!--
### Availability
-->
&lt;p>Kubernetes 1.17 可以&lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0">在GitHub下载&lt;/a>。开始使用Kubernetes，看看这些&lt;a href="https://kubernetes.io/docs/tutorials/">交互教学&lt;/a>。你可以非常容易使用&lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>安装1.17。&lt;/p>
&lt;!--
Kubernetes 1.17 is available for [download on GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0). To get started with Kubernetes, check out these [interactive tutorials](https://kubernetes.io/docs/tutorials/). You can also easily install 1.17 using
[kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/).
-->
&lt;h3 id="发布团队">发布团队&lt;/h3>
&lt;!--
### Release Team
-->
&lt;p>正是因为有上千人参与技术或非技术内容的贡献才使这个版本成为可能。特别感谢由Guinevere Saenger领导的&lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md">发布团队&lt;/a>。发布团队的35名成员在发布版本的多方面进行了协调，从文档到测试，校验和特性的完善。&lt;/p>
&lt;!--
This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the [release team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md) led by Guinevere Saenger. The 35 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.
-->
&lt;p>随着Kubernetes社区的成长，我们的发布流程是在开源软件协作方面惊人的示例。Kubernetes快速并持续获得新用户。这一成长产生了良性的反馈循环，更多的贡献者贡献代码创造了更加活跃的生态。Kubernetes已经有超过&lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">39000位贡献者&lt;/a>和一个超过66000人的活跃社区。&lt;/p>
&lt;!--
As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over [39,000 individual contributors](https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1) to date and an active community of more than 66,000 people.
-->
&lt;h3 id="网络研讨会">网络研讨会&lt;/h3>
&lt;!--
### Webinar
-->
&lt;p>2020年1月7号，加入Kubernetes 1.17发布团队，学习关于这次发布的主要特性。&lt;a href="https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A">这里&lt;/a>注册。&lt;/p>
&lt;!--
Join members of the Kubernetes 1.17 release team on Jan 7th, 2020 to learn about the major features in this release. Register [here](https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A).
-->
&lt;h3 id="参与其中">参与其中&lt;/h3>
&lt;!--
### Get Involved
-->
&lt;p>最简单的参与Kubernetes的方式是加入其中一个与你兴趣相同的&lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">特别兴趣组&lt;/a>（SIGs)。有什么想要广播到Kubernetes社区吗？通过如下的频道，在每周的&lt;a href="https://github.com/kubernetes/community/tree/master/communication">社区会议&lt;/a>分享你的声音。感谢你的贡献和支持。&lt;/p>
&lt;!--
The simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.
-->
&lt;ul>
&lt;li>在Twitter上关注我们&lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a>获取最新的更新&lt;/li>
&lt;li>在&lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>参与社区的讨论&lt;/li>
&lt;li>在&lt;a href="http://slack.k8s.io/">Slack&lt;/a>加入社区&lt;/li>
&lt;li>在&lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>发布问题(或回答问题)&lt;/li>
&lt;li>分享你的Kubernetes&lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">故事&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
- Join the community on [Slack](http://slack.k8s.io/)
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
--></description></item><item><title>Blog: 使用 Java 开发一个 Kubernetes controller</title><link>https://kubernetes.io/zh/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</guid><description>
&lt;!--
---
layout: blog
title: "Develop a Kubernetes controller in Java"
date: 2019-11-26
slug: Develop-A-Kubernetes-Controller-in-Java
---
-->
&lt;!--
**Authors:** Min Kim (Ant Financial), Tony Ado (Ant Financial)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Min Kim (蚂蚁金服), Tony Ado (蚂蚁金服)&lt;/p>
&lt;!--
The official [Kubernetes Java SDK](https://github.com/kubernetes-client/java) project
recently released their latest work on providing the Java Kubernetes developers
a handy Kubernetes controller-builder SDK which is helpful for easily developing
advanced workloads or systems.
-->
&lt;p>&lt;a href="https://github.com/kubernetes-client/java">Kubernetes Java SDK&lt;/a> 官方项目最近发布了他们的最新工作，为 Java Kubernetes 开发人员提供一个便捷的 Kubernetes 控制器-构建器 SDK，它有助于轻松开发高级工作负载或系统。&lt;/p>
&lt;!--
## Overall
Java is no doubt one of the most popular programming languages in the world but
it's been difficult for a period time for those non-Golang developers to build up
their customized controller/operator due to the lack of library resources in the
community. In the world of Golang, there're already some excellent controller
frameworks, for example, [controller runtime](https://github.com/kubernetes-sigs/controller-runtime),
[operator SDK](https://github.com/operator-framework/operator-sdk). These
existing Golang frameworks are relying on the various utilities from the
[Kubernetes Golang SDK](https://github.com/kubernetes/client-go) proven to
be stable over years. Driven by the emerging need of further integration into
the platform of Kubernetes, we not only ported many essential toolings from the Golang
SDK into the kubernetes Java SDK including informers, work-queues, leader-elections,
etc. but also developed a controller-builder SDK which wires up everything into
a runnable controller without hiccups.
-->
&lt;h2 id="综述">综述&lt;/h2>
&lt;p>Java 无疑是世界上最流行的编程语言之一，但由于社区中缺少库资源，一段时间以来，那些非 Golang 开发人员很难构建他们定制的 controller/operator。在 Golang 的世界里，已经有一些很好的 controller 框架了，例如，&lt;a href="https://github.com/kubernetes-sigs/controller-runtime">controller runtime&lt;/a>，&lt;a href="https://github.com/operator-framework/operator-sdk">operator SDK&lt;/a>。这些现有的 Golang 框架依赖于 &lt;a href="https://github.com/kubernetes/client-go">Kubernetes Golang SDK&lt;/a> 提供的各种实用工具，这些工具经过多年证明是稳定的。受进一步集成到 Kubernetes 平台的需求驱动，我们不仅将 Golang SDK 中的许多基本工具移植到 kubernetes Java SDK 中，包括 informers、work-queues、leader-elections 等，也开发了一个控制器构建 SDK，它可以将所有东西连接到一个可运行的控制器中，而不会产生任何问题。&lt;/p>
&lt;!--
## Backgrounds
Why use Java to implement Kubernetes tooling? You might pick Java for:
- __Integrating legacy enterprise Java systems__: Many companies have their legacy
systems or frameworks written in Java in favor of stability. We are not able to
move everything to Golang easily.
- __More open-source community resources__: Java is mature and has accumulated abundant open-source
libraries over decades, even though Golang is getting more and more fancy and
popular for developers. Additionally, nowadays developers are able to develop
their aggregated-apiservers over SQL-storage and Java has way better support on SQLs.
-->
&lt;h2 id="背景">背景&lt;/h2>
&lt;p>为什么要使用 Java 实现 kubernetes 工具？选择 Java 的原因可能是：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>集成遗留的企业级 Java 系统&lt;/strong>：许多公司的遗留系统或框架都是用 Java 编写的，用以支持稳定性。我们不能轻易把所有东西搬到 Golang。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>更多开源社区的资源&lt;/strong>：Java 是成熟的，并且在过去几十年中累计了丰富的开源库，尽管 Golang 对于开发人员来说越来越具有吸引力，越来越流行。此外，现在开发人员能够在 SQL 存储上开发他们的聚合-apiserver，而 Java 在 SQL 上有更好的支持。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;!--
## How to use?
Take maven project as example, adding the following dependencies into your dependencies:
-->
&lt;h2 id="如何去使用">如何去使用&lt;/h2>
&lt;p>以 maven 项目为例，将以下依赖项添加到您的依赖中：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-xml" data-lang="xml">&lt;span style="color:#008000;font-weight:bold">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;groupId&amp;gt;&lt;/span>io.kubernetes&lt;span style="color:#008000;font-weight:bold">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;artifactId&amp;gt;&lt;/span>client-java-extended&lt;span style="color:#008000;font-weight:bold">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;version&amp;gt;&lt;/span>6.0.1&lt;span style="color:#008000;font-weight:bold">&amp;lt;/version&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Then we can make use of the provided builder libraries to write your own controller.
For example, the following one is a simple controller prints out node information
on watch notification, see complete example [here](https://github.com/kubernetes-client/java/blob/master/examples/src/main/java/io/kubernetes/client/examples/ControllerExample.java):
-->
&lt;p>然后我们可以使用提供的生成器库来编写自己的控制器。例如，下面是一个简单的控制，它打印出关于监视通知的节点信息，请看完整的例子：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-java" data-lang="java">&lt;span style="color:#666">...&lt;/span>
Reconciler reconciler &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">new&lt;/span> Reconciler&lt;span style="color:#666">()&lt;/span> &lt;span style="color:#666">{&lt;/span>
&lt;span style="color:#a2f">@Override&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">public&lt;/span> Result &lt;span style="color:#00a000">reconcile&lt;/span>&lt;span style="color:#666">(&lt;/span>Request request&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#666">{&lt;/span>
V1Node node &lt;span style="color:#666">=&lt;/span> nodeLister&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">get&lt;/span>&lt;span style="color:#666">(&lt;/span>request&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">getName&lt;/span>&lt;span style="color:#666">());&lt;/span>
System&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">out&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">println&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b44">&amp;#34;triggered reconciling &amp;#34;&lt;/span> &lt;span style="color:#666">+&lt;/span> node&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">getMetadata&lt;/span>&lt;span style="color:#666">().&lt;/span>&lt;span style="color:#b44">getName&lt;/span>&lt;span style="color:#666">());&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> &lt;span style="color:#a2f;font-weight:bold">new&lt;/span> Result&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#666">);&lt;/span>
&lt;span style="color:#666">}&lt;/span>
&lt;span style="color:#666">};&lt;/span>
Controller controller &lt;span style="color:#666">=&lt;/span>
ControllerBuilder&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">defaultBuilder&lt;/span>&lt;span style="color:#666">(&lt;/span>informerFactory&lt;span style="color:#666">)&lt;/span>
&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">watch&lt;/span>&lt;span style="color:#666">(&lt;/span>
&lt;span style="color:#666">(&lt;/span>workQueue&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#666">-&amp;gt;&lt;/span> ControllerBuilder&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">controllerWatchBuilder&lt;/span>&lt;span style="color:#666">(&lt;/span>V1Node&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">class&lt;/span>&lt;span style="color:#666">,&lt;/span> workQueue&lt;span style="color:#666">).&lt;/span>&lt;span style="color:#b44">build&lt;/span>&lt;span style="color:#666">())&lt;/span>
&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withReconciler&lt;/span>&lt;span style="color:#666">(&lt;/span>nodeReconciler&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// required, set the actual reconciler
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withName&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b44">&amp;#34;node-printing-controller&amp;#34;&lt;/span>&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, set name for controller for logging, thread-tracing
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withWorkerCount&lt;/span>&lt;span style="color:#666">(&lt;/span>4&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, set worker thread count
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withReadyFunc&lt;/span>&lt;span style="color:#666">(&lt;/span> nodeInformer&lt;span style="color:#666">::&lt;/span>hasSynced&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, only starts controller when the cache has synced up
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">build&lt;/span>&lt;span style="color:#666">();&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
If you notice, the new Java controller framework learnt a lot from the design of
[controller-runtime](https://github.com/kubernetes-sigs/controller-runtime) which
successfully encapsulates the complex components inside controller into several
clean interfaces. With the help of Java Generics, we even move on a bit and simply
the encapsulation in a better way.
-->
&lt;p>如果您留意，新的 Java 控制器框架很多地方借鉴于 &lt;a href="https://github.com/kubernetes-sigs/controller-runtime">controller-runtime&lt;/a> 的设计，它成功地将控制器内部的复杂组件封装到几个干净的接口中。在 Java 泛型的帮助下，我们甚至更进一步，以更好的方式简化了封装。&lt;/p>
&lt;!--
As for more advanced usage, we can wrap multiple controllers into a controller-manager
or a leader-electing controller which helps deploying in HA setup. In a word, we can
basically find most of the equivalence implementations here from Golang SDK and
more advanced features are under active development by us.
-->
&lt;p>我们可以将多个控制器封装到一个 controller-manager 或 leader-electing controller 中，这有助于在 HA 设置中进行部署。&lt;/p>
&lt;!--
## Future steps
The community behind the official Kubernetes Java SDK project will be focusing on
providing more useful utilities for developers who hope to program cloud native
Java applications to extend Kubernetes. If you are interested in more details,
please look at our repo [kubernetes-client/java](https://github.com/kubernetes-client/java).
Feel free to share also your feedback with us, through Issues or [Slack](http://kubernetes.slack.com/messages/kubernetes-client/).
-->
&lt;h2 id="未来计划">未来计划&lt;/h2>
&lt;p>Kubernetes Java SDK 项目背后的社区将专注于为希望编写云原生 Java 应用程序来扩展 Kubernetes 的开发人员提供更有用的实用程序。如果您对更详细的信息感兴趣，请查看我们的仓库 &lt;a href="https://github.com/kubernetes-client/java">kubernetes-client/java&lt;/a>。请通过问题或 &lt;a href="http://kubernetes.slack.com/messages/kubernetes-client/">Slack&lt;/a> 与我们分享您的反馈。&lt;/p></description></item><item><title>Blog: 使用 Microk8s 在 Linux 上本地运行 Kubernetes</title><link>https://kubernetes.io/zh/blog/2019/11/26/%E4%BD%BF%E7%94%A8-microk8s-%E5%9C%A8-linux-%E4%B8%8A%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C-kubernetes/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/11/26/%E4%BD%BF%E7%94%A8-microk8s-%E5%9C%A8-linux-%E4%B8%8A%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C-kubernetes/</guid><description>
&lt;!--
---
title: 'Running Kubernetes locally on Linux with Microk8s'
date: 2019-11-26
---
-->
&lt;!--
**Authors**: [Ihor Dvoretskyi](https://twitter.com/idvoretskyi), Developer Advocate, Cloud Native Computing Foundation; [Carmine Rimi](https://twitter.com/carminerimi)
-->
&lt;p>&lt;strong>作者&lt;/strong>: &lt;a href="https://twitter.com/idvoretskyi">Ihor Dvoretskyi&lt;/a>，开发支持者，云原生计算基金会；&lt;a href="https://twitter.com/carminerimi">Carmine Rimi&lt;/a>&lt;/p>
&lt;!--
This article, the second in a [series](/blog/2019/03/28/running-kubernetes-locally-on-linux-with-minikube-now-with-kubernetes-1.14-support/) about local deployment options on Linux, and covers [MicroK8s](https://microk8s.io/). Microk8s is the click-and-run solution for deploying a Kubernetes cluster locally, originally developed by Canonical, the publisher of Ubuntu.
-->
&lt;p>本文是关于 Linux 上的本地部署选项&lt;a href="https://twitter.com/idvoretskyi">系列&lt;/a>的第二篇，涵盖了 &lt;a href="https://microk8s.io/">MicroK8s&lt;/a>。Microk8s 是本地部署 Kubernetes 集群的 'click-and-run' 方案，最初由 Ubuntu 的发布者 Canonical 开发。&lt;/p>
&lt;!--
While Minikube usually spins up a local virtual machine (VM) for the Kubernetes cluster, MicroK8s doesn’t require a VM. It uses [snap](https://snapcraft.io/) packages, an application packaging and isolation technology.
-->
&lt;p>虽然 Minikube 通常为 Kubernetes 集群创建一个本地虚拟机（VM），但是 MicroK8s 不需要 VM。它使用&lt;a href="https://snapcraft.io/">snap&lt;/a> 包，这是一种应用程序打包和隔离技术。&lt;/p>
&lt;!--
This difference has its pros and cons. Here we’ll discuss a few of the interesting differences, and comparing the benefits of a VM based approach with the benefits of a non-VM approach. One of the first factors is cross-platform portability. While a Minikube VM is portable across operating systems - it supports not only Linux, but Windows, macOS, and even FreeBSD - Microk8s requires Linux, and only on those distributions [that support snaps](https://snapcraft.io/docs/installing-snapd). Most popular Linux distributions are supported.
-->
&lt;p>这种差异有其优点和缺点。在这里，我们将讨论一些有趣的区别，并且基于 VM 的方法和非 VM 方法的好处。第一个因素是跨平台的移植性。虽然 Minikube VM 可以跨操作系统移植——它不仅支持 Linux，还支持 Windows、macOS、甚至 FreeBSD，但 Microk8s 需要 Linux，而且只在&lt;a href="https://snapcraft.io/docs/installing-snapd">那些支持 snaps&lt;/a> 的发行版上。支持大多数流行的 Linux 发行版。&lt;/p>
&lt;!--
Another factor to consider is resource consumption. While a VM appliance gives you greater portability, it does mean you’ll consume more resources to run the VM, primarily because the VM ships a complete operating system, and runs on top of a hypervisor. You’ll consume more disk space when the VM is dormant. You’ll consume more RAM and CPU while it is running. Since Microk8s doesn’t require spinning up a virtual machine you’ll have more resources to run your workloads and other applications. Given its smaller footprint, MicroK8s is ideal for IoT devices - you can even use it on a Raspberry Pi device!
-->
&lt;p>另一个考虑到的因素是资源消耗。虽然 VM 设备为您提供了更好的可移植性，但它确实意味着您将消耗更多资源来运行 VM，这主要是因为 VM 提供了一个完整的操作系统，并且运行在管理程序之上。当 VM 处于休眠时你将消耗更多的磁盘空间。当它运行时，你将会消耗更多的 RAM 和 CPU。因为 Microk8s 不需要创建虚拟机，你将会有更多的资源去运行你的工作负载和其他设备。考虑到所占用的空间更小，MicroK8s 是物联网设备的理想选择-你甚至可以在 Paspberry Pi 和设备上使用它！&lt;/p>
&lt;!--
Finally, the projects appear to follow a different release cadence and strategy. MicroK8s, and snaps in general provide [channels](https://snapcraft.io/docs/channels) that allow you to consume beta and release candidate versions of new releases of Kubernetes, as well as the previous stable release. Microk8s generally releases the stable release of upstream Kubernetes almost immediately.
-->
&lt;p>最后，项目似乎遵循了不同的发布节奏和策略。Microk8s 和 snaps 通常提供&lt;a href="https://snapcraft.io/docs/channels">渠道&lt;/a>允许你使用测试版和发布 KUbernetes 新版本的候选版本，同样也提供先前稳定版本。Microk8s 通常几乎立刻发布 Kubernetes 上游的稳定版本。&lt;/p>
&lt;!--
But wait, there’s more! Minikube and MicroK8s both started as single-node clusters. Essentially, they allow you to create a Kubernetes cluster with a single worker node. That is about to change - there’s an early alpha release of MicroK8s that includes clustering. With this capability, you can create Kubernetes clusters with as many worker nodes as you wish. This is effectively an un-opinionated option for creating a cluster - the developer must create the network connectivity between the nodes, as well as integrate with other infrastructure that may be required, like an external load-balancer. In summary, MicroK8s offers a quick and easy way to turn a handful of computers or VMs into a multi-node Kubernetes cluster. We’ll write more about this kind of architecture in a future article.
-->
&lt;p>但是等等，还有更多！Minikube 和 Microk8s 都是作为单节点集群启动的。本质上来说，它们允许你用单个工作节点创建 Kubernetes 集群。这种情况即将改变 - MicroK8s 早期的 alpha 版本包括集群。有了这个能力，你可以创建正如你希望多的工作节点的 KUbernetes 集群。对于创建集群来说，这是一个没有主见的选项 - 开发者在节点之间创建网络连接和集成了其他所需要的基础设施，比如一个外部的负载均衡。总的来说，MicroK8s 提供了一种快速简易的方法，使得少量的计算机和虚拟机变成一个多节点的 Kubernetes 集群。以后我们将撰写更多这种体系结构的文章。&lt;/p>
&lt;!--
## Disclaimer
This is not an official guide to MicroK8s. You may find detailed information on running and using MicroK8s on it's official [webpage](https://microk8s.io/docs/), where different use cases, operating systems, environments, etc. are covered. Instead, the purpose of this post is to provide clear and easy guidelines for running MicroK8s on Linux.
-->
&lt;h2 id="免责声明">免责声明&lt;/h2>
&lt;p>这不是 MicroK8s 官方介绍文档。你可以在它的官方&lt;a href="https://microk8s.io/docs/">网页&lt;/a>查询运行和使用 MicroK8s 的详情信息，其中覆盖了不同的用例，操作系统，环境等。相反，这篇文章的意图是提供在 Linux 上运行 MicroK8s 清晰易懂的指南。&lt;/p>
&lt;!--
## Prerequisites
A Linux distribution that [supports snaps](https://snapcraft.io/docs/installing-snapd), is required. In this guide, we’ll use Ubuntu 18.04 LTS, it supports snaps out-of-the-box.
If you are interested in running Microk8s on Windows or Mac, you should check out [Multipass](https://multipass.run) to stand up a quick Ubuntu VM as the official way to run virtual Ubuntu on your system.
-->
&lt;h2 id="前提条件">前提条件&lt;/h2>
&lt;p>一个&lt;a href="https://snapcraft.io/docs/installing-snapd">支持 snaps&lt;/a> 的 Linux 发行版是被需要的。这篇指南，我们将会用支持 snaps 且即开即用的 Ubuntu 18.04 LTS。如果你对运行在 Windows 或者 Mac 上的 MicroK8s 感兴趣，你应该检查&lt;a href="https://multipass.run">多通道&lt;/a>，安装一个快速的 Ubuntu VM，作为在你的系统上运行虚拟机 Ubuntu 的官方方式。&lt;/p>
&lt;!--
## MicroK8s installation
MicroK8s installation is straightforward:
-->
&lt;h2 id="microk8s-安装">MicroK8s 安装&lt;/h2>
&lt;p>简洁的 MicroK8s 安装：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap install microk8s --classic
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/001-install.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
The command above installs a local single-node Kubernetes cluster in seconds. Once the command execution is finished, your Kubernetes cluster is up and running.
&lt;p>You may verify the MicroK8s status with the following command:
--&amp;gt;
以上的命令将会在几秒内安装一个本地单节点的 Kubernetes 集群。一旦命令执行结束，你的 Kubernetes 集群将会启动并运行。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo microk8s.status
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/002-status.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Using microk8s
Using MicroK8s is as straightforward as installing it. MicroK8s itself includes a `kubectl` binary, which can be accessed by running the `microk8s.kubectl` command. As an example:
-->
&lt;h2 id="使用-microk8s">使用 microk8s&lt;/h2>
&lt;p>使用 MicrosK8s 就像和安装它一样便捷。MicroK8s 本身包括一个 &lt;code>kubectl&lt;/code> 库，该库可以通过执行 &lt;code>microk8s.kubectl&lt;/code> 命令去访问。例如：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">microk8s.kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/003-nodes.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
While using the prefix `microk8s.kubectl` allows for a parallel install of another system-wide kubectl without impact, you can easily get rid of it by using the `snap alias` command:
-->
&lt;p>当使用前缀 &lt;code>microk8s.kubectl&lt;/code> 时，允许在没有影响的情况下并行地安装另一个系统级的 kubectl，你可以便捷地使用 &lt;code>snap alias&lt;/code> 命令摆脱它：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap &lt;span style="color:#a2f">alias&lt;/span> microk8s.kubectl kubectl
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
This will allow you to simply use `kubectl` after. You can revert this change using the `snap unalias` command.
-->
&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/004-alias.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;p>这将允许你以后便捷地使用 &lt;code>kubectl&lt;/code>，你可以用 &lt;code>snap unalias&lt;/code>命令恢复这个改变。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/005-nodes.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## MicroK8s addons
One of the biggest benefits of using Microk8s is the fact that it also supports various add-ons and extensions. What is even more important is they are shipped out of the box, the user just has to enable them.
The full list of extensions can be checked by running the `microk8s.status` command:
-->
&lt;h2 id="microk8s-插件">MicroK8s 插件&lt;/h2>
&lt;p>使用 MicroK8s 其中最大的好处之一事实上是也支持各种各样的插件和扩展。更重要的是它们是开箱即用的，用户仅仅需要启动它们。通过运行 &lt;code>microk8s.status&lt;/code> 命令检查出扩展的完整列表。&lt;/p>
&lt;pre>&lt;code>sudo microk8s.status
&lt;/code>&lt;/pre>&lt;!--
As of the time of writing this article, the following add-ons are supported:
-->
&lt;p>截至到写这篇文章为止，MicroK8s 已支持以下插件：&lt;/p>
&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/006-status.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
More add-ons are being created and contributed by the community all the time, it definitely helps to check often!
-->
&lt;p>社区创建和贡献了越来越多的插件，经常检查他们是十分有帮助的。&lt;/p>
&lt;!--
## Release channels
-->
&lt;h2 id="发布渠道">发布渠道&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap info microk8s
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/010-releases.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Installing the sample application
In this tutorial we’ll use NGINX as a sample application ([the official Docker Hub image](https://hub.docker.com/_/nginx)).
It will be installed as a Kubernetes deployment:
-->
&lt;h2 id="安装简单的应用">安装简单的应用&lt;/h2>
&lt;p>在这篇指南中我将会用 NGINX 作为一个示例应用程序（&lt;a href="https://hub.docker.com/_/nginx">官方 Docker Hub 镜像&lt;/a>）。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl create deployment nginx --image&lt;span style="color:#666">=&lt;/span>nginx
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
To verify the installation, let’s run the following:
-->
&lt;p>为了检查安装，让我们运行以下命令：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get deployments
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get pods
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/007-deployments.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
Also, we can retrieve the full output of all available objects within our Kubernetes cluster:
-->
&lt;p>我们也可以检索出 Kubernetes 集群中所有可用对象的完整输出。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/008-all.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Uninstalling MicroK8s
Uninstalling your microk8s cluster is so easy as uninstalling the snap:
-->
&lt;h2 id="卸载-mircrok8s">卸载 MircroK8s&lt;/h2>
&lt;p>卸载您的 microk8s 集群与卸载 Snap 同样便捷。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap remove microk8s
&lt;/code>&lt;/pre>&lt;/div>&lt;center>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/009-remove.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Screencast
-->
&lt;h2 id="截屏视频">截屏视频&lt;/h2>
&lt;p>&lt;a href="https://asciinema.org/a/263394">&lt;img src="https://asciinema.org/a/263394.svg" alt="asciicast">&lt;/a>&lt;/p></description></item><item><title>Blog: Kubernetes 文档最终用户调研</title><link>https://kubernetes.io/zh/blog/2019/10/29/kubernetes-documentation-end-user-survey/</link><pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/10/29/kubernetes-documentation-end-user-survey/</guid><description>
&lt;!--
---
layout: blog
title: "Kubernetes Documentation Survey"
date: 2019-10-29
slug: kubernetes-documentation-end-user-survey
---
-->
&lt;p>&lt;strong>Author:&lt;/strong> &lt;a href="https://www.linkedin.com/in/aimee-ukasick/">Aimee Ukasick&lt;/a> and SIG Docs&lt;/p>
&lt;!--
In September, SIG Docs conducted its first survey about the [Kubernetes
documentation](https://kubernetes.io/docs/). We'd like to thank the CNCF's Kim
McMahon for helping us create the survey and access the results.
-->
&lt;p>9月，SIG Docs 进行了第一次关于 &lt;a href="https://kubernetes.io/docs/">Kubernetes 文档&lt;/a>的用户调研。我们要感谢 CNCF
的 Kim McMahon 帮助我们创建调查并获取结果。&lt;/p>
&lt;!--
# Key takeaways
-->
&lt;h1 id="主要收获">主要收获&lt;/h1>
&lt;!--
Respondents would like more example code, more detailed content, and more
diagrams in the Concepts, Tasks, and Reference sections.
-->
&lt;p>受访者希望能在概念、任务和参考部分得到更多示例代码、更详细的内容和更多图表。&lt;/p>
&lt;!--
74% of respondents would like the Tutorials section to contain advanced content.
-->
&lt;p>74% 的受访者希望教程部分包含高级内容。&lt;/p>
&lt;!--
69.70% said the Kubernetes documentation is the first place they look for
information about Kubernetes.
-->
&lt;p>69.70% 的受访者认为 Kubernetes 文档是他们首要寻找关于 Kubernetes 资料的地方。&lt;/p>
&lt;!--
# Survey methodology and respondents
-->
&lt;h1 id="调查方法和受访者">调查方法和受访者&lt;/h1>
&lt;!--
We conducted the survey in English. The survey was only available for 4 days due
to time constraints. We announced the survey on Kubernetes mailing lists, in
Kubernetes Slack channels, on Twitter, and in Kube Weekly. There were 23
questions, and respondents took an average of 4 minutes to complete the survey.
-->
&lt;p>我们用英语进行了调查。由于时间限制，调查的有效期只有 4 天。
我们在 Kubernetes 邮件列表、Kubernetes Slack 频道、Twitter、Kube Weekly 上发布了我们的调查问卷。
这份调查有 23 个问题， 受访者平均用 4 分钟完成这个调查。&lt;/p>
&lt;!--
## Quick facts about respondents:
-->
&lt;h2 id="关于受访者的简要情况">关于受访者的简要情况&lt;/h2>
&lt;!--
- 48.48% are experienced Kubernetes users, 26.26% expert, and 25.25% beginner
- 57.58% use Kubernetes in both administrator and developer roles
- 64.65% have been using the Kubernetes documentation for more than 12 months
- 95.96% read the documentation in English
-->
&lt;ul>
&lt;li>48.48% 是经验丰富的 Kubernetes 用户，26.26% 是专家，25.25% 是初学者&lt;/li>
&lt;li>57.58% 的人同时使用 Kubernetes 作为管理员和开发人员&lt;/li>
&lt;li>64.65% 的人使用 Kubernetes 文档超过 12 个月&lt;/li>
&lt;li>95.96% 的人阅读英文文档&lt;/li>
&lt;/ul>
&lt;!--
# Question and response highlights
-->
&lt;h1 id="问题和回答要点">问题和回答要点&lt;/h1>
&lt;!--
## Why people access the Kubernetes documentation
-->
&lt;h2 id="人们为什么访问-kubernetes-文档">人们为什么访问 Kubernetes 文档&lt;/h2>
&lt;!--
The majority of respondents stated that they access the documentation for the Concepts.
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-sig-docs-survey/Q9-k8s-docs-use.png"
alt="Why respondents access the Kubernetes documentation"/>
&lt;/figure>
-->
&lt;p>大多数受访者表示，他们访问文档是为了了解概念。&lt;/p>
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-sig-docs-survey/Q9-k8s-docs-use.png"
alt="受访者为什么访问 Kubernetes 文档"/>
&lt;/figure>
&lt;!--
This deviates only slightly from what we see in Google Analytics: of the top 10
most viewed pages this year, #1 is the kubectl cheatsheet in the Reference section,
followed overwhelmingly by pages in the Concepts section.
-->
&lt;p>这与我们在 Google Analytics 上看到的略有不同：在今年浏览量最多的10个页面中，第一是在参考部分的 kubectl
的备忘单，其次是概念部分的页面。&lt;/p>
&lt;!--
## Satisfaction with the documentation
-->
&lt;h2 id="对文档的满意程度">对文档的满意程度&lt;/h2>
&lt;!--
We asked respondents to record their level of satisfaction with the detail in
the Concepts, Tasks, Reference, and Tutorials sections:
-->
&lt;p>我们要求受访者从概念、任务、参考和教程部分记录他们对细节的满意度：&lt;/p>
&lt;!--
- Concepts: 47.96% Moderately Satisfied
- Tasks: 50.54% Moderately Satisfied
- Reference: 40.86% Very Satisfied
- Tutorial: 47.25% Moderately Satisfied
-->
&lt;ul>
&lt;li>概念：47.96% 中等满意&lt;/li>
&lt;li>任务：50.54% 中等满意&lt;/li>
&lt;li>参考：40.86% 非常满意&lt;/li>
&lt;li>教程：47.25% 中等满意&lt;/li>
&lt;/ul>
&lt;!--
## How SIG Docs can improve each documentation section
-->
&lt;h2 id="sig-docs-如何改进文档的各个部分">SIG Docs 如何改进文档的各个部分&lt;/h2>
&lt;!--
We asked how we could improve each section, providing respondents with
selectable answers as well as a text field. The clear majority would like more
example code, more detailed content, more diagrams, and advanced tutorials:
-->
&lt;p>我们询问如何改进每个部分，为受访者提供可选答案以及文本输入框。绝大多数人想要更多
示例代码、更详细的内容、更多图表和更高级的教程：&lt;/p>
&lt;!--
```text
- Personally, would like to see more analogies to help further understanding.
- Would be great if corresponding sections of code were explained too
- Expand on the concepts to bring them together - they're a bucket of separate eels moving in different directions right now
- More diagrams, and more example code
```
-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- 就个人而言，希望看到更多的类比，以帮助进一步理解。
- 如果代码的相应部分也能解释一下就好了
- 通过扩展概念把它们融合在一起 - 它们现在宛如在一桶水内朝各个方向游动的一条条鳗鱼。
- 更多的图表，更多的示例代码
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Respondents used the "Other" text box to record areas causing frustration:
-->
&lt;p>受访者使用“其他”文本框记录引发阻碍的区域：&lt;/p>
&lt;!--
```text
- Keep concepts up to date and accurate
- Keep task topics up to date and accurate. Human testing.
- Overhaul the examples. Many times the output of commands shown is not actual.
- I've never understood how to navigate or interpret the reference section
- Keep the tutorials up to date, or remove them
```
-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- 使概念保持最新和准确
- 保持任务主题的最新性和准确性。亲身试验。
- 彻底检查示例。很多时候显示的命令输出不是实际情况。
- 我从来都不知道如何导航或解释参考部分
- 使教程保持最新，或将其删除
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
## How SIG Docs can improve the documentation overall
-->
&lt;h2 id="sig-docs-如何全面改进文档">SIG Docs 如何全面改进文档&lt;/h2>
&lt;!--
We asked respondents how we can improve the Kubernetes documentation
overall. Some took the opportunity to tell us we are doing a good job:
-->
&lt;p>我们询问受访者如何从整体上改进 Kubernetes 文档。一些人抓住这次机会告诉我们我们正在做一个很棒的
工作：&lt;/p>
&lt;!--
```text
- For me, it is the best documented open source project.
- Keep going!
- I find the documentation to be excellent.
- You [are] doing a great job. For real.
```
-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- 对我而言，这是我见过的文档最好的开源项目。
- 继续努力！
- 我觉得文档很好。
- 你们做得真好。真的。
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Other respondents provided feedback on the content:
-->
&lt;p>其它受访者提供关于内容的反馈：&lt;/p>
&lt;!--
```text
- ...But since we're talking about docs, more is always better. More
advanced configuration examples would be, to me, the way to go. Like a Use Case page for each configuration topic with beginner to advanced example scenarios. Something like that would be
awesome....
-->
&lt;!--
- More in-depth examples and use cases would be great. I often feel that the Kubernetes documentation scratches the surface of a topic, which might be great for new users, but it leaves more experienced users without much "official" guidance on how to implement certain things.
-->
&lt;!--
- More production like examples in the resource sections (notably secrets) or links to production like examples
-->
&lt;!--
- It would be great to see a very clear "Quick Start" A->Z up and running like many other tech projects. There are a handful of almost-quick-starts, but no single guidance. The result is information overkill.
```
-->
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- ...但既然我们谈论的是文档，多多益善。更多的高级配置示例对我来说将是最好的选择。比如每个配置主题的用例页面，
从初学者到高级示例场景。像这样的东西真的是令人惊叹......
- 更深入的例子和用例将是很好的。我经常感觉 Kubernetes 文档只是触及了一个主题的表面，这可能对新用户很好，
但是它没有让更有经验的用户获取多少关于如何实现某些东西的“官方”指导。
- 资源节（特别是 secrets）希望有更多类似于产品的示例或指向类似产品的示例的链接
- 如果能像很多其它技术项目那样有非常清晰的“快速启动” 逐步教学完成搭建就更好了。现有的快速入门内容屈指可数，
也没有统一的指南。结果是信息泛滥。
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
A few respondents provided technical suggestions:
```text
- Make table columns sortable and filterable using a ReactJS or Angular component.
-->
&lt;!--
- For most, I think creating documentation with Hugo - a system for static site generation - is not appropriate. There are better systems for documenting large software project.
-->
&lt;!--
Specifically, I would like to see k8s switch to Sphinx for documentation. It has an excellent built-in search, it is easy tolearn if you know markdown, it is widely adopted by other projects (e.g. every software project in readthedocs.io, linux kernel, docs.python.org etc).
```
-->
&lt;p>少数受访者提供的技术建议：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-text" data-lang="text">- 使用 ReactJS 或者 Angular component 使表的列可排序和可筛选。
- 对于大多数人来说，我认为用 Hugo - 一个静态站点生成系统 - 创建文档是不合适的。有更好的系统来记录大型软件项目。
具体来说，我希望看到 k8s 切换到 Sphinx 来获取文档。Sphinx 有一个很好的内置搜索。如果你了解 markdown，学习起来也很容易。
Sphinx 被其他项目广泛采用（例如，在 readthedocs.io、linux kernel、docs.python.org 等等）。
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Overall, respondents provided constructive criticism focusing on the need for
advanced use cases as well as more in-depth examples, guides, and walkthroughs.
-->
&lt;p>总体而言，受访者提供了建设性的批评，其关注点是高级用例以及更深入的示例、指南和演练。&lt;/p>
&lt;!--
# Where to see more
-->
&lt;h1 id="哪里可以看到更多">哪里可以看到更多&lt;/h1>
&lt;!--
Survey results summary, charts, and raw data are available in `kubernetes/community` sig-docs [survey](https://github.com/kubernetes/community/tree/master/sig-docs/survey) directory.
-->
&lt;p>调查结果摘要、图表和原始数据可在 &lt;code>kubernetes/community&lt;/code> sig-docs
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-docs/survey">survey&lt;/a>
目录下。&lt;/p></description></item><item><title>Blog: 圣迭戈贡献者峰会日程公布！</title><link>https://kubernetes.io/zh/blog/2019/10/10/contributor-summit-san-diego-schedule/</link><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/10/10/contributor-summit-san-diego-schedule/</guid><description>
&lt;!--
layout: blog
title: "Contributor Summit San Diego Schedule Announced!"
date: 2019-10-10
slug: contributor-summit-san-diego-schedule
-->
&lt;!--
Authors: Josh Berkus (Red Hat), Paris Pittman (Google), Jonas Rosland (VMware)
-->
&lt;p>作者：Josh Berkus (Red Hat), Paris Pittman (Google), Jonas Rosland (VMware)&lt;/p>
&lt;!--
tl;dr A week ago we announced that [registration is open][reg] for the contributor
summit , and we're now live with [the full Contributor Summit schedule!][schedule]
Grab your spot while tickets are still available. There is currently a waitlist
for new contributor workshop. ([Register here!][reg])
-->
&lt;p>一周前，我们宣布贡献者峰会&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">开放注册&lt;/a>，现在我们已经完成了整个&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/program/schedule/">贡献者峰会的日程安排&lt;/a>！趁现在还有票，马上抢占你的位置。这里有一个新贡献者研讨会的等待名单。 (&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">点击这里注册!&lt;/a>)&lt;/p>
&lt;!--
There are many great sessions planned for the Contributor Summit, spread across
five rooms of current contributor content in addition to the new contributor
workshops. Since this is an upstream contributor summit and we don't often meet,
being a globally distributed team, most of these sessions are discussions or
hands-on labs, not just presentations. We want folks to learn and have a
good time meeting their OSS teammates.
-->
&lt;p>除了新贡献者研讨会之外，贡献者峰会还安排了许多精彩的会议，这些会议分布在当前五个贡献者内容的会议室中。由于这是一个上游贡献者峰会，并且我们不经常见面，所以作为一个全球分布的团队，这些会议大多是讨论或动手实践，而不仅仅是演示。我们希望大家互相学习，并于他们的开源代码队友玩的开心。&lt;/p>
&lt;!--
Unconference tracks are returning from last year with sessions to be chosen
Monday morning. These are ideal for the latest hot topics and specific
discussions that contributors want to have. In previous years, we've covered
flaky tests, cluster lifecycle, KEPs (Kubernetes Enhancement Proposals), mentoring,
security, and more.
-->
&lt;p>像去年一样，非组织会议将重新开始，会议将在周一上午进行选择。对于最新的热门话题和贡献者想要进行的特定讨论，这是理想的选择。在过去的几年中，我们涵盖了不稳定的测试，集群生命周期，KEP（Kubernetes增强建议），指导，安全性等等。&lt;/p>
&lt;!--
![Unconference](/images/blog/2019-10-10-contributor-summit-san-diego-schedule/DSCF0806.jpg)
-->
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-10-10-contributor-summit-san-diego-schedule/DSCF0806.jpg" alt="非组织会议">&lt;/p>
&lt;!--
While the schedule contains difficult decisions in every timeslot, we've picked
a few below to give you a taste of what you'll hear, see, and participate in, at
the summit:
-->
&lt;p>尽管在每个时间间隙日程安排都包含困难的决定，但我们选择了以下几点，让您体验一下您将在峰会上听到、看到和参与的内容：&lt;/p>
&lt;!--
* **[Vision]**: SIG-Architecture will be sharing their vision of where we're going
with Kubernetes development for the next year and beyond.
* **[Security]**: Tim Allclair and CJ Cullen will present on the current state of
Kubernetes security. In another security talk, Vallery Lancey will lead a
discussion about making our platform secure by default.
* **[Prow]**: Interested in working with Prow and contributing to Test-Infra, but
not sure where to start? Rob Keilty will help you get a Prow test environment
running on your laptop.
* **[Git]**: Staff from GitHub will be collaborating with Christoph Blecker to share
practical Git tips for Kubernetes contributors.
* **[Reviewing]**: Tim Hockin will share the secrets of becoming a great code
reviewer, and Jordan Liggitt will conduct a live API review so that you can do
one, or at least pass one.
* **[End Users]**: Several end users from the CNCF partner ecosystem, invited by
Cheryl Hung, will hold a Q&amp;A with contributors to strengthen our feedback loop.
* **[Docs]**: As always, SIG-Docs will run a three-hour contributing-to-documentation
workshop.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvMc">预见&lt;/a>&lt;/strong>: SIG组织将分享他们对于明年和以后Kubernetes开发发展方向的认识。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvMj">安全&lt;/a>&lt;/strong>: Tim Allclair和CJ Cullen将介绍Kubernetes安全的当前情况。在另一个安全性演讲中，Vallery Lancey将主持有关使我们的平台默认情况下安全的讨论。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/Vv6Z">Prow&lt;/a>&lt;/strong>: 有兴趣与Prow合作并为Test-Infra做贡献，但不确定从哪里开始？ Rob Keilty将帮助您在笔记本电脑上运行Prow测试环境&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvNa">Git&lt;/a>&lt;/strong>: GitHub的员工将与Christoph Blecker合作，为Kubernetes贡献者分享实用的Git技巧。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VutA">审阅&lt;/a>&lt;/strong>: 蒂姆·霍金（Tim Hockin）将分享成为一名出色的代码审阅者的秘密，而乔丹·利吉特（Jordan Liggitt）将进行实时API审阅，以便您可以进行一次或至少了解一次审阅。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvNJ">终端用户&lt;/a>&lt;/strong>: 应Cheryl Hung邀请，来自CNCF合作伙伴生态的数个终端用户，将回答贡献者的问题，以加强我们的反馈循环。&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/Vux2">文档&lt;/a>&lt;/strong>: 与往常一样，SIG-Docs将举办一个为时三个小时的文档撰写研讨会。&lt;/li>
&lt;/ul>
&lt;!--
We're also giving out awards to contributors who distinguished themselves in 2019,
and there will be a huge Meet &amp; Greet for new contributors to find their SIG
(and for existing contributors to ask about their PRs) at the end of the day on
Monday.
-->
&lt;p>我们还将向在2019年杰出的贡献者颁发奖项，周一星期一结束时将有一个巨大的见面会，供新的贡献者找到他们的SIG（以及现有的贡献者询问他们的PR）。&lt;/p>
&lt;!--
Hope to see you all there, and [make sure you register!][reg]
-->
&lt;p>希望能够在峰会上见到您，并且确保您已经提前&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">注册&lt;/a>！&lt;/p>
&lt;!--
[San Diego team][team]
-->
&lt;p>&lt;a href="http://git.k8s.io/community/events/events-team">圣迭戈团队&lt;/a>&lt;/p></description></item><item><title>Blog: 2019 指导委员会选举结果</title><link>https://kubernetes.io/zh/blog/2019/10/03/2019-steering-committee-election-results/</link><pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/10/03/2019-steering-committee-election-results/</guid><description>
&lt;!--
---
layout: blog
title: "2019 Steering Committee Election Results"
date: 2019-10-03
slug: 2019-steering-committee-election-results
---
-->
&lt;!--
**Authors**: Bob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)
-->
&lt;p>&lt;strong>作者&lt;/strong>：Bob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)&lt;/p>
&lt;!--
The [2019 Steering Committee Election] is a landmark milestone for the
Kubernetes project. The initial bootstrap committee is graduating to emeritus
and the committee has now shrunk to its final allocation of seven seats. All
members of the Steering Committee are now fully elected by the Kubernetes
Community.
-->
&lt;p>&lt;a href="https://git.k8s.io/community/events/elections/2021">2019 指导委员会选举&lt;/a> 是 Kubernetes 项目的重要里程碑。最初的自助委员会正逐步退休，现在该委员会已缩减到最后分配的 7 个席位。指导委员会的所有成员现在都由 Kubernetes 社区选举产生。&lt;/p>
&lt;!--
Moving forward elections will elect either 3 or 4 people to the committee for
two-year terms.
-->
&lt;p>接下来的选举将选出 3 到 4 名委员，任期两年。&lt;/p>
&lt;!--
## **Results**
The Kubernetes Steering Committee Election is now complete and the following
candidates came ahead to secure two-year terms that start immediately
(in alphabetical order by GitHub handle):
-->
&lt;h2 id="选举结果">选举结果&lt;/h2>
&lt;p>Kubernetes 指导委员会选举现已完成，以下候选人提前获得立即开始的两年任期 (按 GitHub handle 的字母顺序排列) ：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Christoph Blecker (&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>), Red Hat&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>), Red Hat&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>), Loodse&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Paris Pittman (&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>)&lt;/strong>, &lt;strong>Google&lt;/strong>&lt;/li>
&lt;/ul>
&lt;!--
They join Aaron Crickenberger ([@spiffxp]), Google; Davanum Srinivas ([@dims]),
VMware; and Timothy St. Clair ([@timothysc]), VMware, to round out the committee.
The seats held by Aaron, Davanum, and Timothy will be up for election around
this time next year.
-->
&lt;p>他们加入了 Aaron Crickenberger (&lt;a href="https://github.com/spiffxp">@spiffxp&lt;/a>)， Google；Davanum Srinivas (&lt;a href="https://github.com/dims">@dims&lt;/a>)，VMware; and Timothy St. Clair (&lt;a href="https://github.com/timothysc">@timothysc&lt;/a>), VMware，使得委员会更圆满。Aaron、Davanum 和 Timothy 占据的这些席位将会在明年的这个时候进行选举。&lt;/p>
&lt;!--
## Big Thanks!
* Thanks to the initial bootstrap committee for establishing the initial
project governance and overseeing a multi-year transition period:
-->
&lt;h2 id="诚挚的感谢">诚挚的感谢！&lt;/h2>
&lt;ul>
&lt;li>感谢最初的引导委员会创立了最初项目的管理并监督了多年的过渡期：
&lt;ul>
&lt;li>Joe Beda (&lt;a href="https://github.com/jbeda">@jbeda&lt;/a>), VMware&lt;/li>
&lt;li>Brendan Burns (&lt;a href="https://github.com/brendandburns">@brendandburns&lt;/a>), Microsoft&lt;/li>
&lt;li>Clayton Coleman (&lt;a href="https://github.com/smarterclayton">@smarterclayton&lt;/a>), Red Hat&lt;/li>
&lt;li>Brian Grant (&lt;a href="https://github.com/bgrant0607">@bgrant0607&lt;/a>), Google&lt;/li>
&lt;li>Tim Hockin (&lt;a href="https://github.com/thockin">@thockin&lt;/a>), Google&lt;/li>
&lt;li>Sarah Novotny (&lt;a href="https://github.com/sarahnovotny">@sarahnovotny&lt;/a>), Microsoft&lt;/li>
&lt;li>Brandon Philips (&lt;a href="https://github.com/philips">@philips&lt;/a>), Red Hat&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* And also thanks to the other Emeritus Steering Committee Members. Your
prior service is appreciated by the community:
-->
&lt;ul>
&lt;li>同样感谢其他的已退休指导委员会成员。社区对你们先前的服务表示赞赏：
&lt;ul>
&lt;li>Quinton Hoole (&lt;a href="https://github.com/quinton-hoole">@quinton-hoole&lt;/a>), Huawei&lt;/li>
&lt;li>Michelle Noorali (&lt;a href="https://github.com/michelleN">@michelleN&lt;/a>), Microsoft&lt;/li>
&lt;li>Phillip Wittrock (&lt;a href="https://github.com/pwittrock">@pwittrock&lt;/a>), Google&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* Thanks to the candidates that came forward to run for election. May we always
have a strong set of people who want to push the community forward like yours
in every election.
* Thanks to all 377 voters who cast a ballot.
* And last but not least…Thanks to Cornell University for hosting [CIVS]!
-->
&lt;ul>
&lt;li>感谢参选的候选人。 愿在每次选举中，我们都能拥有一群像您一样推动社区向前发展的人。&lt;/li>
&lt;li>感谢所有投票的377位选民。&lt;/li>
&lt;li>最后，感谢康奈尔大学举办的 &lt;a href="https://civs.cs.cornell.edu/">CIVS&lt;/a>!&lt;/li>
&lt;/ul>
&lt;!--
## Get Involved with the Steering Committee
You can follow along with Steering Committee [backlog items] and weigh in by
filing an issue or creating a PR against their [repo]. They meet bi-weekly on
[Wednesdays at 8pm UTC] and regularly attend Meet Our Contributors. They can
also be contacted at their public mailing list [steering@kubernetes.io].
Steering Committee Meetings:
-->
&lt;h2 id="参与指导委员会">参与指导委员会&lt;/h2>
&lt;p>你可以跟进指导委员会的 &lt;a href="https://github.com/kubernetes/steering/projects/1">代办事项&lt;/a>，通过提出问题或者向 &lt;a href="https://github.com/kubernetes/steering">仓库&lt;/a> 提交一个 pr 。他们每两周一次，在 &lt;a href="https://github.com/kubernetes/steering">UTC 时间周三晚上 8 点&lt;/a> 会面，并定期与我们的贡献者见面。也可以通过他们的公共邮件列表 &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a> 联系他们。&lt;/p>
&lt;p>指导委员会会议：&lt;/p>
&lt;!--
* [YouTube Playlist]
-->
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube 播放列表&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: San Diego 贡献者峰会开放注册！</title><link>https://kubernetes.io/zh/blog/2019/09/24/san-diego-contributor-summit/</link><pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/09/24/san-diego-contributor-summit/</guid><description>
&lt;!--
---
layout: blog
title: "Contributor Summit San Diego Registration Open!"
date: 2019-09-24
slug: san-diego-contributor-summit
---
--->
&lt;!--
**Authors: Paris Pittman (Google), Jeffrey Sica (Red Hat), Jonas Rosland (VMware)**
--->
&lt;p>&lt;strong>作者：Paris Pittman (Google), Jeffrey Sica (Red Hat), Jonas Rosland (VMware)&lt;/strong>&lt;/p>
&lt;!--
[Contributor Summit San Diego 2019 Event Page]
Registration is now open and in record time, we’ve hit capacity for the
*new contributor workshop* session of the event! Waitlist is now available.
--->
&lt;p>&lt;a href="https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/">2019 San Diego 贡献者峰会活动页面&lt;/a>
注册已经开放，并且在创纪录的时间内，&lt;em>新贡献者研讨会&lt;/em> 活动已满员！候补名单已经开放。&lt;/p>
&lt;!--
**Sunday, November 17**
Evening Contributor Celebration:
[QuartYard]*
Address: 1301 Market Street, San Diego, CA 92101
Time: 6:00PM - 9:00PM
--->
&lt;p>&lt;strong>11月17日，星期日&lt;/strong>&lt;br>
晚间贡献者庆典：&lt;br>
&lt;a href="https://quartyardsd.com/">QuartYard&lt;/a>*&lt;br>
地址: 1301 Market Street, San Diego, CA 92101&lt;br>
时间: 下午6:00 - 下午9:00&lt;/p>
&lt;!--
**Monday, November 18**
All Day Contributor Summit:
[Marriott Marquis San Diego Marina]
Address: 333 W Harbor Dr, San Diego, CA 92101
Time: 9:00AM - 5:00PM
--->
&lt;p>&lt;strong>11月18日，星期一&lt;/strong>&lt;br>
全天贡献者峰会：&lt;br>
&lt;a href="https://www.marriott.com/hotels/travel/sandt-marriott-marquis-san-diego-marina/?scid=bb1a189a-fec3-4d19-a255-54ba596febe2">Marriott Marquis San Diego Marina&lt;/a>&lt;br>
地址: 333 W Harbor Dr, San Diego, CA 92101&lt;br>
时间: 上午9:00 - 下午5:00&lt;/p>
&lt;!--
While the Kubernetes project is only five years old, we’re already going into our
9th Contributor Summit this November in San Diego before KubeCon + CloudNativeCon.
The rapid increase is thanks to adding European and Asian Contributor Summits to
the North American events we’ve done previously. We will continue to run Contributor
Summits across the globe, as it is important that our contributor base grows in
all forms of diversity.
--->
&lt;p>虽然 Kubernetes 项目只有五年的历史，但是在 KubeCon + CloudNativeCon 之前，今年11月在圣迭戈时我们举办的第九届贡献者峰会了。快速增长的原因是在我们之前所做的北美活动中增加了欧洲和亚洲贡献者峰会。我们将继续在全球举办贡献者峰会，因为重要的是，我们的贡献者要以各种形式的多样性地成长。&lt;/p>
&lt;!--
Kubernetes has a large distributed remote contributing team, from [individuals and
organizations] all over the world. The Contributor Summits give the community three
chances a year to get together, work on community topics, and have hallway track
time. The upcoming San Diego summit is expected to bring over 450 attendees, and
will contain multiple tracks with something for everyone. The focus will be around
contributor growth and sustainability. We're going to stop here with capacity for
future summits; we want this event to offer value to individuals and the project.
We've heard from past summit attendee feedback that getting work done, learning,
and meeting folks face to face is a priority. By capping attendance and offering
the contributor gatherings in more locations, it will help us achieve those goals.
--->
&lt;p>Kubernetes 拥有一个庞大的分布式远程贡献团队，由来自世界各地的 &lt;a href="https://k8s.devstats.cncf.io/d/8/company-statistics-by-repository-group?orgId=1&amp;amp;var-period=y&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All&amp;amp;var-companies=All">个人和组织&lt;/a> 组成。贡献者峰会每年为社区提供三次聚会的机会，围绕社区主题开展工作，并有互相了解的时间。即将举行的 San Diego 峰会预计将吸引 450 多名与会者，并将包含多个方向，适合所有人。重点将围绕贡献者的增长和可持续性。我们将在这里停留，为举行未来峰会做准备；我们希望这次活动为个人和项目提供价值。我们已经从峰会与会者的反馈中得知，完成工作、学习和与人们面对面交流是当务之急。通过限制参加人数并在更多地方提供贡献者聚会，将有助于我们实现这些目标。&lt;/p>
&lt;!--
This summit is unique as we’ve taken big moves on sustaining ourselves, the
contributor experience events team. Taking a page from the release team’s playbook,
we have added additional core team and shadow roles making it a natural mentoring
(watching+doing) relationship. The shadows are expected to fill another role at
one of the three events in 2020, and core team members to take the lead.
In preparation for this team, we’ve open sourced our [rolebooks, guidelines,
best practices] and opened up our [meetings] and [project board]. Our team makes up
many parts of the Kubernetes project and takes care of making sure all voices
are represented.
--->
&lt;p>这次峰会是独一无二的，因为我们在贡献者体验活动团队的坚持自我方面已采取了重大举措。从发行团队的手册中摘录，我们添加了其他核心团队和跟随学习角色，使其成为天然的辅导关系（包括监督和实施）。预计跟随学习者将在 2020 年的三项活动中扮演另一个角色，核心团队成员将带头完成。为这个团队做准备，我们开源了 &lt;a href="https://github.com/kubernetes/community/tree/master/events/events-team">技术手册，指南，最佳做法&lt;/a>，并开放了 &lt;a href="https://docs.google.com/document/d/1oLXv5_rM4f645jlXym_Vd7AUq7x6DV-O87E6tcW1sjU/edit?usp=sharing">会议&lt;/a> 和 &lt;a href="https://github.com/orgs/kubernetes/projects/21">项目委员会&lt;/a>。我们的团队组成了 Kubernetes 项目的许多部分，并确保所有声音都得到体现。&lt;/p>
&lt;!--
Are you at KubeCon + CloudNativeCon but can’t make it to the summit? Check out
the [SIG Intro and Deep Dive sessions] during KubeCon + CloudNativeCon to
participate in Q&amp;A and hear what’s up with each Special interest Group (SIG).
We’ll also record all of Contributor Summit’s presentation sessions, take notes
in discussions, and share it back with you, after the event is complete.
--->
&lt;p>您是否已经在 KubeCon + CloudNativeCon 上，但无法参与会议？ 在 KubeCon + CloudNativeCon 期间查看 &lt;a href="https://kccncna19.sched.com/overview/type/Maintainer+Track+Sessions?iframe=yes">SIG入门和深潜课程&lt;/a> 参与问答，并听取每个特殊兴趣小组（SIG）的最新消息。活动结束后，我们还将记录所有贡献者峰会的课题，在讨论中做笔记，并与您分享。&lt;/p>
&lt;!--
We hope to see you all at Kubernetes Contributor Summit San Diego, make sure you
head over and [register right now]! This event will sell out - here’s your warning.
:smiley:
--->
&lt;p>我们希望能在 San Diego 的 Kubernetes 贡献者峰会上与大家见面，确保您直接进入并点击 &lt;a href="https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/">立即注册&lt;/a>！ 此活动将关闭 - 特此提醒。 ：笑脸：&lt;/p>
&lt;!--
Check out past blogs on [persona building around our events] and the [Barcelona summit story].
![Group Picture in 2018](/images/blog/2019-09-24-san-diego-contributor-summit/IMG_2588.JPG)
--->
&lt;p>查看往期博客有关 &lt;a href="https://kubernetes.io/blog/2019/03/20/a-look-back-and-whats-in-store-for-kubernetes-contributor-summits/">围绕我们的活动构建角色&lt;/a> 和 &lt;a href="https://kubernetes.io/blog/2019/06/25/recap-of-kubernetes-contributor-summit-barcelona-2019/">巴塞罗那峰会故事&lt;/a>。&lt;/p>
&lt;p>！&lt;a href="https://kubernetes.io/images/blog/2019-09-24-san-diego-contributor-summit/IMG_2588.JPG">2018年集体照&lt;/a>&lt;/p>
&lt;!--
*=QuartYard has a huge stage! Want to perform something in front of your contributor peers? Reach out to us! community@kubernetes.io
--->
&lt;p>*=QuartYard 有一个巨大的舞台！想要在您的贡献者同行面前做点什么？加入我们吧！ &lt;a href="mailto:community@kubernetes.io">community@kubernetes.io&lt;/a>&lt;/p></description></item><item><title>Blog: OPA Gatekeeper：Kubernetes 的策略和管理</title><link>https://kubernetes.io/zh/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</link><pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</guid><description>
&lt;!--
---
layout: blog
title: "OPA Gatekeeper: Policy and Governance for Kubernetes"
date: 2019-08-06
slug: OPA-Gatekeeper-Policy-and-Governance-for-Kubernetes
---
--->
&lt;!--
**Authors:** Rita Zhang (Microsoft), Max Smythe (Google), Craig Hooper (Commonwealth Bank AU), Tim Hinrichs (Styra), Lachie Evenson (Microsoft), Torin Sandall (Styra)
--->
&lt;p>&lt;strong>作者：&lt;/strong> Rita Zhang (Microsoft), Max Smythe (Google), Craig Hooper (Commonwealth Bank AU), Tim Hinrichs (Styra), Lachie Evenson (Microsoft), Torin Sandall (Styra)&lt;/p>
&lt;!--
The [Open Policy Agent Gatekeeper](https://github.com/open-policy-agent/gatekeeper) project can be leveraged to help enforce policies and strengthen governance in your Kubernetes environment. In this post, we will walk through the goals, history, and current state of the project.
--->
&lt;p>可以从项目 &lt;a href="https://github.com/open-policy-agent/gatekeeper">Open Policy Agent Gatekeeper&lt;/a> 中获得帮助，在 Kubernetes 环境下实施策略并加强治理。在本文中，我们将逐步介绍该项目的目标，历史和当前状态。&lt;/p>
&lt;!--
The following recordings from the Kubecon EU 2019 sessions are a great starting place in working with Gatekeeper:
* [Intro: Open Policy Agent Gatekeeper](https://youtu.be/Yup1FUc2Qn0)
* [Deep Dive: Open Policy Agent](https://youtu.be/n94_FNhuzy4)
--->
&lt;p>以下是 Kubecon EU 2019 会议的录音，帮助我们更好地开展与 Gatekeeper 合作：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/Yup1FUc2Qn0">简介：开放策略代理 Gatekeeper&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/n94_FNhuzy4">深入研究：开放策略代理&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Motivations
If your organization has been operating Kubernetes, you probably have been looking for ways to control what end-users can do on the cluster and ways to ensure that clusters are in compliance with company policies. These policies may be there to meet governance and legal requirements or to enforce best practices and organizational conventions. With Kubernetes, how do you ensure compliance without sacrificing development agility and operational independence?
--->
&lt;h2 id="出发点">出发点&lt;/h2>
&lt;p>如果您所在的组织一直在使用 Kubernetes，您可能一直在寻找如何控制终端用户在集群上的行为，以及如何确保集群符合公司政策。这些策略可能需要满足管理和法律要求，或者符合最佳执行方法和组织惯例。使用 Kubernetes，如何在不牺牲开发敏捷性和运营独立性的前提下确保合规性？&lt;/p>
&lt;!--
For example, you can enforce policies like:
* All images must be from approved repositories
* All ingress hostnames must be globally unique
* All pods must have resource limits
* All namespaces must have a label that lists a point-of-contact
--->
&lt;p>例如，您可以执行以下策略：&lt;/p>
&lt;ul>
&lt;li>所有镜像必须来自获得批准的存储库&lt;/li>
&lt;li>所有入口主机名必须是全局唯一的&lt;/li>
&lt;li>所有 Pod 必须有资源限制&lt;/li>
&lt;li>所有命名空间都必须具有列出联系的标签&lt;/li>
&lt;/ul>
&lt;!--
Kubernetes allows decoupling policy decisions from the API server by means of [admission controller webhooks](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) to intercept admission requests before they are persisted as objects in Kubernetes. [Gatekeeper](https://github.com/open-policy-agent/gatekeeper) was created to enable users to customize admission control via configuration, not code and to bring awareness of the cluster’s state, not just the single object under evaluation at admission time. Gatekeeper is a customizable admission webhook for Kubernetes that enforces policies executed by the [Open Policy Agent (OPA)](https://www.openpolicyagent.org), a policy engine for Cloud Native environments hosted by CNCF.
--->
&lt;p>在接收请求被持久化为 Kubernetes 中的对象之前，Kubernetes 允许通过 &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">admission controller webhooks&lt;/a> 将策略决策与 API 服务器分离，从而拦截这些请求。&lt;a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper&lt;/a> 创建的目的是使用户能够通过配置（而不是代码）自定义控制许可，并使用户了解群集的状态，而不仅仅是针对评估状态的单个对象，在这些对象准许加入的时候。Gatekeeper 是 Kubernetes 的一个可定制的许可 webhook ，它由 &lt;a href="https://www.openpolicyagent.org">Open Policy Agent (OPA)&lt;/a> 强制执行， OPA 是 Cloud Native 环境下的策略引擎，由 CNCF 主办。&lt;/p>
&lt;!--
## Evolution
Before we dive into the current state of Gatekeeper, let’s take a look at how the Gatekeeper project has evolved.
--->
&lt;h2 id="发展">发展&lt;/h2>
&lt;p>在深入了解 Gatekeeper 的当前情况之前，让我们看一下 Gatekeeper 项目是如何发展的。&lt;/p>
&lt;!--
* Gatekeeper v1.0 - Uses OPA as the admission controller with the kube-mgmt sidecar enforcing configmap-based policies. It provides validating and mutating admission control. Donated by Styra.
* Gatekeeper v2.0 - Uses Kubernetes policy controller as the admission controller with OPA and kube-mgmt sidecars enforcing configmap-based policies. It provides validating and mutating admission control and audit functionality. Donated by Microsoft.
* Gatekeeper v3.0 - The admission controller is integrated with the [OPA Constraint Framework](https://github.com/open-policy-agent/frameworks/tree/master/constraint) to enforce CRD-based policies and allow declaratively configured policies to be reliably shareable. Built with kubebuilder, it provides validating and, eventually, mutating (to be implemented) admission control and audit functionality. This enables the creation of policy templates for [Rego](https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/) policies, creation of policies as CRDs, and storage of audit results on policy CRDs. This project is a collaboration between Google, Microsoft, Red Hat, and Styra.
--->
&lt;ul>
&lt;li>Gatekeeper v1.0 - 使用 OPA 作为带有 kube-mgmt sidecar 的许可控制器，用来强制执行基于 configmap 的策略。这种方法实现了验证和转换许可控制。贡献方：Styra&lt;/li>
&lt;li>Gatekeeper v2.0 - 使用 Kubernetes 策略控制器作为许可控制器，OPA 和 kube-mgmt sidecar 实施基于 configmap 的策略。这种方法实现了验证和转换准入控制和审核功能。贡献方：Microsoft&lt;/li>
&lt;li>Gatekeeper v3.0 - 准入控制器与 &lt;a href="https://github.com/open-policy-agent/frameworks/tree/master/constraint">OPA Constraint Framework&lt;/a> 集成在一起，用来实施基于 CRD 的策略，并可以可靠地共享已完成声明配置的策略。使用 kubebuilder 进行构建，实现了验证以及最终转换（待完成）为许可控制和审核功能。这样就可以为 &lt;a href="https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/">Rego&lt;/a> 策略创建策略模板，将策略创建为 CRD 并存储审核结果到策略 CRD 上。该项目是 Google，Microsoft，Red Hat 和 Styra 合作完成的。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-08-06-opa-gatekeeper/v3.png" alt="">&lt;/p>
&lt;!--
## Gatekeeper v3.0 Features
Now let’s take a closer look at the current state of Gatekeeper and how you can leverage all the latest features. Consider an organization that wants to ensure all objects in a cluster have departmental information provided as part of the object’s labels. How can you do this with Gatekeeper?
--->
&lt;h2 id="gatekeeper-v3-0-的功能">Gatekeeper v3.0 的功能&lt;/h2>
&lt;p>现在我们详细看一下 Gatekeeper 当前的状态，以及如何利用所有最新的功能。假设一个组织希望确保集群中的所有对象都有 department 信息，这些信息是对象标签的一部分。如何利用 Gatekeeper 完成这项需求？&lt;/p>
&lt;!--
### Validating Admission Control
Once all the Gatekeeper components have been [installed](https://github.com/open-policy-agent/gatekeeper) in your cluster, the API server will trigger the Gatekeeper admission webhook to process the admission request whenever a resource in the cluster is created, updated, or deleted.
During the validation process, Gatekeeper acts as a bridge between the API server and OPA. The API server will enforce all policies executed by OPA.
--->
&lt;h3 id="验证许可控制">验证许可控制&lt;/h3>
&lt;p>在集群中所有 Gatekeeper 组件都 &lt;a href="https://github.com/open-policy-agent/gatekeeper">安装&lt;/a> 完成之后，只要集群中的资源进行创建、更新或删除，API 服务器将触发 Gatekeeper 准入 webhook 来处理准入请求。&lt;/p>
&lt;p>在验证过程中，Gatekeeper 充当 API 服务器和 OPA 之间的桥梁。API 服务器将强制实施 OPA 执行的所有策略。&lt;/p>
&lt;!--
### Policies and Constraints
With the integration of the OPA Constraint Framework, a Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected.
--->
&lt;h3 id="策略与-constraint">策略与 Constraint&lt;/h3>
&lt;p>结合 OPA Constraint Framework，Constraint 是一个声明，表示作者希望系统满足给定的一系列要求。Constraint 都使用 Rego 编写，Rego 是声明性查询语言，OPA 用 Rego 来枚举违背系统预期状态的数据实例。所有 Constraint 都遵循逻辑 AND。假使有一个 Constraint 不满足，那么整个请求都将被拒绝。&lt;/p>
&lt;!--
Before defining a Constraint, you need to create a Constraint Template that allows people to declare new Constraints. Each template describes both the Rego logic that enforces the Constraint and the schema for the Constraint, which includes the schema of the CRD and the parameters that can be passed into a Constraint, much like arguments to a function.
For example, here is a Constraint template CRD that requires certain labels to be present on an arbitrary object.
--->
&lt;p>在定义 Constraint 之前，您需要创建一个 Constraint Template，允许大家声明新的 Constraint。每个模板都描述了强制执行 Constraint 的 Rego 逻辑和 Constraint 的模式，其中包括 CRD 的模式和传递到 enforces 中的参数，就像函数的参数一样。&lt;/p>
&lt;p>例如，以下是一个 Constraint 模板 CRD，它的请求是在任意对象上显示某些标签。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>templates.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ConstraintTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8srequiredlabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">crd&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">names&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">listKind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabelsList&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">plural&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8srequiredlabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">singular&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8srequiredlabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">validation&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Schema for the `parameters` field&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>array&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">items&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>string&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">targets&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">target&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admission.k8s.gatekeeper.sh&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rego&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>|&lt;span style="color:#b44;font-style:italic">
&lt;/span>&lt;span style="color:#b44;font-style:italic"> package k8srequiredlabels
&lt;/span>&lt;span style="color:#b44;font-style:italic">
&lt;/span>&lt;span style="color:#b44;font-style:italic"> deny[{&amp;#34;msg&amp;#34;: msg, &amp;#34;details&amp;#34;: {&amp;#34;missing_labels&amp;#34;: missing}}] {
&lt;/span>&lt;span style="color:#b44;font-style:italic"> provided := {label | input.review.object.metadata.labels[label]}
&lt;/span>&lt;span style="color:#b44;font-style:italic"> required := {label | label := input.parameters.labels[_]}
&lt;/span>&lt;span style="color:#b44;font-style:italic"> missing := required - provided
&lt;/span>&lt;span style="color:#b44;font-style:italic"> count(missing) &amp;gt; 0
&lt;/span>&lt;span style="color:#b44;font-style:italic"> msg := sprintf(&amp;#34;you must provide labels: %v&amp;#34;, [missing])
&lt;/span>&lt;span style="color:#b44;font-style:italic"> }&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Once a Constraint template has been deployed in the cluster, an admin can now create individual Constraint CRDs as defined by the Constraint template. For example, here is a Constraint CRD that requires the label `hr` to be present on all namespaces.
--->
&lt;p>在集群中部署了 Constraint 模板后，管理员现在可以创建由 Constraint 模板定义的单个 Constraint CRD。例如，这里以下是一个 Constraint CRD，要求标签 &lt;code>hr&lt;/code> 出现在所有命名空间上。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>constraints.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns-must-have-hr&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">match&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Similarly, another Constraint CRD that requires the label `finance` to be present on all namespaces can easily be created from the same Constraint template.
--->
&lt;p>类似地，可以从同一个 Constraint 模板轻松地创建另一个 Constraint CRD，该 Constraint CRD 要求所有命名空间上都有 &lt;code>finance&lt;/code> 标签。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>constraints.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns-must-have-finance&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">match&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;finance&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
As you can see, with the Constraint framework, we can reliably share Regos via the Constraint templates, define the scope of enforcement with the match field, and provide user-defined parameters to the Constraints to create customized behavior for each Constraint.
--->
&lt;p>如您所见，使用 Constraint framework，我们可以通过 Constraint 模板可靠地共享 rego，使用匹配字段定义执行范围，并为 Constraint 提供用户定义的参数，从而为每个 Constraint 创建自定义行为。&lt;/p>
&lt;!--
### Audit
The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as `violations` listed in the `status` field of the relevant Constraint. --->
&lt;h3 id="审核">审核&lt;/h3>
&lt;p>根据群集中强制执行的 Constraint，审核功能可定期评估复制的资源，并检测先前存在的错误配置。Gatekeeper 将审核结果存储为 &lt;code>violations&lt;/code>，在相关 Constraint 的 &lt;code>status&lt;/code> 字段中列出。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>constraints.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns-must-have-hr&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">match&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">auditTimestamp&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2019-08-06T01:46:13Z&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">byPod&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">enforced&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">id&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gatekeeper-controller-manager-0&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">violations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">enforcementAction&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>deny&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Namespace&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">message: &amp;#39;you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> name: default
&lt;/span>&lt;span style="color:#b44"> - enforcementAction: deny
&lt;/span>&lt;span style="color:#b44"> kind: Namespace
&lt;/span>&lt;span style="color:#b44"> message: &amp;#39;&lt;/span>&lt;span style="color:#008000;font-weight:bold">you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> name: gatekeeper-system
&lt;/span>&lt;span style="color:#b44"> - enforcementAction: deny
&lt;/span>&lt;span style="color:#b44"> kind: Namespace
&lt;/span>&lt;span style="color:#b44"> message: &amp;#39;&lt;/span>&lt;span style="color:#008000;font-weight:bold">you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> name: kube-public
&lt;/span>&lt;span style="color:#b44"> - enforcementAction: deny
&lt;/span>&lt;span style="color:#b44"> kind: Namespace
&lt;/span>&lt;span style="color:#b44"> message: &amp;#39;&lt;/span>&lt;span style="color:#008000;font-weight:bold">you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&amp;#39;&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
### Data Replication
Audit requires replication of Kubernetes resources into OPA before they can be evaluated against the enforced Constraints. Data replication is also required by Constraints that need access to objects in the cluster other than the object under evaluation. For example, a Constraint that enforces uniqueness of ingress hostname must have access to all other ingresses in the cluster.
--->
&lt;h3 id="数据复制">数据复制&lt;/h3>
&lt;p>审核要求将 Kubernetes 复制到 OPA 中，然后才能根据强制的 Constraint 对其进行评估。数据复制同样也需要 Constraint，这些 Constraint 需要访问集群中除评估对象之外的对象。例如，一个 Constraint 要强制确定入口主机名的唯一性，就必须有权访问集群中的所有其他入口。&lt;/p>
&lt;!--
To configure Kubernetes data to be replicated, create a sync config resource with the resources to be replicated into OPA. For example, the below configuration replicates all namespace and pod resources to OPA.
--->
&lt;p>对 Kubernetes 数据进行复制，请使用复制到 OPA 中的资源创建 sync config 资源。例如，下面的配置将所有命名空间和 Pod 资源复制到 OPA。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>config.gatekeeper.sh/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;gatekeeper-system&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">sync&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">syncOnly&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Pod&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
## Planned for Future
The community behind the Gatekeeper project will be focusing on providing mutating admission control to support mutation scenarios (for example: annotate objects automatically with departmental information when creating a new resource), support external data to inject context external to the cluster into the admission decisions, support dry run to see impact of a policy on existing resources in the cluster before enforcing it, and more audit functionalities.
--->
&lt;h2 id="未来计划">未来计划&lt;/h2>
&lt;p>Gatekeeper 项目背后的社区将专注于提供转换许可控制，可以用来支持转换方案（例如：在创建新资源时使用 department 信息自动注释对象），支持外部数据以将集群外部环境加入到许可决策中，支持试运行以便在执行策略之前了解策略对集群中现有资源的影响，还有更多的审核功能。&lt;/p>
&lt;!--
If you are interested in learning more about the project, check out the [Gatekeeper](https://github.com/open-policy-agent/gatekeeper) repo. If you are interested in helping define the direction of Gatekeeper, join the [#kubernetes-policy](https://openpolicyagent.slack.com/messages/CDTN970AX) channel on OPA Slack, and join our [weekly meetings](https://docs.google.com/document/d/1A1-Q-1OMw3QODs1wT6eqfLTagcGmgzAJAjJihiO3T48/edit) to discuss development, issues, use cases, etc.
--->
&lt;p>如果您有兴趣了解更多有关该项目的信息，请查看 &lt;a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper&lt;/a> 存储库。如果您有兴趣帮助确定 Gatekeeper 的方向，请加入 &lt;a href="https://openpolicyagent.slack.com/messages/CDTN970AX">#kubernetes-policy&lt;/a> OPA Slack 频道，并加入我们的 &lt;a href="https://docs.google.com/document/d/1A1-Q-1OMw3QODs1wT6eqfLTagcGmgzAJAjJihiO3T48/edit">周会&lt;/a> 一同讨论开发、任务、用例等。&lt;/p></description></item><item><title>Blog: 欢迎参加在上海举行的贡献者峰会</title><link>https://kubernetes.io/zh/blog/2019/06/11/%E6%AC%A2%E8%BF%8E%E5%8F%82%E5%8A%A0%E5%9C%A8%E4%B8%8A%E6%B5%B7%E4%B8%BE%E8%A1%8C%E7%9A%84%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</link><pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/06/11/%E6%AC%A2%E8%BF%8E%E5%8F%82%E5%8A%A0%E5%9C%A8%E4%B8%8A%E6%B5%B7%E4%B8%BE%E8%A1%8C%E7%9A%84%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</guid><description>
&lt;!-- ---
layout: blog
title: 'Join us at the Contributor Summit in Shanghai'
date: 2019-06-11
--- -->
&lt;p>&lt;strong>Author&lt;/strong>: Josh Berkus (Red Hat)&lt;/p>
&lt;!-- ![Picture of contributor panel at 2018 Shanghai contributor summit. Photo by Josh Berkus, licensed CC-BY 4.0](/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png) -->
&lt;p>![贡献者小组讨论掠影，摄于 2018 年上海贡献者峰会，作者 Josh Berkus, 许可证 CC-BY 4.0](/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png)&lt;/p>
&lt;!-- For the second year, we will have [a Contributor Summit event](https://www.lfasiallc.com/events/contributors-summit-china-2019/) the day before [KubeCon China](https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/) in Shanghai. If you already contribute to Kubernetes or would like to contribute, please consider attending and [register](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/). The Summit will be held June 24th, at the Shanghai Expo Center (the same location where KubeCon will take place), and will include a Current Contributor Day as well as the New Contributor Workshop and the Documentation Sprints. -->
&lt;p>连续第二年，我们将在 &lt;a href="https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/">KubeCon China&lt;/a> 之前举行一天的 &lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/">贡献者峰会&lt;/a>。
不管您是否已经是一名 Kubernetes 贡献者，还是想要加入社区队伍，贡献一份力量，都请考虑&lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/register/">注册&lt;/a>参加这次活动。
这次峰会将于六月 24 号，在上海世博中心（和 KubeCon 的举办地点相同）举行，
一天的活动将包含“现有贡献者活动”，以及“新贡献者工作坊”和“文档小组活动”。&lt;/p>
&lt;!-- ### Current Contributor Day -->
&lt;h3 id="现有贡献者活动">现有贡献者活动&lt;/h3>
&lt;!-- After last year's Contributor Day, our team received feedback that many of our contributors in Asia and Oceania would like content for current contributors as well. As such, we have added a Current Contributor track to the schedule. -->
&lt;p>去年的贡献者节之后，我们的团队收到了很多反馈意见，很多亚洲和大洋洲的贡献者也想要针对当前贡献者的峰会内容。
有鉴于此，我们在今年的安排中加入了当前贡献者的主题。&lt;/p>
&lt;!-- While we do not yet have a full schedule up, the topics covered in the current contributor track will include: -->
&lt;p>尽管我们还没有一个完整的时间安排，下面是当前贡献者主题所会包含的话题：&lt;/p>
&lt;!-- * How to write a KEP (Kubernetes Enhancement Proposal)
* Codebase and repository review
* Local Build &amp; Test troubleshooting session
* Guide to Non-Code Contribution opportunities
* SIG-Azure face-to-face meeting
* SIG-Scheduling face-to-face meeting
* Other SIG face-to-face meetings as we confirm them -->
&lt;ul>
&lt;li>如何撰写 Kubernetes 改进议案 (KEP)&lt;/li>
&lt;li>代码库研习&lt;/li>
&lt;li>本地构建以及测试调试&lt;/li>
&lt;li>不写代码的贡献机会&lt;/li>
&lt;li>SIG-Azure 面对面交流&lt;/li>
&lt;li>SIG-Scheduling 面对面交流&lt;/li>
&lt;li>其他兴趣小组的面对面机会&lt;/li>
&lt;/ul>
&lt;!-- The schedule will be on [the Community page](https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit) once it is complete. -->
&lt;p>整个计划安排将会在完全确定之后，整理放在&lt;a href="https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit">社区页面&lt;/a>上。&lt;/p>
&lt;!-- If your SIG wants to have a face-to-face meeting at Kubecon Shanghai, please contact [Josh Berkus](mailto:jberkus@redhat.com). -->
&lt;p>如果您的 SIG 想要在 Kubecon Shanghai 上进行面对面的交流，请联系 &lt;a href="mailto:jberkus@redhat.com">Josh Berkus&lt;/a>。&lt;/p>
&lt;!-- ### New Contributor Workshop -->
&lt;h3 id="新贡献者工作坊">新贡献者工作坊&lt;/h3>
&lt;!-- Students at [last year's New Contributor Workshop](/blog/2018/12/05/new-contributor-workshop-shanghai/) (NCW) found it to be extremely valuable, and the event helped to orient a few of the many Asian and Pacific developers looking to participate in the Kubernetes community. -->
&lt;p>参与过&lt;a href="https://kubernetes.io/blog/2018/12/05/new-contributor-workshop-shanghai/">去年新贡献者工作坊（NCW）&lt;/a>的学生觉得这项活动非常的有价值，
这项活动也帮助、引导了很多亚洲和大洋洲的开发者更多地参与到 Kubernetes 社区之中。&lt;/p>
&lt;!-- > "It's a one-stop-shop for becoming familiar with the community." said one participant. -->
&lt;blockquote>
&lt;p>“这次活动可以让人一次快速熟悉社区。”其中的一位参与者提到。&lt;/p>
&lt;/blockquote>
&lt;!-- If you have not contributed to Kubernetes before, or have only done one or two things, please consider [enrolling](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/) in the NCW. -->
&lt;p>如果您之前从没有参与过 Kubernetes 的贡献，或者只是做过一次或两次贡献，都请考虑&lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/register/">注册参加&lt;/a>新贡献者工作坊。&lt;/p>
&lt;!-- > "Got to know the process from signing CLA to PR and made friends with other contributors." said another. -->
&lt;blockquote>
&lt;p>“熟悉了从 CLA 到 PR 的整个流程，也认识结交了很多贡献者。”另一位开发者提到。&lt;/p>
&lt;/blockquote>
&lt;!-- ### Documentation Sprints -->
&lt;h3 id="文档小组活动">文档小组活动&lt;/h3>
&lt;!-- Both old and new contributors on our Docs Team will spend a day both improving our documentation and translating it into other languages. If you are interested in having better documentation, fully localized into Chinese and other languages, please [sign up](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/) to help with the Doc Sprints. -->
&lt;p>文档小组的新老贡献者都会聚首一天，讨论如何提升文档质量，以及将文档翻译成更多的语言。
如果您对翻译文档，将这些知识和信息翻译成中文和其他语言感兴趣的话，请在这里&lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/register/">注册&lt;/a>，报名参加文档小组活动。&lt;/p>
&lt;!-- ### Before you attend -->
&lt;h3 id="参与之前">参与之前&lt;/h3>
&lt;!-- Regardless of where you participate, everyone at the Contributor Summit should [sign the Kubernetes Contributor License Agreement](https://git.k8s.io/community/CLA.md#the-contributor-license-agreement) (CLA) before coming to the conference. You should also bring a laptop suitable for working on documentation or code development. -->
&lt;p>不论您参与的是哪一项活动，所有人都需要在到达贡献者峰会前签署 &lt;a href="https://git.k8s.io/community/CLA.md#the-contributor-license-agreement">Kubernetes CLA&lt;/a>。
您也同时需要考虑带一个合适的笔记本电脑，帮助文档写作或是编程开发。&lt;/p></description></item><item><title>Blog: 壮大我们的贡献者研讨会</title><link>https://kubernetes.io/zh/blog/2019/05/14/expanding-our-contributor-workshops/</link><pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/05/14/expanding-our-contributor-workshops/</guid><description>
&lt;!--
---
layout: blog
title: "Expanding our Contributor Workshops"
date: 2019-05-14
slug: expanding-our-contributor-workshops
---
-->
&lt;!--
**Authors:** Guinevere Saenger (GitHub) and Paris Pittman (Google)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Guinevere Saenger (GitHub) 和 Paris Pittman (Google)（谷歌）&lt;/p>
&lt;!--
**tl;dr** - learn about the contributor community with us and land your first PR! We have spots available in [Barcelona][eu] (registration **closes** on Wednesday May 15, so grab your spot!) and the upcoming [Shanghai][cn] Summit.
-->
&lt;p>&lt;strong>tl;dr&lt;/strong> - 与我们一起了解贡献者社区，并获得你的第一份 PR ! 我们在[巴塞罗那][欧洲]有空位（登记 在5月15号周三&lt;strong>结束&lt;/strong>，所以抓住这次机会！）并且在[上海][中国]有即将到来的峰会。&lt;/p>
&lt;!--
The Barcelona event is poised to be our biggest one yet, with more registered attendees than ever before!
-->
&lt;p>巴塞罗那的活动将是我们迄今为止最大的一次，登记的参与者比以往任何时候都多！&lt;/p>
&lt;!--
Have you always wanted to contribute to Kubernetes, but not sure where to begin?
Have you seen our community’s many code bases and seen places to improve? We have a workshop for you!
-->
&lt;p>你是否曾经想为 Kubernetes 做贡献，但不知道从哪里开始？你有没有曾经看过我们社区的许多代码库并认为需要改进的地方？我们为你准备了一个工作室！&lt;/p>
&lt;!--
KubeCon + CloudNativeCon Barcelona’s [new contributor workshop][ncw] will be the fourth one of its kind, and we’re really looking forward to it!
-->
&lt;p>KubeCon + CloudNativeCon 巴塞罗那的研讨会&lt;a href="https://events.linuxfoundation.org/events/contributor-summit-europe-2019/">新贡献者研讨会&lt;/a>将是第四个这样的研讨会，我们真的很期待！&lt;/p>
&lt;!--
The workshop was kickstarted last year at KubeConEU in Copenhagen, and so far we have taken it to Shanghai and Seattle, and now Barcelona, as well as some non-KubeCon locations.
-->
&lt;p>这个研讨会去年在哥本哈根的 KubeConEU 启动，到目前为止，我们已经把它带到了上海和西雅图，现在是巴塞罗那，以及一些非 KubeConEU 的地方。&lt;/p>
&lt;!--
We are constantly updating and improving the workshop content based on feedback from past sessions.
-->
&lt;p>我们根据以往课程的反馈，不断更新和改进研讨会的内容。&lt;/p>
&lt;!--
This time, we’re breaking up the participants by their experience and comfort level with open source and Kubernetes.
-->
&lt;p>这一次，我们将根据参与者对开源和 Kubernetes 的经验和适应程度来进行划分。&lt;/p>
&lt;!--
We’ll have developer setup and project workflow support for folks entirely new to open
source and Kubernetes as part of the 101 track, and hope to set up each
participant with their very own first issue to work on.
-->
&lt;p>作为101课程的一部分，我们将为完全不熟悉开源和 Kubernetes 的人提供开发人员设置和项目工作流支持，并希望为每个参与者开展他们自己的第一期工作。&lt;/p>
&lt;!--
In the 201 track, we will have a codebase walkthrough and local development and test demonstration for folks who have a bit more experience in open source but may be unfamiliar with our community’s development tools.
-->
&lt;p>在201课程中，我们将为那些在开源方面有更多经验但可能不熟悉我们社区开发工具的人进行代码库演练和本地开发和测试演示。&lt;/p>
&lt;!--
For both tracks, you will have a chance to get your hands dirty and have some fun. Because not every contributor works with code, and not every contribution is technical, we will spend the beginning of the workshop learning how our project is structured and organized, where to
find the right people, and where to get help when stuck.
-->
&lt;p>对于这两门课程，你将有机会亲自动手并体会到其中的乐趣。因为不是每个贡献者都使用代码，也不是每项贡献都是技术性的，所以我们将在研讨会开始时学习如何构建和组织项目，以及如何进行找到合适的人，以及遇到困难时在哪里寻求帮助。&lt;/p>
&lt;!--
## Mentoring Opportunities
-->
&lt;h2 id="辅导机会">辅导机会&lt;/h2>
&lt;!--
We will also bring back the SIG Meet-and-Greet where new contributors will have a chance to mingle with current contributors, perhaps find their dream SIG, learn what exciting areas they can help with, gain mentors, and make friends.
-->
&lt;p>我们还将回归 SIG Meet-and-Greet，在这里新入门的菜鸟贡献者将有机会与当值的贡献者交流，这也许会让他们找到他们梦想的 SIG，了解他们可以帮助哪些激动人心的领域，获得导师，结交朋友。&lt;/p>
&lt;!--
PS - there are also two mentoring sessions DURING KubeCon + CloudNativeCon on Thursday, May 23. [Sign up here][mentor]. 60% of the attendees during the Seattle event asked contributor questions.
-->
&lt;p>PS - 5月23日星期四，在 KubeCon+CloudNativeCon 会议期间会有两次导师会议。[在这里注册][导师]。在西雅图活动期间，60% 的与会者会向贡献者提问。&lt;/p>
&lt;!--
## Past Attendee Story - Vallery Lancy, Engineer at Lyft
-->
&lt;h2 id="曾经与会者的故事-vallery-lancy-lyft-的工程师">曾经与会者的故事 - Vallery Lancy，Lyft 的工程师&lt;/h2>
&lt;!--
We talked to a few of our past participants in a series of interviews that we will publish throughout the course of the year.
-->
&lt;p>在一系列采访中，我们与一些过去的参与者进行了交谈，这些采访将在今年全年公布。&lt;/p>
&lt;!--
In our first two clips, we meet Vallery Lancy, an Engineer at Lyft and one of 75 attendees at our recent Seattle edition of the workshop. She was poking around in the community for a while to see where she could jump in.
-->
&lt;p>在我们的前两个片段中，我们会提到 Vallery Lancy，Lyft 公司的工程师，也是我们最近西雅图版研讨会的75名与会者之一。她在社区里闲逛了一段时间，想看看能不能投身到某个领域当中。&lt;/p>
&lt;!--
Watch Vallery talk about her experience here:
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/uKg5WUcl6WU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
-->
&lt;p>在这里观看 Vallery 讲述她的经历：&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/uKg5WUcl6WU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>&lt;/p>
&lt;!--
What does Vallery say to folks curious about the workshops, or those attending the Barcelona edition?
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/niHiem7JmPA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
-->
&lt;p>Vallery 和那些对研讨会感兴趣的人或者参加巴塞罗那会议的人说了些什么？&lt;/p>
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/niHiem7JmPA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
&lt;!--
Be like Vallery and hundreds of previous New Contributor Workshop attendees: join us in Barcelona (or Shanghai - or San Diego!) for a unique experience without digging into our documentation!
[eu]: https://events.linuxfoundation.org/events/contributor-summit-europe-2019/
[cn]: https://www.lfasiallc.com/events/contributors-summit-china-2019/
[ncw]: https://events.linuxfoundation.org/events/contributor-summit-europe-2019/
[mentor]: http://bit.ly/mentor-bcn
-->
&lt;p>想变得和 Vallery 还有数百名参与之前的新贡献者研讨会的与会者一样的话：在巴塞罗那（或者上海-或者圣地亚哥！）加入我们！你会有一个与众不同的体验，而不再是死读我们的文档！&lt;/p>
&lt;p>Have the opportunity to meet with the experts and go step by step into your journey with your peers around you. We’re looking forward to seeing you there! &lt;a href="https://events.linuxfoundation.org/events/contributor-summit-europe-2019/">Register here&lt;/a>
--&amp;gt;
你将有机会与专家见面，并与周围的同龄人一步步走上属于你的道路。我们期待着在那里与你见面！&lt;a href="https://events.linuxfoundation.org/events/contributor-summit-europe-2019/"> 在这里注册&lt;/a>&lt;/p></description></item><item><title>Blog: 如何参与 Kubernetes 文档的本地化工作</title><link>https://kubernetes.io/zh/blog/2019/04/26/%E5%A6%82%E4%BD%95%E5%8F%82%E4%B8%8E-kubernetes-%E6%96%87%E6%A1%A3%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%B7%A5%E4%BD%9C/</link><pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/04/26/%E5%A6%82%E4%BD%95%E5%8F%82%E4%B8%8E-kubernetes-%E6%96%87%E6%A1%A3%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%B7%A5%E4%BD%9C/</guid><description>
&lt;p>&lt;strong>作者: Zach Corleissen（Linux 基金会）&lt;/strong>&lt;/p>
&lt;p>去年我们对 Kubernetes 网站进行了优化，加入了&lt;a href="https://kubernetes.io/blog/2018/11/08/kubernetes-docs-updates-international-edition/">多语言内容的支持&lt;/a>。贡献者们踊跃响应，加入了多种新的本地化内容：截至 2019 年 4 月，Kubernetes 文档有了 9 个不同语言的未完成版本，其中有 6 个是 2019 年加入的。在每个 Kubernetes 文档页面的上方，读者都可以看到一个语言选择器，其中列出了所有可用语言。&lt;/p>
&lt;p>不论是完成度最高的&lt;a href="https://v1-12.docs.kubernetes.io/zh/">中文版 v1.12&lt;/a>，还是最新加入的&lt;a href="https://kubernetes.io/pt/">葡萄牙文版 v1.14&lt;/a>，各语言的本地化内容还未完成，这是一个进行中的项目。如果读者有兴趣对现有本地化工作提供支持，请继续阅读。&lt;/p>
&lt;h2 id="什么是本地化">什么是本地化&lt;/h2>
&lt;p>翻译是以词表意的问题。而本地化在此基础之上，还包含了过程和设计方面的工作。&lt;/p>
&lt;p>本地化和翻译很像，但是包含更多内容。除了进行翻译之外，本地化还要为编写和发布过程的框架进行优化。例如，Kubernetes.io 多数的站点浏览功能（按钮文字）都保存在&lt;a href="https://github.com/kubernetes/website/tree/master/i18n">单独的文件&lt;/a>之中。所以启动新本地化的过程中，需要包含加入对特定文件中字符串进行翻译的工作。&lt;/p>
&lt;p>本地化很重要，能够有效的降低 Kubernetes 的采纳和支持门槛。如果能用母语阅读 Kubernetes 文档，就能更轻松的开始使用 Kubernetes，并对其发展作出贡献。&lt;/p>
&lt;h2 id="如何启动本地化工作">如何启动本地化工作&lt;/h2>
&lt;p>不同语言的本地化工作都是单独的功能——和其它 Kubernetes 功能一致，贡献者们在一个 SIG 中进行本地化工作，分享出来进行评审，并加入项目。&lt;/p>
&lt;p>贡献者们在团队中进行内容的本地化工作。因为自己不能批准自己的 PR，所以一个本地化团队至少应该有两个人——例如意大利文的本地化团队有两个人。这个团队规模可能很大：中文团队有几十个成员。&lt;/p>
&lt;p>每个团队都有自己的工作流。有些团队手工完成所有的内容翻译；有些会使用带有翻译插件的编译器，并使用评审机来提供正确性的保障。SIG Docs 专注于输出的标准；这就给了本地化团队采用适合自己工作情况的工作流。这样一来，团队可以根据最佳实践进行协作，并以 Kubernetes 的社区精神进行分享。&lt;/p>
&lt;h2 id="为本地化工作添砖加瓦">为本地化工作添砖加瓦&lt;/h2>
&lt;p>如果你有兴趣为 Kubernetes 文档加入新语种的本地化内容，&lt;a href="https://kubernetes.io/docs/contribute/localization/">Kubernetes contribution guide&lt;/a> 中包含了这方面的相关内容。&lt;/p>
&lt;p>已经启动的的本地化工作同样需要支持。如果有兴趣为现存项目做出贡献，可以加入本地化团队的 Slack 频道，去做个自我介绍。各团队的成员会帮助你开始工作。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>语种&lt;/th>
&lt;th>Slack 频道&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>中文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CE3LNFYJ1/">#kubernetes-docs-zh&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>英文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/C1J0BPD2M/">#sig-docs&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>法文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CG838BFT9/">#kubernetes-docs-fr&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>德文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CH4UJ2BAL/">#kubernetes-docs-de&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>印地&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CJ14B9BDJ/">#kubernetes-docs-hi&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>印度尼西亚文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CJ1LUCUHM/">#kubernetes-docs-id&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>意大利文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CGB1MCK7X/">#kubernetes-docs-it&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>日文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CAG2M83S8/">#kubernetes-docs-ja&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>韩文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CA1MMR86S/">#kubernetes-docs-ko&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>葡萄牙文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CJ21AS0NA/">#kubernetes-docs-pt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>西班牙文&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CH7GB2E3B/">#kubernetes-docs-es&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="下一步">下一步？&lt;/h2>
&lt;p>最新的&lt;a href="https://kubernetes.slack.com/messages/CJ14B9BDJ/">印地文本地化&lt;/a>工作正在启动。为什么不加入你的语言？&lt;/p>
&lt;p>身为 SIG Docs 的主席，我甚至希望本地化工作跳出文档范畴，直接为 Kubernetes 组件提供本地化支持。有什么组件是你希望支持不同语言的么？可以提交一个 &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps">Kubernetes Enhancement Proposal&lt;/a> 来促成这一进步。&lt;/p></description></item><item><title>Blog: Kubernetes 1.14 稳定性改进中的进程ID限制</title><link>https://kubernetes.io/zh/blog/2019/04/15/kubernetes-1.14-%E7%A8%B3%E5%AE%9A%E6%80%A7%E6%94%B9%E8%BF%9B%E4%B8%AD%E7%9A%84%E8%BF%9B%E7%A8%8Bid%E9%99%90%E5%88%B6/</link><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/04/15/kubernetes-1.14-%E7%A8%B3%E5%AE%9A%E6%80%A7%E6%94%B9%E8%BF%9B%E4%B8%AD%E7%9A%84%E8%BF%9B%E7%A8%8Bid%E9%99%90%E5%88%B6/</guid><description>
&lt;!--
---
title: 'Process ID Limiting for Stability Improvements in Kubernetes 1.14'
date: 2019-04-15
---
-->
&lt;!--
**Author: Derek Carr**
Have you ever seen someone take more than their fair share of the cookies? The one person who reaches in and grabs a half dozen fresh baked chocolate chip chunk morsels and skitters off like Cookie Monster exclaiming “Om nom nom nom.”
In some rare workloads, a similar occurrence was taking place inside Kubernetes clusters. With each Pod and Node, there comes a finite number of possible process IDs (PIDs) for all applications to share. While it is rare for any one process or pod to reach in and grab all the PIDs, some users were experiencing resource starvation due to this type of behavior. So in Kubernetes 1.14, we introduced an enhancement to mitigate the risk of a single pod monopolizing all of the PIDs available.
-->
&lt;p>&lt;strong>作者: Derek Carr&lt;/strong>&lt;/p>
&lt;p>你是否见过有人拿走了比属于他们那一份更多的饼干？ 一个人走过来，抓起半打新鲜烤制的大块巧克力饼干然后匆匆离去，就像饼干怪兽大喊 “Om nom nom nom”。&lt;/p>
&lt;p>在一些罕见的工作负载中，Kubernetes 集群内部也发生了类似的情况。每个 Pod 和 Node 都有有限数量的可能的进程 ID（PID），供所有应用程序共享。尽管很少有进程或 Pod 能够进入并获取所有 PID，但由于这种行为，一些用户会遇到资源匮乏的情况。 因此，在 Kubernetes 1.14 中，我们引入了一项增强功能，以降低单个 Pod 垄断所有可用 PID 的风险。&lt;/p>
&lt;!--
## Can You Spare Some PIDs?
Here, we’re talking about the greed of certain containers. Outside the ideal, runaway processes occur from time to time, particularly in clusters where testing is taking place. Thus, some wildly non-production-ready activity is happening.
In such a scenario, it’s possible for something akin to a fork bomb taking place inside a node. As resources slowly erode, being taken over by some zombie-like process that continually spawns children, other legitimate workloads begin to get bumped in favor of this inflating balloon of wasted processing power. This could result in other processes on the same pod being starved of their needed PIDs. It could also lead to interesting side effects as a node could fail and a replica of that pod is scheduled to a new machine where the process repeats across your entire cluster.
-->
&lt;h2 id="你能预留一些-pids-吗">你能预留一些 PIDs 吗？&lt;/h2>
&lt;p>在这里，我们谈论的是某些容器的贪婪性。 在理想情况之外，失控进程有时会发生，特别是在测试集群中。 因此，在这些集群中会发生一些混乱的非生产环境准备就绪的事情。&lt;/p>
&lt;p>在这种情况下，可能会在节点内部发生类似于 fork 炸弹耗尽 PID 的攻击。随着资源的缓慢腐蚀，被一些不断产生子进程的僵尸般的进程所接管，其他正常的工作负载会因为这些像气球般不断膨胀的浪费的处理能力而开始受到冲击。这可能导致同一 Pod 上的其他进程缺少所需的 PID。这也可能导致有趣的副作用，因为节点可能会发生故障，并且该Pod的副本将安排到新的机器上，至此，该过程将在整个群集中重复进行。&lt;/p>
&lt;!--
## Fixing the Problem
Thus, in Kubernetes 1.14, we have added a feature that allows for the configuration of a kubelet to limit the number of PIDs a given pod can consume. If that machine supports 32,768 PIDs and 100 pods, one can give each pod a budget of 300 PIDs to prevent total exhaustion of PIDs. If the admin wants to overcommit PIDs similar to cpu or memory, they may do so as well with some additional risks. Either way, no one pod can bring the whole machine down. This will generally prevent against simple fork bombs from taking over your cluster.
This change allows administrators to protect one pod from another, but does not ensure if all pods on the machine can protect the node, and the node agents themselves from falling over. Thus, we’ve introduced a feature in this release in alpha form that provides isolation of PIDs from end user workloads on a pod from the node agents (kubelet, runtime, etc.). The admin is able to reserve a specific number of PIDs--similar to how one reserves CPU or memory today--and ensure they are never consumed by pods on that machine. Once that graduates from alpha, to beta, then stable in future releases of Kubernetes, we’ll have protection against an easily starved Linux resource.
Get started with [Kubernetes 1.14](https://github.com/kubernetes/kubernetes/releases/tag/v1.14.0).
-->
&lt;h2 id="解决问题">解决问题&lt;/h2>
&lt;p>因此，在 Kubernetes 1.14 中，我们添加了一个特性，允许通过配置 kubelet，限制给定 Pod 可以消耗的 PID 数量。如果该机器支持 32768 个 PIDs 和 100 个 Pod，则可以为每个 Pod 提供 300 个 PIDs 的预算，以防止 PIDs 完全耗尽。如果管理员想要像 CPU 或内存那样过度使用 PIDs，那么他们也可以配置超额使用，但是这样会有一些额外风险。不管怎样，没有一个Pod能搞坏整个机器。这通常会防止简单的分叉函数炸弹接管你的集群。&lt;/p>
&lt;p>此更改允许管理员保护一个 Pod 不受另一个 Pod 的影响，但不能确保计算机上的所有 Pod 都能保护节点和节点代理本身不受影响。因此，我们在这个版本中以 Alpha 的形式引入了这个一个特性，它提供了 PIDs 在节点代理（ kubelet、runtime 等）与 Pod 上的最终用户工作负载的分离。管理员可以预定特定数量的 pid（类似于今天如何预定 CPU 或内存），并确保它们不会被该计算机上的 pod 消耗。一旦从 Alpha 进入到 Beta，然后在将来的 Kubernetes 版本中稳定下来，我们就可以使用这个特性防止 Linux 资源耗尽。&lt;/p>
&lt;p>开始使用 &lt;a href="https://github.com/Kubernetes/Kubernetes/releases/tag/v1.14.0">Kubernetes 1.14&lt;/a>。&lt;/p>
&lt;!--
## Get Involved
If you have feedback for this feature or are interested in getting involved with the design and development, join the [Node Special Interest Group](https://github.com/kubernetes/community/tree/master/sig-node).
### About the author:
Derek Carr is Senior Principal Software Engineer at Red Hat. He is a Kubernetes contributor and member of the Kubernetes Community Steering Committee.
-->
&lt;p>##参与其中&lt;/p>
&lt;p>如果您对此特性有反馈或有兴趣参与其设计与开发，请加入[节点特别兴趣小组](&lt;a href="https://github.com/kubernetes/community/tree/master/sig">https://github.com/kubernetes/community/tree/master/sig&lt;/a> Node)。&lt;/p>
&lt;p>###关于作者：
Derek Carr 是 Red Hat 高级首席软件工程师。他也是 Kubernetes 的贡献者和 Kubernetes 社区指导委员会的成员。&lt;/p></description></item><item><title>Blog: Raw Block Volume 支持进入 Beta</title><link>https://kubernetes.io/zh/blog/2019/03/07/raw-block-volume-%E6%94%AF%E6%8C%81%E8%BF%9B%E5%85%A5-beta/</link><pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/03/07/raw-block-volume-%E6%94%AF%E6%8C%81%E8%BF%9B%E5%85%A5-beta/</guid><description>
&lt;!--
---
title: Raw Block Volume support to Beta
date: 2019-03-07
---
--->
&lt;!--
**Authors:**
Ben Swartzlander (NetApp), Saad Ali (Google)
Kubernetes v1.13 moves raw block volume support to beta. This feature allows persistent volumes to be exposed inside containers as a block device instead of as a mounted file system.
--->
&lt;p>&lt;strong>作者：&lt;/strong>
Ben Swartzlander (NetApp), Saad Ali (Google)&lt;/p>
&lt;p>Kubernetes v1.13 中对原生数据块卷（Raw Block Volume）的支持进入 Beta 阶段。此功能允许将持久卷作为块设备而不是作为已挂载的文件系统暴露在容器内部。&lt;/p>
&lt;!--
## What are block devices?
Block devices enable random access to data in fixed-size blocks. Hard drives, SSDs, and CD-ROMs drives are all examples of block devices.
Typically persistent storage is implemented in a layered maner with a file system (like ext4) on top of a block device (like a spinning disk or SSD). Applications then read and write files instead of operating on blocks. The operating systems take care of reading and writing files, using the specified filesystem, to the underlying device as blocks.
It's worth noting that while whole disks are block devices, so are disk partitions, and so are LUNs from a storage area network (SAN) device.
--->
&lt;h2 id="什么是块设备">什么是块设备？&lt;/h2>
&lt;p>块设备允许对固定大小的块中的数据进行随机访问。硬盘驱动器、SSD 和 CD-ROM 驱动器都是块设备的例子。&lt;/p>
&lt;p>通常，持久性性存储是在通过在块设备（例如磁盘或 SSD）之上构造文件系统（例如 ext4）的分层方式实现的。这样应用程序就可以读写文件而不是操作数据块进。操作系统负责使用指定的文件系统将文件读写转换为对底层设备的数据块读写。&lt;/p>
&lt;p>值得注意的是，整个磁盘都是块设备，磁盘分区也是如此，存储区域网络（SAN）设备中的 LUN 也是一样的。&lt;/p>
&lt;!--
## Why add raw block volumes to kubernetes?
There are some specialized applications that require direct access to a block device because, for example, the file system layer introduces unneeded overhead. The most common case is databases, which prefer to organize their data directly on the underlying storage. Raw block devices are also commonly used by any software which itself implements some kind of storage service (software defined storage systems).
--->
&lt;h2 id="为什么要将-raw-block-volume-添加到-kubernetes">为什么要将 raw block volume 添加到 kubernetes？&lt;/h2>
&lt;p>有些特殊的应用程序需要直接访问块设备，原因例如，文件系统层会引入不必要的开销。最常见的情况是数据库，通常会直接在底层存储上组织数据。原生的块设备（Raw Block Devices）还通常由能自己实现某种存储服务的软件（软件定义的存储系统）使用。&lt;/p>
&lt;!--
From a programmer's perspective, a block device is a very large array of bytes, usually with some minimum granularity for reads and writes, often 512 bytes, but frequently 4K or larger.
As it becomes more common to run database software and storage infrastructure software inside of Kubernetes, the need for raw block device support in Kubernetes becomes more important.
--->
&lt;p>从程序员的角度来看，块设备是一个非常大的字节数组，具有某种最小读写粒度，通常为 512 个字节，大部分情况为 4K 或更大。&lt;/p>
&lt;p>随着在 Kubernetes 中运行数据库软件和存储基础架构软件变得越来越普遍，在 Kubernetes 中支持原生块设备的需求变得越来越重要。&lt;/p>
&lt;!--
## Which volume plugins support raw blocks?
As of the publishing of this blog, the following in-tree volumes types support raw blocks:
--->
&lt;h2 id="哪些卷插件支持-raw-block">哪些卷插件支持 raw block？&lt;/h2>
&lt;p>在发布此博客时，以下 in-tree 卷类型支持原生块设备：&lt;/p>
&lt;ul>
&lt;li>AWS EBS&lt;/li>
&lt;li>Azure Disk&lt;/li>
&lt;li>Cinder&lt;/li>
&lt;li>Fibre Channel&lt;/li>
&lt;li>GCE PD&lt;/li>
&lt;li>iSCSI&lt;/li>
&lt;li>Local volumes&lt;/li>
&lt;li>RBD (Ceph)&lt;/li>
&lt;li>Vsphere&lt;/li>
&lt;/ul>
&lt;!--
Out-of-tree [CSI volume drivers](https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/) may also support raw block volumes. Kubernetes CSI support for raw block volumes is currently alpha. See documentation [here](https://kubernetes-csi.github.io/docs/raw-block.html).
--->
&lt;p>Out-of-tree &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">CSI 卷驱动程序&lt;/a> 可能也支持原生数据块卷。Kubernetes CSI 对原生数据块卷的支持目前为 alpha 阶段。参考 &lt;a href="https://kubernetes-csi.github.io/docs/raw-block.html">这篇&lt;/a> 文档。&lt;/p>
&lt;!--
## Kubernetes raw block volume API
Raw block volumes share a lot in common with ordinary volumes. Both are requested by creating `PersistentVolumeClaim` objects which bind to `PersistentVolume` objects, and are attached to Pods in Kubernetes by including them in the volumes array of the `PodSpec`.
There are 2 important differences however. First, to request a raw block `PersistentVolumeClaim`, you must set `volumeMode = "Block"` in the `PersistentVolumeClaimSpec`. Leaving `volumeMode` blank is the same as specifying `volumeMode = "Filesystem"` which results in the traditional behavior. `PersistentVolumes` also have a `volumeMode` field in their `PersistentVolumeSpec`, and `"Block"` type PVCs can only bind to `"Block"` type PVs and `"Filesystem"` PVCs can only bind to `"Filesystem"` PVs.
--->
&lt;h2 id="kubernetes-raw-block-volume-的-api">Kubernetes raw block volume 的 API&lt;/h2>
&lt;p>原生数据块卷与普通存储卷有很多共同点。两者都通过创建与 &lt;code>PersistentVolume&lt;/code> 对象绑定的 &lt;code>PersistentVolumeClaim&lt;/code> 对象发起请求，并通过将它们加入到 &lt;code>PodSpec&lt;/code> 的 volumes 数组中来连接到 Kubernetes 中的 Pod。&lt;/p>
&lt;p>但是有两个重要的区别。首先，要请求原生数据块设备的 &lt;code>PersistentVolumeClaim&lt;/code> 必须在 &lt;code>PersistentVolumeClaimSpec&lt;/code> 中设置 &lt;code>volumeMode = &amp;quot;Block&amp;quot;&lt;/code>。&lt;code>volumeMode&lt;/code> 为空时与传统设置方式中的指定 &lt;code>volumeMode = &amp;quot;Filesystem&amp;quot;&lt;/code> 是一样的。&lt;code>PersistentVolumes&lt;/code> 在其 &lt;code>PersistentVolumeSpec&lt;/code> 中也有一个 &lt;code>volumeMode&lt;/code> 字段，&lt;code>&amp;quot;Block&amp;quot;&lt;/code> 类型的 PVC 只能绑定到 &lt;code>&amp;quot;Block&amp;quot;&lt;/code> 类型的 PV 上，而&lt;code>&amp;quot;Filesystem&amp;quot;&lt;/code> 类型的 PVC 只能绑定到 &lt;code>&amp;quot;Filesystem&amp;quot;&lt;/code> PV 上。&lt;/p>
&lt;!--
Secondly, when using a raw block volume in your Pods, you must specify a `VolumeDevice` in the Container portion of the `PodSpec` rather than a `VolumeMount`. `VolumeDevices` have `devicePaths` instead of `mountPaths`, and inside the container, applications will see a device at that path instead of a mounted file system.
Applications open, read, and write to the device node inside the container just like they would interact with any block device on a system in a non-containerized or virtualized context.
--->
&lt;p>其次，在 Pod 中使用原生数据块卷时，必须在 &lt;code>PodSpec&lt;/code> 的 Container 部分指定一个 &lt;code>VolumeDevice&lt;/code>，而不是 &lt;code>VolumeMount&lt;/code>。&lt;code>VolumeDevices&lt;/code> 具备 &lt;code>devicePaths&lt;/code> 而不是 &lt;code>mountPaths&lt;/code>，在容器中，应用程序将看到位于该路径的设备，而不是挂载了的文件系统。&lt;/p>
&lt;p>应用程序打开、读取和写入容器内的设备节点，就像它们在非容器化或虚拟环境中与系统上的任何块设备交互一样。&lt;/p>
&lt;!--
## Creating a new raw block PVC
First, ensure that the provisioner associated with the storage class you choose is one that support raw blocks. Then create the PVC.
--->
&lt;h2 id="创建一个新的原生块设备-pvc">创建一个新的原生块设备 PVC&lt;/h2>
&lt;p>首先，请确保与您选择的存储类关联的驱动支持原生块设备。然后创建 PVC。&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: my-pvc
spec:
accessModes:
- ReadWriteMany
volumeMode: Block
storageClassName: my-sc
resources:
requests:
storage: 1Gi
&lt;/code>&lt;/pre>&lt;!--
## Using a raw block PVC
When you use the PVC in a pod definition, you get to choose the device path for the block device rather than the mount path for the file system.
--->
&lt;h2 id="使用原生块-pvc">使用原生块 PVC&lt;/h2>
&lt;p>在 Pod 定义中使用 PVC 时，需要选择块设备的设备路径，而不是文件系统的安装路径。&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: my-pod
spec:
containers:
- name: my-container
image: busybox
command:
- sleep
- “3600”
volumeDevices:
- devicePath: /dev/block
name: my-volume
imagePullPolicy: IfNotPresent
volumes:
- name: my-volume
persistentVolumeClaim:
claimName: my-pvc
&lt;/code>&lt;/pre>&lt;!--
## As a storage vendor, how do I add support for raw block devices to my CSI plugin?
Raw block support for CSI plugins is still alpha, but support can be added today. The [CSI specification](https://github.com/container-storage-interface/spec/blob/master/spec.md) details how to handle requests for volume that have the `BlockVolume` capability instead of the `MountVolume` capability. CSI plugins can support both kinds of volumes, or one or the other. For more details see [documentation here](https://kubernetes-csi.github.io/docs/raw-block.html).
--->
&lt;h2 id="作为存储供应商-我如何在-csi-插件中添加对原生块设备的支持">作为存储供应商，我如何在 CSI 插件中添加对原生块设备的支持？&lt;/h2>
&lt;p>CSI 插件的原生块支持仍然是 alpha 版本，但是现在可以改进了。&lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI 规范&lt;/a> 详细说明了如何处理具有 &lt;code>BlockVolume&lt;/code> 能力而不是 &lt;code>MountVolume&lt;/code> 能力的卷的请求。CSI 插件可以支持两种类型的卷，也可以支持其中一种或另一种。更多详细信息，请查看 &lt;a href="https://kubernetes-csi.github.io/docs/raw-block.html">这个文档&lt;/a>。&lt;/p>
&lt;!--
## Issues/gotchas
Because block devices are actually devices, it’s possible to do low-level actions on them from inside containers that wouldn’t be possible with file system volumes. For example, block devices that are actually SCSI disks support sending SCSI commands to the device using Linux ioctls.
--->
&lt;h2 id="问题-陷阱">问题/陷阱&lt;/h2>
&lt;p>由于块设备实质上还是设备，因此可以从容器内部对其进行底层操作，而文件系统的卷则无法执行这些操作。例如，实际上是块设备的 SCSI 磁盘支持使用 Linux ioctl 向设备发送 SCSI 命令。&lt;/p>
&lt;!--
By default, Linux won’t allow containers to send SCSI commands to disks from inside containers though. In order to do so, you must grant the `SYS_RAWIO` capability to the container security context to allow this. See documentation [here](/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container).
Also, while Kubernetes is guaranteed to deliver a block device to the container, there’s no guarantee that it’s actually a SCSI disk or any other kind of disk for that matter. The user must either ensure that the desired disk type is used with his pods, or only deploy applications that can handle a variety of block device types.
--->
&lt;p>默认情况下，Linux 不允许容器将 SCSI 命令从容器内部发送到磁盘。为此，必须向容器安全层级认证 &lt;code>SYS_RAWIO&lt;/code> 功能实现这种行为。请参阅 &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container">这篇&lt;/a> 文档。&lt;/p>
&lt;p>另外，尽管 Kubernetes 保证可以将块设备交付到容器中，但不能保证它实际上是 SCSI 磁盘或任何其他类型的磁盘。用户必须确保所需的磁盘类型与 Pod 一起使用，或只部署可以处理各种块设备类型的应用程序。&lt;/p>
&lt;!--
## How can I learn more?
Check out additional documentation on the snapshot feature here: [Raw Block Volume Support](/docs/concepts/storage/persistent-volumes/#raw-block-volume-support)
How do I get involved?
Join the Kubernetes storage SIG and the CSI community and help us add more great features and improve existing ones like raw block storage!
--->
&lt;h2 id="如何学习更多">如何学习更多？&lt;/h2>
&lt;p>在此处查看有关 snapshot 功能的其他文档：&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#raw-block-volume-support">Raw Block Volume 支持&lt;/a>&lt;/p>
&lt;p>如何参与进来？&lt;/p>
&lt;p>加入 Kubernetes 存储 SIG 和 CSI 社区，帮助我们添加更多出色的功能并改进现有功能，就像 raw block 存储一样！&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">https://github.com/kubernetes/community/tree/master/sig-storage&lt;/a>
&lt;a href="https://github.com/container-storage-interface/community/blob/master/README.md">https://github.com/container-storage-interface/community/blob/master/README.md&lt;/a>&lt;/p>
&lt;!--
Special thanks to all the contributors who helped add block volume support to Kubernetes including:
--->
&lt;p>特别感谢所有为 Kubernetes 增加 block volume 支持的贡献者，包括：&lt;/p>
&lt;ul>
&lt;li>Ben Swartzlander (&lt;a href="https://github.com/bswartz">https://github.com/bswartz&lt;/a>)&lt;/li>
&lt;li>Brad Childs (&lt;a href="https://github.com/childsb">https://github.com/childsb&lt;/a>)&lt;/li>
&lt;li>Erin Boyd (&lt;a href="https://github.com/erinboyd">https://github.com/erinboyd&lt;/a>)&lt;/li>
&lt;li>Masaki Kimura (&lt;a href="https://github.com/mkimuram">https://github.com/mkimuram&lt;/a>)&lt;/li>
&lt;li>Matthew Wong (&lt;a href="https://github.com/wongma7">https://github.com/wongma7&lt;/a>)&lt;/li>
&lt;li>Michelle Au (&lt;a href="https://github.com/msau42">https://github.com/msau42&lt;/a>)&lt;/li>
&lt;li>Mitsuhiro Tanino (&lt;a href="https://github.com/mtanino">https://github.com/mtanino&lt;/a>)&lt;/li>
&lt;li>Saad Ali (&lt;a href="https://github.com/saad-ali">https://github.com/saad-ali&lt;/a>)&lt;/li>
&lt;/ul></description></item><item><title>Blog: 新贡献者工作坊上海站</title><link>https://kubernetes.io/zh/blog/2018/12/05/%E6%96%B0%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B7%A5%E4%BD%9C%E5%9D%8A%E4%B8%8A%E6%B5%B7%E7%AB%99/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/12/05/%E6%96%B0%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B7%A5%E4%BD%9C%E5%9D%8A%E4%B8%8A%E6%B5%B7%E7%AB%99/</guid><description>
&lt;!--
---
layout: blog
title: 'New Contributor Workshop Shanghai'
date: 2018-12-05
---
-->
&lt;!--
**Authors**: Josh Berkus (Red Hat), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (ZTE)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Josh Berkus (红帽), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (中兴通讯)&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png"
alt="KubeCon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang"/> &lt;figcaption>
&lt;p>KubeCon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png"
alt="KubeCon 上海站新贡献者峰会与会者，摄影：Jerry Zhang"/> &lt;figcaption>
&lt;p>KubeCon 上海站新贡献者峰会与会者，摄影：Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
We recently completed our first New Contributor Summit in China, at the first KubeCon in China. It was very exciting to see all of the Chinese and Asian developers (plus a few folks from around the world) interested in becoming contributors. Over the course of a long day, they learned how, why, and where to contribute to Kubernetes, created pull requests, attended a panel of current contributors, and got their CLAs signed.
-->
&lt;p>最近，在中国的首次 KubeCon 上，我们完成了在中国的首次新贡献者峰会。看到所有中国和亚洲的开发者（以及来自世界各地的一些人）有兴趣成为贡献者，这令人非常兴奋。在长达一天的课程中，他们了解了如何、为什么以及在何处为 Kubernetes 作出贡献，创建了 PR，参加了贡献者圆桌讨论，并签署了他们的 CLA。&lt;/p>
&lt;!--
This was our second New Contributor Workshop (NCW), building on the one created and led by SIG Contributor Experience members in Copenhagen. Because of the audience, it was held in both Chinese and English, taking advantage of the superb simultaneous interpretation services the CNCF sponsored. Likewise, the NCW team included both English and Chinese-speaking members of the community: Yang Li, XiangPeng Zhao, Puja Abbassi, Noah Abrahams, Tim Pepper, Zach Corleissen, Sen Lu, and Josh Berkus. In addition to presenting and helping students, the bilingual members of the team translated all of the slides into Chinese. Fifty-one students attended.
-->
&lt;p>这是我们的第二届新贡献者工作坊（NCW），它由前一次贡献者体验 SIG 成员创建和领导的哥本哈根研讨会延伸而来。根据受众情况，本次活动采用了中英文两种语言，充分利用了 CNCF 赞助的一流的同声传译服务。同样，NCW 团队由社区成员组成，既有说英语的，也有说汉语的：Yang Li、XiangPeng Zhao、Puja Abbassi、Noah Abrahams、Tim Pepper、Zach Corleissen、Sen Lu 和 Josh Berkus。除了演讲和帮助学员外，团队的双语成员还将所有幻灯片翻译成了中文。共有五十一名学员参加。&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png"
alt="Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang"/> &lt;figcaption>
&lt;p>Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png"
alt="Noah Abrahams 讲解 Kubernetes 沟通渠道。摄影：Jerry Zhang"/> &lt;figcaption>
&lt;p>Noah Abrahams 讲解 Kubernetes 沟通渠道。摄影：Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
The NCW takes participants through the stages of contributing to Kubernetes, starting from deciding where to contribute, followed by an introduction to the SIG system and our repository structure. We also have "guest speakers" from Docs and Test Infrastructure who cover contributing in those areas. We finally wind up with some hands-on exercises in filing issues and creating and approving PRs.
-->
&lt;p>NCW 让参与者完成了为 Kubernetes 作出贡献的各个阶段，从决定在哪里作出贡献开始，接着介绍了 SIG 系统和我们的代码仓库结构。我们还有来自文档和测试基础设施领域的「客座讲者」，他们负责讲解有关的贡献。最后，我们在创建 issue、提交并批准 PR 的实践练习后，结束了工作坊。&lt;/p>
&lt;!--
Those hands-on exercises use a repository known as [the contributor playground](https://github.com/kubernetes-sigs/contributor-playground), created by SIG Contributor Experience as a place for new contributors to try out performing various actions on a Kubernetes repo. It has modified Prow and Tide automation, uses Owners files like in the real repositories. This lets students learn how the mechanics of contributing to our repositories work without disrupting normal development.
-->
&lt;p>这些实践练习使用一个名为&lt;a href="https://github.com/kubernetes-sigs/contributor-playground">贡献者游乐场&lt;/a>的代码仓库，由贡献者体验 SIG 创建，让新贡献者尝试在一个 Kubernetes 仓库中执行各种操作。它修改了 Prow 和 Tide 自动化，使用与真实代码仓库类似的 Owners 文件。这可以让学员了解为我们的仓库做出贡献的有关机制，同时又不妨碍正常的开发流程。&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png"
alt="Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus"/> &lt;figcaption>
&lt;p>Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png"
alt="Yang Li 讲到如何让你的 PR 通过评审。摄影：Josh Berkus"/> &lt;figcaption>
&lt;p>Yang Li 讲到如何让你的 PR 通过评审。摄影：Josh Berkus&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
Both the "Great Firewall" and the language barrier prevent contributing Kubernetes from China from being straightforward. What's more, because open source business models are not mature in China, the time for employees work on open source projects is limited.
-->
&lt;p>「防火长城」和语言障碍都使得在中国为 Kubernetes 作出贡献变得困难。而且，中国的开源商业模式并不成熟，员工在开源项目上工作的时间有限。&lt;/p>
&lt;!--
Chinese engineers are eager to participate in the development of Kubernetes, but many of them don't know where to start since Kubernetes is such a large project. With this workshop, we hope to help those who want to contribute, whether they wish to fix some bugs they encountered, improve or localize documentation, or they need to work with Kubernetes at their work. We are glad to see more and more Chinese contributors joining the community in the past few years, and we hope to see more of them in the future.
-->
&lt;p>中国工程师渴望参与 Kubernetes 的研发，但他们中的许多人不知道从何处开始，因为 Kubernetes 是一个如此庞大的项目。通过本次工作坊，我们希望帮助那些想要参与贡献的人，不论他们希望修复他们遇到的一些错误、改进或本地化文档，或者他们需要在工作中用到 Kubernetes。我们很高兴看到越来越多的中国贡献者在过去几年里加入社区，我们也希望将来可以看到更多。&lt;/p>
&lt;!--
"I have been participating in the Kubernetes community for about three years," said XiangPeng Zhao. "In the community, I notice that more and more Chinese developers are showing their interest in contributing to Kubernetes. However, it's not easy to start contributing to such a project. I tried my best to help those who I met in the community, but I think there might still be some new contributors leaving the community due to not knowing where to get help when in trouble. Fortunately, the community initiated NCW at KubeCon Copenhagen and held a second one at KubeCon Shanghai. I was so excited to be invited by Josh Berkus to help organize this workshop. During the workshop, I met community friends in person, mentored attendees in the exercises, and so on. All of this was a memorable experience for me. I also learned a lot as a contributor who already has years of contributing experience. I wish I had attended such a workshop when I started contributing to Kubernetes years ago."
-->
&lt;p>「我已经参与了 Kubernetes 社区大约三年」，XiangPeng Zhao 说，「在社区，我注意到越来越多的中国开发者表现出对 Kubernetes 贡献的兴趣。但是，开始为这样一个项目做贡献并不容易。我尽力帮助那些我在社区遇到的人，但是，我认为可能仍有一些新的贡献者离开社区，因为他们在遇到麻烦时不知道从哪里获得帮助。幸运的是，社区在 KubeCon 哥本哈根站发起了 NCW，并在 KubeCon 上海站举办了第二届。我很高兴受到 Josh Berkus 的邀请，帮助组织这个工作坊。在工作坊期间，我当面见到了社区里的朋友，在练习中指导了与会者，等等。所有这些对我来说都是难忘的经历。作为有着多年贡献者经验的我，也学习到了很多。我希望几年前我开始为 Kubernetes 做贡献时参加过这样的工作坊」。&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png"
alt="Panel of contributors. Photo by Jerry Zhang"/> &lt;figcaption>
&lt;p>Panel of contributors. Photo by Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png"
alt="贡献者圆桌讨论。摄影：Jerry Zhang"/> &lt;figcaption>
&lt;p>贡献者圆桌讨论。摄影：Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
The workshop ended with a panel of current contributors, featuring Lucas Käldström, Janet Kuo, Da Ma, Pengfei Ni, Zefeng Wang, and Chao Xu. The panel aimed to give both new and current contributors a look behind the scenes on the day-to-day of some of the most active contributors and maintainers, both from China and around the world. Panelists talked about where to begin your contributor's journey, but also how to interact with reviewers and maintainers. They further touched upon the main issues of contributing from China and gave attendees an outlook into exciting features they can look forward to in upcoming releases of Kubernetes.
-->
&lt;p>工作坊以现有贡献者圆桌讨论结束，嘉宾包括 Lucas Käldström、Janet Kuo、Da Ma、Pengfei Ni、Zefeng Wang 和 Chao Xu。这场圆桌讨论旨在让新的和现有的贡献者了解一些最活跃的贡献者和维护者的幕后日常工作，不论他们来自中国还是世界各地。嘉宾们讨论了从哪里开始贡献者的旅程，以及如何与评审者和维护者进行互动。他们进一步探讨了在中国参与贡献的主要问题，并向与会者预告了在 Kubernetes 的未来版本中可以期待的令人兴奋的功能。&lt;/p>
&lt;!--
After the workshop, XiangPeng Zhao chatted with some attendees on WeChat and Twitter about their experiences. They were very glad to have attended the NCW and had some suggestions on improving the workshop. One attendee, Mohammad, said, "I had a great time at the workshop and learned a lot about the entire process of k8s for a contributor." Another attendee, Jie Jia, said, "The workshop was wonderful. It systematically explained how to contribute to Kubernetes. The attendee could understand the process even if s/he knew nothing about that before. For those who were already contributors, they could also learn something new. Furthermore, I could make new friends from inside or outside of China in the workshop. It was awesome!"
-->
&lt;p>工作坊结束后，XiangPeng Zhao 和一些与会者就他们的经历在微信和 Twitter 上进行了交谈。他们很高兴参加了 NCW，并就改进工作坊提出了一些建议。一位名叫 Mohammad 的与会者说：「我在工作坊上玩得很开心，学习了参与 k8s 贡献的整个过程。」另一位与会者 Jie Jia 说：「工作坊非常精彩。它系统地解释了如何为 Kubernetes 做出贡献。即使参与者之前对此一无所知，他（她）也可以理解这个过程。对于那些已经是贡献者的人，他们也可以学习到新东西。此外，我还可以在工作坊上结识来自国内外的新朋友。真是棒极了！」&lt;/p>
&lt;!--
SIG Contributor Experience will continue to run New Contributor Workshops at each upcoming KubeCon, including Seattle, Barcelona, and the return to Shanghai in June 2019. If you failed to get into one this year, register for one at a future KubeCon. And, when you meet an NCW attendee, make sure to welcome them to the community.
-->
&lt;p>贡献者体验 SIG 将继续在未来的 KubeCon 上举办新贡献者工作坊，包括西雅图站、巴塞罗那站，然后在 2019 年六月回到上海。如果你今年未能参加，请在未来的 KubeCon 上注册。并且，如果你遇到工作坊的与会者，请务必欢迎他们加入社区。&lt;/p>
&lt;!--
Links:
-->
&lt;p>链接：&lt;/p>
&lt;!--
* English versions of the slides: [PDF](https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf) or [Google Docs with speaker notes](https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing)
* Chinese version of the slides: [PDF](https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf)
* [Contributor playground](https://github.com/kubernetes-sigs/contributor-playground)
-->
&lt;ul>
&lt;li>中文版幻灯片：&lt;a href="https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf">PDF&lt;/a>&lt;/li>
&lt;li>英文版幻灯片：&lt;a href="https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf">PDF&lt;/a> 或 &lt;a href="https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing">带有演讲者笔记的 Google Docs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/contributor-playground">贡献者游乐场&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 文档更新，国际版</title><link>https://kubernetes.io/zh/blog/2018/11/08/kubernetes-%E6%96%87%E6%A1%A3%E6%9B%B4%E6%96%B0%E5%9B%BD%E9%99%85%E7%89%88/</link><pubDate>Thu, 08 Nov 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/11/08/kubernetes-%E6%96%87%E6%A1%A3%E6%9B%B4%E6%96%B0%E5%9B%BD%E9%99%85%E7%89%88/</guid><description>
&lt;!--
---
layout: blog
title: 'Kubernetes Docs Updates, International Edition'
date: 2018-11-08
---
-->
&lt;!-- **Author**: Zach Corleissen (Linux Foundation) -->
&lt;p>&lt;strong>作者&lt;/strong>：Zach Corleissen （Linux 基金会）&lt;/p>
&lt;!-- As a co-chair of SIG Docs, I'm excited to share that Kubernetes docs have a fully mature workflow for localization (l10n). -->
&lt;p>作为文档特别兴趣小组（SIG Docs）的联合主席，我很高兴能与大家分享 Kubernetes 文档在本地化（l10n）方面所拥有的一个完全成熟的工作流。&lt;/p>
&lt;!-- ## Abbreviations galore -->
&lt;h2 id="丰富的缩写">丰富的缩写&lt;/h2>
&lt;!-- L10n is an abbreviation for _localization_. -->
&lt;p>L10n 是 &lt;em>localization&lt;/em> 的缩写。&lt;/p>
&lt;!-- I18n is an abbreviation for _internationalization_. -->
&lt;p>I18n 是 &lt;em>internationalization&lt;/em> 的缩写。&lt;/p>
&lt;!-- I18n is [what you do](https://www.w3.org/International/questions/qa-i18n) to make l10n easier. L10n is a fuller, more comprehensive process than translation (_t9n_). -->
&lt;p>I18n 定义了&lt;a href="https://www.w3.org/International/questions/qa-i18n">做什么&lt;/a> 能让 l10n 更容易。而 L10n 更全面，相比翻译（ &lt;em>t9n&lt;/em> ）具备更完善的流程。&lt;/p>
&lt;!-- ## Why localization matters -->
&lt;h2 id="为什么本地化很重要">为什么本地化很重要&lt;/h2>
&lt;!-- The goal of SIG Docs is to make Kubernetes easier to use for as many people as possible. -->
&lt;p>SIG Docs 的目标是让 Kubernetes 更容易为尽可能多的人使用。&lt;/p>
&lt;!-- One year ago, we looked at whether it was possible to host the output of a Chinese team working independently to translate the Kubernetes docs. After many conversations (including experts on OpenStack l10n), [much transformation](https://kubernetes.io/blog/2018/05/05/hugo-migration/), and [renewed commitment to easier localization](https://github.com/kubernetes/website/pull/10485), we realized that open source documentation is, like open source software, an ongoing exercise at the edges of what's possible. -->
&lt;p>一年前，我们研究了是否有可能由一个独立翻译 Kubernetes 文档的中国团队来主持文档输出。经过多次交谈（包括 OpenStack l10n 的专家），&lt;a href="https://kubernetes.io/blog/2018/05/05/hugo-migration/">多次转变&lt;/a>，以及&lt;a href="https://github.com/kubernetes/website/pull/10485">重新致力于更轻松的本地化&lt;/a>，我们意识到，开源文档就像开源软件一样，是在可能的边缘不断进行实践。&lt;/p>
&lt;!-- Consolidating workflows, language labels, and team-level ownership may seem like simple improvements, but these features make l10n scalable for increasing numbers of l10n teams. While SIG Docs continues to iterate improvements, we've paid off a significant amount of technical debt and streamlined l10n in a single workflow. That's great for the future as well as the present. -->
&lt;p>整合工作流程、语言标签和团队级所有权可能看起来像是十分简单的改进，但是这些功能使 l10n 可以扩展到规模越来越大的 l10n 团队。随着 SIG Docs 不断改进，我们已经在单一工作流程中偿还了大量技术债务并简化了 l10n。这对未来和现在都很有益。&lt;/p>
&lt;!-- ## Consolidated workflow -->
&lt;h2 id="整合的工作流程">整合的工作流程&lt;/h2>
&lt;!-- Localization is now consolidated in the [kubernetes/website](https://github.com/kubernetes/website) repository. We've configured the Kubernetes CI/CD system, [Prow](https://github.com/kubernetes/test-infra/tree/master/prow), to handle automatic language label assignment as well as team-level PR review and approval. -->
&lt;p>现在，本地化已整合到 &lt;a href="https://github.com/kubernetes/website">kubernetes/website&lt;/a> 存储库。我们已经配置了 Kubernetes CI/CD 系统，&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow">Prow&lt;/a> 来处理自动语言标签分配以及团队级 PR 审查和批准。&lt;/p>
&lt;!-- ### Language labels -->
&lt;h3 id="语言标签">语言标签&lt;/h3>
&lt;!-- Prow automatically applies language labels based on file path. Thanks to SIG Docs contributor [June Yi](https://github.com/kubernetes/test-infra/pull/9835), folks can also manually assign language labels in pull request (PR) comments. For example, when left as a comment on an issue or PR, this command assigns the label `language/ko` (Korean). -->
&lt;p>Prow 根据文件路径自动添加语言标签。感谢 SIG Docs 贡献者 &lt;a href="https://github.com/kubernetes/test-infra/pull/9835">June Yi&lt;/a>，他让人们还可以在 pull request（PR）注释中手动分配语言标签。例如，当为 issue 或 PR 留下下述注释时，将为之分配标签 &lt;code>language/ko&lt;/code>（Korean）。&lt;/p>
&lt;pre>&lt;code>/language ko
&lt;/code>&lt;/pre>&lt;!-- These repo labels let reviewers filter for PRs and issues by language. For example, you can now filter the k/website dashboard for [PRs with Chinese content](https://github.com/kubernetes/website/pulls?utf8=%E2%9C%93&amp;q=is%3Aopen+is%3Apr+label%3Alanguage%2Fzh). -->
&lt;p>这些存储库标签允许审阅者按语言过滤 PR 和 issue。例如，您现在可以过滤 kubernetes/website 面板中&lt;a href="https://github.com/kubernetes/website/pulls?utf8=%E2%9C%93&amp;amp;q=is%3Aopen+is%3Apr+label%3Alanguage%2Fzh">具有中文内容的 PR&lt;/a>。&lt;/p>
&lt;!-- ### Team review -->
&lt;h3 id="团队审核">团队审核&lt;/h3>
&lt;!-- L10n teams can now review and approve their own PRs. For example, review and approval permissions for English are [assigned in an OWNERS file](https://github.com/kubernetes/website/blob/master/content/en/OWNERS) in the top subfolder for English content. -->
&lt;p>L10n 团队现在可以审查和批准他们自己的 PR。例如，英语的审核和批准权限在位于用于显示英语内容的顶级子文件夹中的 &lt;a href="https://github.com/kubernetes/website/blob/master/content/en/OWNERS">OWNERS 文件中指定&lt;/a>。&lt;/p>
&lt;!-- Adding `OWNERS` files to subdirectories lets localization teams review and approve changes without requiring a rubber stamp approval from reviewers who may lack fluency. -->
&lt;p>将 &lt;code>OWNERS&lt;/code> 文件添加到子目录可以让本地化团队审查和批准更改，而无需由可能并不擅长该门语言的审阅者进行批准。&lt;/p>
&lt;!-- ## What's next -->
&lt;h2 id="下一步是什么">下一步是什么&lt;/h2>
&lt;!-- We're looking forward to the [doc sprint in Shanghai](https://kccncchina2018english.sched.com/event/HVb2/contributor-summit-doc-sprint-additional-registration-required) to serve as a resource for the Chinese l10n team. -->
&lt;p>我们期待着&lt;a href="https://kccncchina2018english.sched.com/event/HVb2/contributor-summit-doc-sprint-additional-registration-required">上海的 doc sprint&lt;/a> 能作为中国 l10n 团队的资源。&lt;/p>
&lt;!-- We're excited to continue supporting the Japanese and Korean l10n teams, who are making excellent progress. -->
&lt;p>我们很高兴继续支持正在取得良好进展的日本和韩国 l10n 队伍。&lt;/p>
&lt;!-- If you're interested in localizing Kubernetes for your own language or region, check out our [guide to localizing Kubernetes docs](https://kubernetes.io/docs/contribute/localization/) and reach out to a [SIG Docs chair](https://github.com/kubernetes/community/tree/master/sig-docs#leadership) for support. -->
&lt;p>如果您有兴趣将 Kubernetes 本地化为您自己的语言或地区，请查看我们的&lt;a href="https://kubernetes.io/docs/contribute/localization/">本地化 Kubernetes 文档指南&lt;/a>，并联系 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-docs#leadership">SIG Docs 主席团&lt;/a>获取支持。&lt;/p>
&lt;!-- ### Get involved with SIG Docs -->
&lt;h3 id="加入sig-docs">加入SIG Docs&lt;/h3>
&lt;!-- If you're interested in Kubernetes documentation, come to a SIG Docs [weekly meeting](https://github.com/kubernetes/community/tree/master/sig-docs#meetings), or join [#sig-docs in Kubernetes Slack](https://kubernetes.slack.com/messages/C1J0BPD2M/details/). -->
&lt;p>如果您对 Kubernetes 文档感兴趣，请参加 SIG Docs &lt;a href="https://github.com/kubernetes/community/tree/master/sig-docs#meetings">每周会议&lt;/a>，或在 &lt;a href="https://kubernetes.slack.com/messages/C1J0BPD2M/details/">Kubernetes Slack 加入 #sig-docs&lt;/a>。&lt;/p></description></item><item><title>Blog: Kubernetes 2018 年北美贡献者峰会</title><link>https://kubernetes.io/zh/blog/2018/10/16/kubernetes-2018-%E5%B9%B4%E5%8C%97%E7%BE%8E%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</link><pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/16/kubernetes-2018-%E5%B9%B4%E5%8C%97%E7%BE%8E%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</guid><description>
&lt;!--
---
layout: "Blog"
title: "Kubernetes 2018 North American Contributor Summit"
date: 2018-10-16
---
-->
&lt;!--
**Authors:**
-->
&lt;p>&lt;strong>作者：&lt;/strong>&lt;/p>
&lt;!--
[Bob Killen][bob] (University of Michigan)
[Sahdev Zala][sahdev] (IBM),
[Ihor Dvoretskyi][ihor] (CNCF)
-->
&lt;p>&lt;a href="https://twitter.com/mrbobbytables">Bob Killen&lt;/a>（密歇根大学）
&lt;a href="https://twitter.com/sp_zala">Sahdev Zala&lt;/a>（IBM），
&lt;a href="https://twitter.com/idvoretskyi">Ihor Dvoretskyi&lt;/a>（CNCF）&lt;/p>
&lt;!--
The 2018 North American Kubernetes Contributor Summit to be hosted right before
[KubeCon + CloudNativeCon][kubecon] Seattle is shaping up to be the largest yet.
-->
&lt;p>2018 年北美 Kubernetes 贡献者峰会将在西雅图 &lt;a href="https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/">KubeCon + CloudNativeCon&lt;/a> 会议之前举办，这将是迄今为止规模最大的一次盛会。&lt;/p>
&lt;!--
It is an event that brings together new and current contributors alike to
connect and share face-to-face; and serves as an opportunity for existing
contributors to help shape the future of community development. For new
community members, it offers a welcoming space to learn, explore and put the
contributor workflow to practice.
-->
&lt;p>这是一个将新老贡献者聚集在一起，面对面交流和分享的活动；并为现有的贡献者提供一个机会，帮助塑造社区发展的未来。它为新的社区成员提供了一个学习、探索和实践贡献工作流程的良好空间。&lt;/p>
&lt;!--
Unlike previous Contributor Summits, the event now spans two-days with a more
relaxed ‘hallway’ track and general Contributor get-together to be hosted from
5-8pm on Sunday December 9th at the [Garage Lounge and Gaming Hall][garage], just
a short walk away from the Convention Center. There, contributors can enjoy
billiards, bowling, trivia and more; accompanied by a variety of food and drink.
-->
&lt;p>与之前的贡献者峰会不同，本次活动为期两天，有一个更为轻松的行程安排，一般贡献者将于 12 月 9 日（周日）下午 5 点至 8 点在距离会议中心仅几步远的 &lt;a href="https://www.garagebilliards.com/">Garage Lounge and Gaming Hall&lt;/a> 举办峰会。在那里，贡献者也可以进行台球、保龄球等娱乐活动，而且还有各种食品和饮料。&lt;/p>
&lt;!--
Things pick up the following day, Monday the 10th with three separate tracks:
-->
&lt;p>接下来的一天，也就是 10 号星期一，有三个独立的会议你可以选择参与：&lt;/p>
&lt;!--
### New Contributor Workshop:
A half day workshop aimed at getting new and first time contributors onboarded
and comfortable with working within the Kubernetes Community. Staying for the
duration is required; this is not a workshop you can drop into.
-->
&lt;h3 id="新贡献者研讨会">新贡献者研讨会：&lt;/h3>
&lt;p>为期半天的研讨会旨在让新贡献者加入社区，并营造一个良好的 Kubernetes 社区工作环境。
请在开会期间保持在场，该讨论会不允许随意进出。&lt;/p>
&lt;!--
### Current Contributor Track:
Reserved for those that are actively engaged with the development of the
project; the Current Contributor Track includes Talks, Workshops, Birds of a
Feather, Unconferences, Steering Committee Sessions, and more! Keep an eye on
the [schedule in GitHub][schedule] as content is frequently being updated.
-->
&lt;h3 id="当前贡献者追踪">当前贡献者追踪：&lt;/h3>
&lt;p>保留给那些积极参与项目开发的贡献者；目前的贡献者追踪包括讲座、研讨会、聚会、Unconferences 会议、指导委员会会议等等!
请留意 &lt;a href="https://git.k8s.io/community/events/2018/12-contributor-summit#agenda">GitHub 中的时间表&lt;/a>，因为内容经常更新。&lt;/p>
&lt;!--
### Docs Sprint:
SIG-Docs will have a curated list of issues and challenges to be tackled closer
to the event date.
-->
&lt;h3 id="docs-冲刺">Docs 冲刺：&lt;/h3>
&lt;p>SIG-Docs 将在活动日期临近的时候列出一个需要处理的问题和挑战列表。&lt;/p>
&lt;!--
## To Register:
To register for the Contributor Summit, see the [Registration section of the
Event Details in GitHub][register]. Please note that registrations are being
reviewed. If you select the “Current Contributor Track” and are not an active
contributor, you will be asked to attend the New Contributor Workshop, or asked
to be put on a waitlist. With thousands of contributors and only 300 spots, we
need to make sure the right folks are in the room.
-->
&lt;h2 id="注册">注册：&lt;/h2>
&lt;p>要注册贡献者峰会，请参阅 Git Hub 上的&lt;a href="https://git.k8s.io/community/events/2018/12-contributor-summit#registration">活动详情注册部分&lt;/a>。请注意报名正在审核中。
如果您选择了 “当前贡献者追踪”，而您却不是一个活跃的贡献者，您将被要求参加新贡献者研讨会，或者被要求进入候补名单。
成千上万的贡献者只有 300 个位置，我们需要确保正确的人被安排席位。&lt;/p>
&lt;!--
If you have any questions or concerns, please don’t hesitate to reach out to
the Contributor Summit Events Team at community@kubernetes.io.
-->
&lt;p>如果您有任何问题或疑虑，请随时通过 &lt;a href="mailto:community@kubernetes.io">community@kubernetes.io&lt;/a> 联系贡献者峰会组织团队。&lt;/p>
&lt;!--
Look forward to seeing everyone there!
-->
&lt;p>期待在那里看到每个人！&lt;/p></description></item><item><title>Blog: 2018 年督导委员会选举结果</title><link>https://kubernetes.io/zh/blog/2018/10/15/2018-%E5%B9%B4%E7%9D%A3%E5%AF%BC%E5%A7%94%E5%91%98%E4%BC%9A%E9%80%89%E4%B8%BE%E7%BB%93%E6%9E%9C/</link><pubDate>Mon, 15 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/15/2018-%E5%B9%B4%E7%9D%A3%E5%AF%BC%E5%A7%94%E5%91%98%E4%BC%9A%E9%80%89%E4%B8%BE%E7%BB%93%E6%9E%9C/</guid><description>
&lt;!--
---
layout: blog
title: '2018 Steering Committee Election Results'
date: 2018-10-15
---
-->
&lt;!-- **Authors**: Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF), Paris Pittman (Google) -->
&lt;p>&lt;strong>作者&lt;/strong>: Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF), Paris Pittman (Google)&lt;/p>
&lt;!--
## Results
-->
&lt;h2 id="结果">结果&lt;/h2>
&lt;!--
The [Kubernetes Steering Committee Election](https://kubernetes.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/) is now complete and the following candidates came ahead to secure two year terms that start immediately:
-->
&lt;p>&lt;a href="https://kubernetes.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/">Kubernetes 督导委员会选举&lt;/a>现已完成，以下候选人获得了立即开始的两年任期：&lt;/p>
&lt;ul>
&lt;li>Aaron Crickenberger, Google, &lt;a href="https://github.com/spiffxp">@spiffxp&lt;/a>&lt;/li>
&lt;li>Davanum Srinivas, Huawei, &lt;a href="https://github.com/dims">@dims&lt;/a>&lt;/li>
&lt;li>Tim St. Clair, Heptio, &lt;a href="https://github.com/timothysc">@timothysc&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Big Thanks!
-->
&lt;h2 id="十分感谢">十分感谢！&lt;/h2>
&lt;!--
* Steering Committee Member Emeritus [Quinton Hoole](https://github.com/quinton-hoole) for his service to the community over the past year. We look forward to
* The candidates that came forward to run for election. May we always have a strong set of people who want to push community forward like yours in every election.
* All 307 voters who cast a ballot.
* And last but not least...Cornell University for hosting [CIVS](https://civs.cs.cornell.edu/)!
-->
&lt;ul>
&lt;li>督导委员会荣誉退休成员 &lt;a href="https://github.com/quinton-hoole">Quinton Hoole&lt;/a>，表扬他在过去一年为社区所作的贡献。我们期待着&lt;/li>
&lt;li>参加竞选的候选人。愿我们永远拥有一群强大的人，他们希望在每一次选举中都能像你们一样推动社区向前发展。&lt;/li>
&lt;li>共计 307 名选民参与投票。&lt;/li>
&lt;li>本次选举由康奈尔大学主办 &lt;a href="https://civs.cs.cornell.edu/">CIVS&lt;/a>！&lt;/li>
&lt;/ul>
&lt;!--
## Get Involved with the Steering Committee
-->
&lt;h2 id="加入督导委员会">加入督导委员会&lt;/h2>
&lt;!--
You can follow along to Steering Committee [backlog items](https://git.k8s.io/steering/backlog.md) and weigh in by filing an issue or creating a PR against their [repo](https://github.com/kubernetes/steering). They meet bi-weekly on [Wednesdays at 8pm UTC](https://github.com/kubernetes/steering) and regularly attend Meet Our Contributors.
-->
&lt;p>你可以关注督导委员会的&lt;a href="https://git.k8s.io/steering/backlog.md">任务清单&lt;/a>，并通过向他们的&lt;a href="https://github.com/kubernetes/steering">代码仓库&lt;/a>提交 issue 或 PR 的方式来参与。他们也会在&lt;a href="https://github.com/kubernetes/steering">UTC 时间每周三晚 8 点&lt;/a>举行会议，并定期与我们的贡献者见面。&lt;/p>
&lt;!--
Steering Committee Meetings:
-->
&lt;p>督导委员会会议：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube 播放列表&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Meet Our Contributors Steering AMA’s:
-->
&lt;p>与我们的贡献者会面：&lt;/p>
&lt;!--
* [Oct 3 2018](https://youtu.be/x6Jm8p0K-IQ)
* [Sept 5 2018](https://youtu.be/UbxWV12Or58)
-->
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/x6Jm8p0K-IQ">2018 年 10 月 3 日&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/UbxWV12Or58">2018 年 7 月 5 日&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 中的拓扑感知数据卷供应</title><link>https://kubernetes.io/zh/blog/2018/10/11/kubernetes-%E4%B8%AD%E7%9A%84%E6%8B%93%E6%89%91%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E5%8D%B7%E4%BE%9B%E5%BA%94/</link><pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/11/kubernetes-%E4%B8%AD%E7%9A%84%E6%8B%93%E6%89%91%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E5%8D%B7%E4%BE%9B%E5%BA%94/</guid><description>
&lt;!--
---
layout: blog
title: 'Topology-Aware Volume Provisioning in Kubernetes'
date: 2018-10-11
---
-->
&lt;!--
**Author**: Michelle Au (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Michelle Au（谷歌）&lt;/p>
&lt;!--
The multi-zone cluster experience with persistent volumes is improving in Kubernetes 1.12 with the topology-aware dynamic provisioning beta feature. This feature allows Kubernetes to make intelligent decisions when dynamically provisioning volumes by getting scheduler input on the best place to provision a volume for a pod. In multi-zone clusters, this means that volumes will get provisioned in an appropriate zone that can run your pod, allowing you to easily deploy and scale your stateful workloads across failure domains to provide high availability and fault tolerance.
-->
&lt;p>通过提供拓扑感知动态卷供应功能，具有持久卷的多区域集群体验在 Kubernetes 1.12 中得到了改进。此功能使得 Kubernetes 在动态供应卷时能做出明智的决策，方法是从调度器获得为 Pod 提供数据卷的最佳位置。在多区域集群环境，这意味着数据卷能够在满足你的 Pod 运行需要的合适的区域被供应，从而允许您跨故障域轻松部署和扩展有状态工作负载，从而提供高可用性和容错能力。&lt;/p>
&lt;!--
## Previous challenges
-->
&lt;h2 id="以前的挑战">以前的挑战&lt;/h2>
&lt;!--
Before this feature, running stateful workloads with zonal persistent disks (such as AWS ElasticBlockStore, Azure Disk, GCE PersistentDisk) in multi-zone clusters had many challenges. Dynamic provisioning was handled independently from pod scheduling, which meant that as soon as you created a PersistentVolumeClaim (PVC), a volume would get provisioned. This meant that the provisioner had no knowledge of what pods were using the volume, and any pod constraints it had that could impact scheduling.
-->
&lt;p>在此功能被提供之前，在多区域集群中使用区域化的持久磁盘（例如 AWS ElasticBlockStore，Azure Disk，GCE PersistentDisk）运行有状态工作负载存在许多挑战。动态供应独立于 Pod 调度处理，这意味着只要您创建了一个 PersistentVolumeClaim（PVC），一个卷就会被供应。这也意味着供应者不知道哪些 Pod 正在使用该卷，也不清楚任何可能影响调度的 Pod 约束。&lt;/p>
&lt;!--
This resulted in unschedulable pods because volumes were provisioned in zones that:
-->
&lt;p>这导致了不可调度的 Pod，因为在以下区域中配置了卷：&lt;/p>
&lt;!--
* did not have enough CPU or memory resources to run the pod
* conflicted with node selectors, pod affinity or anti-affinity policies
* could not run the pod due to taints
-->
&lt;ul>
&lt;li>没有足够的 CPU 或内存资源来运行 Pod&lt;/li>
&lt;li>与节点选择器、Pod 亲和或反亲和策略冲突&lt;/li>
&lt;li>由于污点（taint）不能运行 Pod&lt;/li>
&lt;/ul>
&lt;!--
Another common issue was that a non-StatefulSet pod using multiple persistent volumes could have each volume provisioned in a different zone, again resulting in an unschedulable pod.
-->
&lt;p>另一个常见问题是，使用多个持久卷的非有状态 Pod 可能会在不同的区域中配置每个卷，从而导致一个不可调度的 Pod。&lt;/p>
&lt;!--
Suboptimal workarounds included overprovisioning of nodes, or manual creation of volumes in the correct zones, making it difficult to dynamically deploy and scale stateful workloads.
-->
&lt;p>次优的解决方法包括节点超配，或在正确的区域中手动创建卷，但这会造成难以动态部署和扩展有状态工作负载的问题。&lt;/p>
&lt;!--
The topology-aware dynamic provisioning feature addresses all of the above issues.
-->
&lt;p>拓扑感知动态供应功能解决了上述所有问题。&lt;/p>
&lt;!--
## Supported Volume Types
-->
&lt;h2 id="支持的卷类型">支持的卷类型&lt;/h2>
&lt;!--
In 1.12, the following drivers support topology-aware dynamic provisioning:
-->
&lt;p>在 1.12 中，以下驱动程序支持拓扑感知动态供应：&lt;/p>
&lt;!--
* AWS EBS
* Azure Disk
* GCE PD (including Regional PD)
* CSI (alpha) - currently only the GCE PD CSI driver has implemented topology support
-->
&lt;ul>
&lt;li>AWS EBS&lt;/li>
&lt;li>Azure Disk&lt;/li>
&lt;li>GCE PD （包括 Regional PD）&lt;/li>
&lt;li>CSI（alpha） - 目前只有 GCE PD CSI 驱动实现了拓扑支持&lt;/li>
&lt;/ul>
&lt;!--
## Design Principles
-->
&lt;h2 id="设计原则">设计原则&lt;/h2>
&lt;!--
While the initial set of supported plugins are all zonal-based, we designed this feature to adhere to the Kubernetes principle of portability across environments. Topology specification is generalized and uses a similar label-based specification like in Pod nodeSelectors and nodeAffinity. This mechanism allows you to define your own topology boundaries, such as racks in on-premise clusters, without requiring modifications to the scheduler to understand these custom topologies.
-->
&lt;p>虽然最初支持的插件集都是基于区域的，但我们设计此功能时遵循 Kubernetes 跨环境可移植性的原则。
拓扑规范是通用的，并使用类似于基于标签的规范，如 Pod nodeSelectors 和 nodeAffinity。
该机制允许您定义自己的拓扑边界，例如内部部署集群中的机架，而无需修改调度程序以了解这些自定义拓扑。&lt;/p>
&lt;!--
In addition, the topology information is abstracted away from the pod specification, so a pod does not need knowledge of the underlying storage system’s topology characteristics. This means that you can use the same pod specification across multiple clusters, environments, and storage systems.
-->
&lt;p>此外，拓扑信息是从 Pod 规范中抽象出来的，因此 Pod 不需要了解底层存储系统的拓扑特征。
这意味着您可以在多个集群、环境和存储系统中使用相同的 Pod 规范。&lt;/p>
&lt;!--
## Getting Started
-->
&lt;h2 id="入门">入门&lt;/h2>
&lt;!--
To enable this feature, all you need to do is to create a StorageClass with `volumeBindingMode` set to `WaitForFirstConsumer`:
-->
&lt;p>要启用此功能，您需要做的就是创建一个将 &lt;code>volumeBindingMode&lt;/code> 设置为 &lt;code>WaitForFirstConsumer&lt;/code> 的 StorageClass：&lt;/p>
&lt;pre>&lt;code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: topology-aware-standard
provisioner: kubernetes.io/gce-pd
volumeBindingMode: WaitForFirstConsumer
parameters:
type: pd-standard
&lt;/code>&lt;/pre>&lt;!--
This new setting instructs the volume provisioner to not create a volume immediately, and instead, wait for a pod using an associated PVC to run through scheduling. Note that previous StorageClass `zone` and `zones` parameters do not need to be specified anymore, as pod policies now drive the decision of which zone to provision a volume in.
-->
&lt;p>这个新设置表明卷配置器不立即创建卷，而是等待使用关联的 PVC 的 Pod 通过调度运行。
请注意，不再需要指定以前的 StorageClass &lt;code>zone&lt;/code> 和 &lt;code>zones&lt;/code> 参数，因为现在在哪个区域中配置卷由 Pod 策略决定。&lt;/p>
&lt;!--
Next, create a pod and PVC with this StorageClass. This sequence is the same as before, but with a different StorageClass specified in the PVC. The following is a hypothetical example, demonstrating the capabilities of the new feature by specifying many pod constraints and scheduling policies:
-->
&lt;p>接下来，使用此 StorageClass 创建一个 Pod 和 PVC。
此过程与之前相同，但在 PVC 中指定了不同的 StorageClass。
以下是一个假设示例，通过指定许多 Pod 约束和调度策略来演示新功能特性：&lt;/p>
&lt;!--
* multiple PVCs in a pod
* nodeAffinity across a subset of zones
* pod anti-affinity on zones
-->
&lt;ul>
&lt;li>一个 Pod 多个 PVC&lt;/li>
&lt;li>跨子区域的节点亲和&lt;/li>
&lt;li>同一区域 Pod 反亲和&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>apiVersion: apps/v1
kind: StatefulSet
metadata:
name: web
spec:
serviceName: &amp;quot;nginx&amp;quot;
replicas: 2
selector:
matchLabels:
app: nginx
template:
metadata:
labels:
app: nginx
spec:
affinity:
nodeAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
nodeSelectorTerms:
- matchExpressions:
- key: failure-domain.beta.kubernetes.io/zone
operator: In
values:
- us-central1-a
- us-central1-f
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: app
operator: In
values:
- nginx
topologyKey: failure-domain.beta.kubernetes.io/zone
containers:
- name: nginx
image: gcr.io/google_containers/nginx-slim:0.8
ports:
- containerPort: 80
name: web
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
- name: logs
mountPath: /logs
volumeClaimTemplates:
- metadata:
name: www
spec:
accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
storageClassName: topology-aware-standard
resources:
requests:
storage: 10Gi
- metadata:
name: logs
spec:
accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
storageClassName: topology-aware-standard
resources:
requests:
storage: 1Gi
&lt;/code>&lt;/pre>&lt;!--
Afterwards, you can see that the volumes were provisioned in zones according to the policies set by the pod:
-->
&lt;p>之后，您可以看到根据 Pod 设置的策略在区域中配置卷：&lt;/p>
&lt;pre>&lt;code>$ kubectl get pv -o=jsonpath='{range .items[*]}{.spec.claimRef.name}{&amp;quot;\t&amp;quot;}{.metadata.labels.failure\-domain\.beta\.kubernetes\.io/zone}{&amp;quot;\n&amp;quot;}{end}'
www-web-0 us-central1-f
logs-web-0 us-central1-f
www-web-1 us-central1-a
logs-web-1 us-central1-a
&lt;/code>&lt;/pre>&lt;!--
## How can I learn more?
-->
&lt;h2 id="我怎样才能了解更多">我怎样才能了解更多？&lt;/h2>
&lt;!--
Official documentation on the topology-aware dynamic provisioning feature is available here:https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode
-->
&lt;p>有关拓扑感知动态供应功能的官方文档可在此处获取：https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode&lt;/p>
&lt;!--
Documentation for CSI drivers is available at https://kubernetes-csi.github.io/docs/
-->
&lt;p>有关 CSI 驱动程序的文档，请访问：https://kubernetes-csi.github.io/docs/&lt;/p>
&lt;!--
## What’s next?
-->
&lt;h2 id="下一步是什么">下一步是什么？&lt;/h2>
&lt;!--
We are actively working on improving this feature to support:
-->
&lt;p>我们正积极致力于改进此功能以支持：&lt;/p>
&lt;!--
* more volume types, including dynamic provisioning for local volumes
* dynamic volume attachable count and capacity limits per node
-->
&lt;ul>
&lt;li>更多卷类型，包括本地卷的动态供应&lt;/li>
&lt;li>动态容量可附加计数和每个节点的容量限制&lt;/li>
&lt;/ul>
&lt;!--
## How do I get involved?
-->
&lt;h2 id="我如何参与">我如何参与？&lt;/h2>
&lt;!--
If you have feedback for this feature or are interested in getting involved with the design and development, join the [Kubernetes Storage Special-Interest-Group](https://github.com/kubernetes/community/tree/master/sig-storage) (SIG). We’re rapidly growing and always welcome new contributors.
-->
&lt;p>如果您对此功能有反馈意见或有兴趣参与设计和开发，请加入 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes 存储特别兴趣小组&lt;/a>（SIG）。我们正在快速成长，并始终欢迎新的贡献者。&lt;/p>
&lt;!--
Special thanks to all the contributors that helped bring this feature to beta, including Cheng Xing ([verult](https://github.com/verult)), Chuqiang Li ([lichuqiang](https://github.com/lichuqiang)), David Zhu ([davidz627](https://github.com/davidz627)), Deep Debroy ([ddebroy](https://github.com/ddebroy)), Jan Šafránek ([jsafrane](https://github.com/jsafrane)), Jordan Liggitt ([liggitt](https://github.com/liggitt)), Michelle Au ([msau42](https://github.com/msau42)), Pengfei Ni ([feiskyer](https://github.com/feiskyer)), Saad Ali ([saad-ali](https://github.com/saad-ali)), Tim Hockin ([thockin](https://github.com/thockin)), and Yecheng Fu ([cofyc](https://github.com/cofyc)).
-->
&lt;p>特别感谢帮助推出此功能的所有贡献者，包括 Cheng Xing (&lt;a href="https://github.com/verult">verult&lt;/a>)、Chuqiang Li (&lt;a href="https://github.com/lichuqiang">lichuqiang&lt;/a>)、David Zhu (&lt;a href="https://github.com/davidz627">davidz627&lt;/a>)、Deep Debroy (&lt;a href="https://github.com/ddebroy">ddebroy&lt;/a>)、Jan Šafránek (&lt;a href="https://github.com/jsafrane">jsafrane&lt;/a>)、Jordan Liggitt (&lt;a href="https://github.com/liggitt">liggitt&lt;/a>)、Michelle Au (&lt;a href="https://github.com/msau42">msau42&lt;/a>)、Pengfei Ni (&lt;a href="https://github.com/feiskyer">feiskyer&lt;/a>)、Saad Ali (&lt;a href="https://github.com/saad-ali">saad-ali&lt;/a>)、Tim Hockin (&lt;a href="https://github.com/thockin">thockin&lt;/a>)，以及 Yecheng Fu (&lt;a href="https://github.com/cofyc">cofyc&lt;/a>)。&lt;/p></description></item><item><title>Blog: Kubernetes v1.12: RuntimeClass 简介</title><link>https://kubernetes.io/zh/blog/2018/10/10/kubernetes-v1.12-runtimeclass-%E7%AE%80%E4%BB%8B/</link><pubDate>Wed, 10 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/10/kubernetes-v1.12-runtimeclass-%E7%AE%80%E4%BB%8B/</guid><description>
&lt;!--
---
layout: blog
title: 'Kubernetes v1.12: Introducing RuntimeClass'
date: 2018-10-10
---
-->
&lt;!--
**Author**: Tim Allclair (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Tim Allclair (Google)&lt;/p>
&lt;!--
Kubernetes originally launched with support for Docker containers running native applications on a Linux host. Starting with [rkt](https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-kubernetes/) in Kubernetes 1.3 more runtimes were coming, which lead to the development of the [Container Runtime Interface](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/) (CRI). Since then, the set of alternative runtimes has only expanded: projects like [Kata Containers](https://katacontainers.io/) and [gVisor](https://github.com/google/gvisor) were announced for stronger workload isolation, and Kubernetes' Windows support has been [steadily progressing](https://kubernetes.io/blog/2018/01/kubernetes-v19-beta-windows-support/).
-->
&lt;p>Kubernetes 最初是为了支持在 Linux 主机上运行本机应用程序的 Docker 容器而创建的。
从 Kubernetes 1.3中的 &lt;a href="https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-kubernetes/">rkt&lt;/a> 开始，更多的运行时间开始涌现，
这导致了&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">容器运行时接口（Container Runtime Interface）&lt;/a>（CRI）的开发。
从那时起，备用运行时集合越来越大：
为了加强工作负载隔离，&lt;a href="https://katacontainers.io/">Kata Containers&lt;/a> 和 &lt;a href="https://github.com/google/gvisor">gVisor&lt;/a> 等项目被发起，
并且 Kubernetes 对 Windows 的支持正在 &lt;a href="https://kubernetes.io/blog/2018/01/kubernetes-v19-beta-windows-support/">稳步发展&lt;/a> 。&lt;/p>
&lt;!--
With runtimes targeting so many different use cases, a clear need for mixed runtimes in a cluster arose. But all these different ways of running containers have brought a new set of problems to deal with:
-->
&lt;p>由于存在诸多针对不同用例的运行时，集群对混合运行时的需求变得明晰起来。
但是，所有这些不同的容器运行方式都带来了一系列新问题要处理：&lt;/p>
&lt;!--
- How do users know which runtimes are available, and select the runtime for their workloads?
- How do we ensure pods are scheduled to the nodes that support the desired runtime?
- Which runtimes support which features, and how can we surface incompatibilities to the user?
- How do we account for the varying resource overheads of the runtimes?
-->
&lt;ul>
&lt;li>用户如何知道哪些运行时可用，并为其工作负荷选择运行时？&lt;/li>
&lt;li>我们如何确保将 Pod 被调度到支持所需运行时的节点上？&lt;/li>
&lt;li>哪些运行时支持哪些功能，以及我们如何向用户显示不兼容性？&lt;/li>
&lt;li>我们如何考虑运行时的各种资源开销？&lt;/li>
&lt;/ul>
&lt;!--
**RuntimeClass** aims to solve these issues.
-->
&lt;p>&lt;strong>RuntimeClass&lt;/strong> 旨在解决这些问题。&lt;/p>
&lt;!--
## RuntimeClass in Kubernetes 1.12
-->
&lt;h2 id="kubernetes-1-12-中的-runtimeclass">Kubernetes 1.12 中的 RuntimeClass&lt;/h2>
&lt;!--
RuntimeClass was recently introduced as an alpha feature in Kubernetes 1.12. The initial implementation focuses on providing a runtime selection API, and paves the way to address the other open problems.
-->
&lt;p>最近，RuntimeClass 在 Kubernetes 1.12 中作为 alpha 功能引入。
最初的实现侧重于提供运行时选择 API ，并为解决其他未解决的问题铺平道路。&lt;/p>
&lt;!--
The RuntimeClass resource represents a container runtime supported in a Kubernetes cluster. The cluster provisioner sets up, configures, and defines the concrete runtimes backing the RuntimeClass. In its current form, a RuntimeClassSpec holds a single field, the **RuntimeHandler**. The RuntimeHandler is interpreted by the CRI implementation running on a node, and mapped to the actual runtime configuration. Meanwhile the PodSpec has been expanded with a new field, **RuntimeClassName**, which names the RuntimeClass that should be used to run the pod.
-->
&lt;p>RuntimeClass 资源代表 Kubernetes 集群中支持的容器运行时。
集群制备组件安装、配置和定义支持 RuntimeClass 的具体运行时。
在 RuntimeClassSpec 的当前形式中，只有一个字段，即 &lt;strong>RuntimeHandler&lt;/strong>。
RuntimeHandler 由在节点上运行的 CRI 实现解释，并映射到实际的运行时配置。
同时，PodSpec 被扩展添加了一个新字段 &lt;strong>RuntimeClassName&lt;/strong>，命名应该用于运行 Pod 的 RuntimeClass。&lt;/p>
&lt;!--
Why is RuntimeClass a pod level concept? The Kubernetes resource model expects certain resources to be shareable between containers in the pod. If the pod is made up of different containers with potentially different resource models, supporting the necessary level of resource sharing becomes very challenging. For example, it is extremely difficult to support a loopback (localhost) interface across a VM boundary, but this is a common model for communication between two containers in a pod.
-->
&lt;p>为什么 RuntimeClass 是 Pod 级别的概念？
Kubernetes 资源模型期望 Pod 中的容器之间可以共享某些资源。
如果 Pod 由具有不同资源模型的不同容器组成，支持必要水平的资源共享变得非常具有挑战性。
例如，要跨 VM 边界支持本地回路（localhost）接口非常困难，但这是 Pod 中两个容器之间通信的通用模型。&lt;/p>
&lt;!--
## What's next?
-->
&lt;h2 id="下一步是什么">下一步是什么？&lt;/h2>
&lt;!--
The RuntimeClass resource is an important foundation for surfacing runtime properties to the control plane. For example, to implement scheduler support for clusters with heterogeneous nodes supporting different runtimes, we might add [NodeAffinity](/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity) terms to the RuntimeClass definition. Another area to address is managing the variable resource requirements to run pods of different runtimes. The [Pod Overhead proposal](https://docs.google.com/document/d/1EJKT4gyl58-kzt2bnwkv08MIUZ6lkDpXcxkHqCvvAp4/preview) was an early take on this that aligns nicely with the RuntimeClass design, and may be pursued further.
-->
&lt;p>RuntimeClass 资源是将运行时属性显示到控制平面的重要基础。
例如，要对具有支持不同运行时间的异构节点的群集实施调度程序支持，我们可以在 RuntimeClass 定义中添加
&lt;a href="https://kubernetes.io/zh/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity">NodeAffinity&lt;/a>条件。
另一个需要解决的领域是管理可变资源需求以运行不同运行时的 Pod。
&lt;a href="https://docs.google.com/document/d/1EJKT4gyl58-kzt2bnwkv08MIUZ6lkDpXcxkHqCvvAp4/preview">Pod Overhead 提案&lt;/a>
是一项较早的尝试，与 RuntimeClass 设计非常吻合，并且可能会进一步推广。&lt;/p>
&lt;!--
Many other RuntimeClass extensions have also been proposed, and will be revisited as the feature continues to develop and mature. A few more extensions that are being considered include:
-->
&lt;p>人们还提出了许多其他 RuntimeClass 扩展，随着功能的不断发展和成熟，我们会重新讨论这些提议。
正在考虑的其他扩展包括：&lt;/p>
&lt;!--
- Surfacing optional features supported by runtimes, and better visibility into errors caused by incompatible features.
- Automatic runtime or feature discovery, to support scheduling decisions without manual configuration.
- Standardized or conformant RuntimeClass names that define a set of properties that should be supported across clusters with RuntimeClasses of the same name.
- Dynamic registration of additional runtimes, so users can install new runtimes on existing clusters with no downtime.
- "Fitting" a RuntimeClass to a pod's requirements. For instance, specifying runtime properties and letting the system match an appropriate RuntimeClass, rather than explicitly assigning a RuntimeClass by name.
-->
&lt;ul>
&lt;li>提供运行时支持的可选功能，并更好地查看由不兼容功能导致的错误。&lt;/li>
&lt;li>自动运行时或功能发现，支持无需手动配置的调度决策。&lt;/li>
&lt;li>标准化或一致的 RuntimeClass 名称，用于定义一组具有相同名称的 RuntimeClass 的集群应支持的属性。&lt;/li>
&lt;li>动态注册附加的运行时，因此用户可以在不停机的情况下在现有群集上安装新的运行时。&lt;/li>
&lt;li>根据 Pod 的要求“匹配” RuntimeClass。
例如，指定运行时属性并使系统与适当的 RuntimeClass 匹配，而不是通过名称显式分配 RuntimeClass。&lt;/li>
&lt;/ul>
&lt;!--
RuntimeClass will be under active development at least through 2019, and we’re excited to see the feature take shape, starting with the RuntimeClass alpha in Kubernetes 1.12.
-->
&lt;p>至少要到2019年，RuntimeClass 才会得到积极的开发，我们很高兴看到从 Kubernetes 1.12 中的 RuntimeClass alpha 开始，此功能得以形成。&lt;/p>
&lt;!--
## Learn More
-->
&lt;h2 id="学到更多">学到更多&lt;/h2>
&lt;!--
- Take it for a spin! As an alpha feature, there are some additional setup steps to use RuntimeClass. Refer to the [RuntimeClass documentation](/docs/concepts/containers/runtime-class/#runtime-class) for how to get it running.
- Check out the [RuntimeClass Kubernetes Enhancement Proposal](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/runtime-class.md) for more nitty-gritty design details.
- The [Sandbox Isolation Level Decision](https://docs.google.com/document/d/1fe7lQUjYKR0cijRmSbH_y0_l3CYPkwtQa5ViywuNo8Q/preview) documents the thought process that initially went into making RuntimeClass a pod-level choice.
- Join the discussions and help shape the future of RuntimeClass with the [SIG-Node community](https://github.com/kubernetes/community/tree/master/sig-node)
-->
&lt;ul>
&lt;li>试试吧！ 作为Alpha功能，还有一些其他设置步骤可以使用RuntimeClass。
有关如何使其运行，请参考 &lt;a href="https://kubernetes.io/zh/docs/concepts/containers/runtime-class/#runtime-class">RuntimeClass文档&lt;/a> 。&lt;/li>
&lt;li>查看 &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/runtime-class.md">RuntimeClass Kubernetes 增强建议&lt;/a> 以获取更多细节设计细节。&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/document/d/1fe7lQUjYKR0cijRmSbH_y0_l3CYPkwtQa5ViywuNo8Q/preview">沙盒隔离级别决策&lt;/a>
记录了最初使 RuntimeClass 成为 Pod 级别选项的思考过程。&lt;/li>
&lt;li>加入讨论，并通过 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG-Node社区&lt;/a> 帮助塑造 RuntimeClass 的未来。&lt;/li>
&lt;/ul></description></item><item><title>Blog: KubeDirector：在 Kubernetes 上运行复杂状态应用程序的简单方法</title><link>https://kubernetes.io/zh/blog/2018/10/03/kubedirector%E5%9C%A8-kubernetes-%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%A4%8D%E6%9D%82%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/03/kubedirector%E5%9C%A8-kubernetes-%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%A4%8D%E6%9D%82%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/</guid><description>
&lt;!--
layout: blog
title: 'KubeDirector: The easy way to run complex stateful applications on Kubernetes'
date: 2018-10-03
-->
&lt;!--
**Author**: Thomas Phelan (BlueData)
-->
&lt;p>&lt;strong>作者&lt;/strong>：Thomas Phelan（BlueData）&lt;/p>
&lt;!--
KubeDirector is an open source project designed to make it easy to run complex stateful scale-out application clusters on Kubernetes. KubeDirector is built using the custom resource definition (CRD) framework and leverages the native Kubernetes API extensions and design philosophy. This enables transparent integration with Kubernetes user/resource management as well as existing clients and tools.
-->
&lt;p>KubeDirector 是一个开源项目，旨在简化在 Kubernetes 上运行复杂的有状态扩展应用程序集群。KubeDirector 使用自定义资源定义（CRD）
框架构建，并利用了本地 Kubernetes API 扩展和设计哲学。这支持与 Kubernetes 用户/资源 管理以及现有客户端和工具的透明集成。&lt;/p>
&lt;!--
We recently [introduced the KubeDirector project](https://medium.com/@thomas_phelan/operation-stateful-introducing-bluek8s-and-kubernetes-director-aa204952f619/), as part of a broader open source Kubernetes initiative we call BlueK8s. I’m happy to announce that the pre-alpha
code for [KubeDirector](https://github.com/bluek8s/kubedirector/) is now available. And in this blog post, I’ll show how it works.
-->
&lt;p>我们最近&lt;a href="https://medium.com/@thomas_phelan/operation-stateful-introducing-bluek8s-and-kubernetes-director-aa204952f619/">介绍了 KubeDirector 项目&lt;/a>，作为我们称为 BlueK8s 的更广泛的 Kubernetes 开源项目的一部分。我很高兴地宣布 &lt;a href="https://github.com/bluek8s/kubedirector/">KubeDirector&lt;/a> 的
pre-alpha 代码现在已经可用。在这篇博客文章中，我将展示它是如何工作的。&lt;/p>
&lt;!--
KubeDirector provides the following capabilities:
-->
&lt;p>KubeDirector 提供以下功能：&lt;/p>
&lt;!--
* The ability to run non-cloud native stateful applications on Kubernetes without modifying the code. In other words, it’s not necessary to decompose these existing applications to fit a microservices design pattern.
* Native support for preserving application-specific configuration and state.
* An application-agnostic deployment pattern, minimizing the time to onboard new stateful applications to Kubernetes.
-->
&lt;ul>
&lt;li>无需修改代码即可在 Kubernetes 上运行非云原生有状态应用程序。换句话说，不需要分解这些现有的应用程序来适应微服务设计模式。&lt;/li>
&lt;li>本机支持保存特定于应用程序的配置和状态。&lt;/li>
&lt;li>与应用程序无关的部署模式，最大限度地减少将新的有状态应用程序装载到 Kubernetes 的时间。&lt;/li>
&lt;/ul>
&lt;!--
KubeDirector enables data scientists familiar with data-intensive distributed applications such as Hadoop, Spark, Cassandra, TensorFlow, Caffe2, etc. to run these applications on Kubernetes -- with a minimal learning curve and no need to write GO code. The applications controlled by KubeDirector are defined by some basic metadata and an associated package of configuration artifacts. The application metadata is referred to as a KubeDirectorApp resource.
-->
&lt;p>KubeDirector 使熟悉数据密集型分布式应用程序（如 Hadoop、Spark、Cassandra、TensorFlow、Caffe2 等）的数据科学家能够在 Kubernetes 上运行这些应用程序 -- 只需极少的学习曲线，无需编写 GO 代码。由 KubeDirector 控制的应用程序由一些基本元数据和相关的配置工件包定义。应用程序元数据称为 KubeDirectorApp 资源。&lt;/p>
&lt;!--
To understand the components of KubeDirector, clone the repository on [GitHub](https://github.com/bluek8s/kubedirector/) using a command similar to:
-->
&lt;p>要了解 KubeDirector 的组件，请使用类似于以下的命令在 &lt;a href="https://github.com/bluek8s/kubedirector/">GitHub&lt;/a> 上克隆存储库：&lt;/p>
&lt;pre>&lt;code>git clone http://&amp;lt;userid&amp;gt;@github.com/bluek8s/kubedirector.
&lt;/code>&lt;/pre>&lt;!--
The KubeDirectorApp definition for the Spark 2.2.1 application is located
in the file `kubedirector/deploy/example_catalog/cr-app-spark221e2.json`.
-->
&lt;p>Spark 2.2.1 应用程序的 KubeDirectorApp 定义位于文件 &lt;code>kubedirector/deploy/example_catalog/cr-app-spark221e2.json&lt;/code> 中。&lt;/p>
&lt;pre>&lt;code>~&amp;gt; cat kubedirector/deploy/example_catalog/cr-app-spark221e2.json
{
&amp;quot;apiVersion&amp;quot;: &amp;quot;kubedirector.bluedata.io/v1alpha1&amp;quot;,
&amp;quot;kind&amp;quot;: &amp;quot;KubeDirectorApp&amp;quot;,
&amp;quot;metadata&amp;quot;: {
&amp;quot;name&amp;quot; : &amp;quot;spark221e2&amp;quot;
},
&amp;quot;spec&amp;quot; : {
&amp;quot;systemctlMounts&amp;quot;: true,
&amp;quot;config&amp;quot;: {
&amp;quot;node_services&amp;quot;: [
{
&amp;quot;service_ids&amp;quot;: [
&amp;quot;ssh&amp;quot;,
&amp;quot;spark&amp;quot;,
&amp;quot;spark_master&amp;quot;,
&amp;quot;spark_worker&amp;quot;
],
…
&lt;/code>&lt;/pre>&lt;!--
The configuration of an application cluster is referred to as a KubeDirectorCluster resource. The
KubeDirectorCluster definition for a sample Spark 2.2.1 cluster is located in the file
`kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml`.
-->
&lt;p>应用程序集群的配置称为 KubeDirectorCluster 资源。示例 Spark 2.2.1 集群的 KubeDirectorCluster 定义位于文件
&lt;code>kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml&lt;/code> 中。&lt;/p>
&lt;pre>&lt;code>~&amp;gt; cat kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml
apiVersion: &amp;quot;kubedirector.bluedata.io/v1alpha1&amp;quot;
kind: &amp;quot;KubeDirectorCluster&amp;quot;
metadata:
name: &amp;quot;spark221e2&amp;quot;
spec:
app: spark221e2
roles:
- name: controller
replicas: 1
resources:
requests:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
limits:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
- name: worker
replicas: 2
resources:
requests:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
limits:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
- name: jupyter
…
&lt;/code>&lt;/pre>&lt;!--
## Running Spark on Kubernetes with KubeDirector
-->
&lt;h2 id="使用-kubedirector-在-kubernetes-上运行-spark">使用 KubeDirector 在 Kubernetes 上运行 Spark&lt;/h2>
&lt;!--
With KubeDirector, it’s easy to run Spark clusters on Kubernetes.
-->
&lt;p>使用 KubeDirector，可以轻松在 Kubernetes 上运行 Spark 集群。&lt;/p>
&lt;!--
First, verify that Kubernetes (version 1.9 or later) is running, using the command `kubectl version`
-->
&lt;p>首先，使用命令 &lt;code>kubectl version&lt;/code> 验证 Kubernetes（版本 1.9 或更高）是否正在运行&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl version
Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;11&amp;quot;, GitVersion:&amp;quot;v1.11.3&amp;quot;, GitCommit:&amp;quot;a4529464e4629c21224b3d52edfe0ea91b072862&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-09-09T18:02:47Z&amp;quot;, GoVersion:&amp;quot;go1.10.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;11&amp;quot;, GitVersion:&amp;quot;v1.11.3&amp;quot;, GitCommit:&amp;quot;a4529464e4629c21224b3d52edfe0ea91b072862&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-09-09T17:53:03Z&amp;quot;, GoVersion:&amp;quot;go1.10.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
&lt;/code>&lt;/pre>&lt;!--
Deploy the KubeDirector service and the example KubeDirectorApp resource definitions with the commands:
-->
&lt;p>使用以下命令部署 KubeDirector 服务和示例 KubeDirectorApp 资源定义：&lt;/p>
&lt;pre>&lt;code>cd kubedirector
make deploy
&lt;/code>&lt;/pre>&lt;!--
These will start the KubeDirector pod:
-->
&lt;p>这些将启动 KubeDirector pod：&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get pods
NAME READY STATUS RESTARTS AGE
kubedirector-58cf59869-qd9hb 1/1 Running 0 1m
&lt;/code>&lt;/pre>&lt;!--
List the installed KubeDirector applications with `kubectl get KubeDirectorApp`
-->
&lt;p>&lt;code>kubectl get KubeDirectorApp&lt;/code> 列出中已安装的 KubeDirector 应用程序&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get KubeDirectorApp
NAME AGE
cassandra311 30m
spark211up 30m
spark221e2 30m
&lt;/code>&lt;/pre>&lt;!--
Now you can launch a Spark 2.2.1 cluster using the example KubeDirectorCluster file and the
`kubectl create -f deploy/example_clusters/cr-cluster-spark211up.yaml` command.
Verify that the Spark cluster has been started:
-->
&lt;p>现在，您可以使用示例 KubeDirectorCluster 文件和 &lt;code>kubectl create -f deploy/example_clusters/cr-cluster-spark211up.yaml&lt;/code> 命令
启动 Spark 2.2.1 集群。验证 Spark 集群已经启动:&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get pods
NAME READY STATUS RESTARTS AGE
kubedirector-58cf59869-djdwl 1/1 Running 0 19m
spark221e2-controller-zbg4d-0 1/1 Running 0 23m
spark221e2-jupyter-2km7q-0 1/1 Running 0 23m
spark221e2-worker-4gzbz-0 1/1 Running 0 23m
spark221e2-worker-4gzbz-1 1/1 Running 0 23m
&lt;/code>&lt;/pre>&lt;!--
The running services now include the Spark services:
-->
&lt;p>现在运行的服务包括 Spark 服务：&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get service
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubedirector ClusterIP 10.98.234.194 &amp;lt;none&amp;gt; 60000/TCP 1d
kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 1d
svc-spark221e2-5tg48 ClusterIP None &amp;lt;none&amp;gt; 8888/TCP 21s
svc-spark221e2-controller-tq8d6-0 NodePort 10.104.181.123 &amp;lt;none&amp;gt; 22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP 20s
svc-spark221e2-jupyter-6989v-0 NodePort 10.105.227.249 &amp;lt;none&amp;gt; 22:30632/TCP,8888:30355/TCP 20s
svc-spark221e2-worker-d9892-0 NodePort 10.107.131.165 &amp;lt;none&amp;gt; 22:30358/TCP,8081:32144/TCP 20s
svc-spark221e2-worker-d9892-1 NodePort 10.110.88.221 &amp;lt;none&amp;gt; 22:30294/TCP,8081:31436/TCP 20s
&lt;/code>&lt;/pre>&lt;!--
Pointing the browser at port 31533 connects to the Spark Master UI:
-->
&lt;p>将浏览器指向端口 31533 连接到 Spark 主节点 UI：&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-10-03-kubedirector/kubedirector.png" alt="kubedirector">&lt;/p>
&lt;!--
That’s all there is to it!
In fact, in the example above we also deployed a Jupyter notebook along with the Spark cluster.
-->
&lt;p>就是这样!
事实上，在上面的例子中，我们还部署了一个 Jupyter notebook 和 Spark 集群。&lt;/p>
&lt;!--
To start another application (e.g. Cassandra), just specify another KubeDirectorApp file:
-->
&lt;p>要启动另一个应用程序（例如 Cassandra），只需指定另一个 KubeDirectorApp 文件：&lt;/p>
&lt;pre>&lt;code>kubectl create -f deploy/example_clusters/cr-cluster-cassandra311.yaml
&lt;/code>&lt;/pre>&lt;!--
See the running Cassandra cluster:
-->
&lt;p>查看正在运行的 Cassandra 集群：&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get pods
NAME READY STATUS RESTARTS AGE
cassandra311-seed-v24r6-0 1/1 Running 0 1m
cassandra311-seed-v24r6-1 1/1 Running 0 1m
cassandra311-worker-rqrhl-0 1/1 Running 0 1m
cassandra311-worker-rqrhl-1 1/1 Running 0 1m
kubedirector-58cf59869-djdwl 1/1 Running 0 1d
spark221e2-controller-tq8d6-0 1/1 Running 0 22m
spark221e2-jupyter-6989v-0 1/1 Running 0 22m
spark221e2-worker-d9892-0 1/1 Running 0 22m
spark221e2-worker-d9892-1 1/1 Running 0 22m
&lt;/code>&lt;/pre>&lt;!--
Now you have a Spark cluster (with a Jupyter notebook) and a Cassandra cluster running on Kubernetes.
Use `kubectl get service` to see the set of services.
-->
&lt;p>现在，您有一个 Spark 集群（带有 Jupyter notebook ）和一个运行在 Kubernetes 上的 Cassandra 集群。
使用 &lt;code>kubectl get service&lt;/code> 查看服务集。&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get service
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubedirector ClusterIP 10.98.234.194 &amp;lt;none&amp;gt; 60000/TCP 1d
kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 1d
svc-cassandra311-seed-v24r6-0 NodePort 10.96.94.204 &amp;lt;none&amp;gt; 22:31131/TCP,9042:30739/TCP 3m
svc-cassandra311-seed-v24r6-1 NodePort 10.106.144.52 &amp;lt;none&amp;gt; 22:30373/TCP,9042:32662/TCP 3m
svc-cassandra311-vhh29 ClusterIP None &amp;lt;none&amp;gt; 8888/TCP 3m
svc-cassandra311-worker-rqrhl-0 NodePort 10.109.61.194 &amp;lt;none&amp;gt; 22:31832/TCP,9042:31962/TCP 3m
svc-cassandra311-worker-rqrhl-1 NodePort 10.97.147.131 &amp;lt;none&amp;gt; 22:31454/TCP,9042:31170/TCP 3m
svc-spark221e2-5tg48 ClusterIP None &amp;lt;none&amp;gt; 8888/TCP 24m
svc-spark221e2-controller-tq8d6-0 NodePort 10.104.181.123 &amp;lt;none&amp;gt; 22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP 24m
svc-spark221e2-jupyter-6989v-0 NodePort 10.105.227.249 &amp;lt;none&amp;gt; 22:30632/TCP,8888:30355/TCP 24m
svc-spark221e2-worker-d9892-0 NodePort 10.107.131.165 &amp;lt;none&amp;gt; 22:30358/TCP,8081:32144/TCP 24m
svc-spark221e2-worker-d9892-1 NodePort 10.110.88.221 &amp;lt;none&amp;gt; 22:30294/TCP,8081:31436/TCP 24m
&lt;/code>&lt;/pre>&lt;!--
## Get Involved
-->
&lt;h2 id="参与其中">参与其中&lt;/h2>
&lt;!--
KubeDirector is a fully open source, Apache v2 licensed, project – the first of multiple open source projects within a broader initiative we call BlueK8s.
The pre-alpha code for KubeDirector has just been released and we would love for you to join the growing community of developers, contributors, and adopters.
Follow [@BlueK8s](https://twitter.com/BlueK8s/) on Twitter and get involved through these channels:
-->
&lt;p>KubeDirector 是一个完全开放源码的 Apache v2 授权项目 – 在我们称为 BlueK8s 的更广泛的计划中，它是多个开放源码项目中的第一个。
KubeDirector 的 pre-alpha 代码刚刚发布，我们希望您加入到不断增长的开发人员、贡献者和使用者社区。
在 Twitter 上关注 &lt;a href="https://twitter.com/BlueK8s/">@BlueK8s&lt;/a>，并通过以下渠道参与:&lt;/p>
&lt;!--
* KubeDirector [chat room on Slack](https://join.slack.com/t/bluek8s/shared_invite/enQtNDUwMzkwODY5OTM4LTRhYmRmZmE4YzY3OGUzMjA1NDg0MDVhNDQ2MGNkYjRhM2RlMDNjMTI1NDQyMjAzZGVlMDFkNThkNGFjZGZjMGY/)
* KubeDirector [GitHub repo](https://github.com/bluek8s/kubedirector/)
-->
&lt;ul>
&lt;li>KubeDirector &lt;a href="https://join.slack.com/t/bluek8s/shared_invite/enQtNDUwMzkwODY5OTM4LTRhYmRmZmE4YzY3OGUzMjA1NDg0MDVhNDQ2MGNkYjRhM2RlMDNjMTI1NDQyMjAzZGVlMDFkNThkNGFjZGZjMGY/">Slack 聊天室&lt;/a>&lt;/li>
&lt;li>KubeDirector &lt;a href="https://github.com/bluek8s/kubedirector/">GitHub 仓库&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: 在 Kubernetes 上对 gRPC 服务器进行健康检查</title><link>https://kubernetes.io/zh/blog/2018/10/01/%E5%9C%A8-kubernetes-%E4%B8%8A%E5%AF%B9-grpc-%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9B%E8%A1%8C%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5/</link><pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/01/%E5%9C%A8-kubernetes-%E4%B8%8A%E5%AF%B9-grpc-%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9B%E8%A1%8C%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5/</guid><description>
&lt;!--
---
layout: blog
title: 'Health checking gRPC servers on Kubernetes'
date: 2018-10-01
---
--->
&lt;!--
**Author**: [Ahmet Alp Balkan](https://twitter.com/ahmetb) (Google)
--->
&lt;p>&lt;strong>作者&lt;/strong>： &lt;a href="https://twitter.com/ahmetb">Ahmet Alp Balkan&lt;/a> (Google)&lt;/p>
&lt;!--
[gRPC](https://grpc.io) is on its way to becoming the lingua franca for
communication between cloud-native microservices. If you are deploying gRPC
applications to Kubernetes today, you may be wondering about the best way to
configure health checks. In this article, we will talk about
[grpc-health-probe](https://github.com/grpc-ecosystem/grpc-health-probe/), a
Kubernetes-native way to health check gRPC apps.
--->
&lt;p>&lt;a href="https://grpc.io">gRPC&lt;/a> 将成为本地云微服务间进行通信的通用语言。如果您现在将 gRPC 应用程序部署到 Kubernetes，您可能会想要了解配置健康检查的最佳方法。在本文中，我们将介绍 &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">grpc-health-probe&lt;/a>，这是 Kubernetes 原生的健康检查 gRPC 应用程序的方法。&lt;/p>
&lt;!--
If you're unfamiliar, Kubernetes [health
checks](/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)
(liveness and readiness probes) is what's keeping your applications available
while you're sleeping. They detect unresponsive pods, mark them unhealthy, and
cause these pods to be restarted or rescheduled.
--->
&lt;p>如果您不熟悉，Kubernetes的 &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">健康检查&lt;/a>（存活探针和就绪探针）可以使您的应用程序在睡眠时保持可用状态。当检测到没有回应的 Pod 时，会将其标记为不健康，并使这些 Pod 重新启动或重新安排。&lt;/p>
&lt;!--
Kubernetes [does not
support](https://github.com/kubernetes/kubernetes/issues/21493) gRPC health
checks natively. This leaves the gRPC developers with the following three
approaches when they deploy to Kubernetes:
[![options for health checking grpc on kubernetes today](/images/blog/2019-09-30-health-checking-grpc/options.png)](/images/blog/2019-09-30-health-checking-grpc/options.png)
--->
&lt;p>Kubernetes 原本 &lt;a href="https://github.com/kubernetes/kubernetes/issues/21493">不支持&lt;/a> gRPC 健康检查。gRPC 的开发人员在 Kubernetes 中部署时可以采用以下三种方法：&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/options.png">&lt;img src="https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/options.png" alt="当前在 kubernetes 上进行 gRPC 健康检查的选项">&lt;/a>&lt;/p>
&lt;!--
1. **httpGet probe:** Cannot be natively used with gRPC. You need to refactor
your app to serve both gRPC and HTTP/1.1 protocols (on different port
numbers).
2. **tcpSocket probe:** Opening a socket to gRPC server is not meaningful,
since it cannot read the response body.
3. **exec probe:** This invokes a program in a container's ecosystem
periodically. In the case of gRPC, this means you implement a health RPC
yourself, then write and ship a client tool with your container.
Can we do better? Absolutely.
--->
&lt;ol>
&lt;li>&lt;strong>httpGet prob：&lt;/strong> 不能与 gRPC 一起使用。您需要重构您的应用程序，必须同时支持 gRPC 和 HTTP/1.1 协议（在不同的端口号上）。&lt;/li>
&lt;li>&lt;strong>tcpSocket probe：&lt;/strong> 打开 gRPC 服务器的 Socket 是没有意义的，因为它无法读取响应主体。&lt;/li>
&lt;li>&lt;strong>exec probe：&lt;/strong> 将定期调用容器生态系统中的程序。对于 gRPC，这意味着您要自己实现健康 RPC，然后使用容器编写并交付客户端工具。&lt;/li>
&lt;/ol>
&lt;p>我们可以做得更好吗？这是肯定的。&lt;/p>
&lt;!--
## Introducing “grpc-health-probe”
To standardize the "exec probe" approach mentioned above, we need:
- a **standard** health check "protocol" that can be implemented in any gRPC
server easily.
- a **standard** health check "tool" that can query the health protocol easily.
--->
&lt;h2 id="介绍-grpc-health-probe">介绍 “grpc-health-probe”&lt;/h2>
&lt;p>为了使上述 &amp;quot;exec probe&amp;quot; 方法标准化，我们需要：&lt;/p>
&lt;ul>
&lt;li>可以在任何 gRPC 服务器中轻松实现的 &lt;strong>标准&lt;/strong> 健康检查 &amp;quot;协议&amp;quot; 。&lt;/li>
&lt;li>一种 &lt;strong>标准&lt;/strong> 健康检查 &amp;quot;工具&amp;quot; ，可以轻松查询健康协议。&lt;/li>
&lt;/ul>
&lt;!--
Thankfully, gRPC has a [standard health checking
protocol](https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md). It
can be used easily from any language. Generated code and the utilities for
setting the health status are shipped in nearly all language implementations of
gRPC.
--->
&lt;p>幸运的是，gRPC 具有 &lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md">标准的健康检查协议&lt;/a>。可以用任何语言轻松调用它。几乎所有实现 gRPC 的语言都附带了生成的代码和用于设置健康状态的实用程序。&lt;/p>
&lt;!--
If you
[implement](https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto)
this health check protocol in your gRPC apps, you can then use a standard/common
tool to invoke this `Check()` method to determine server status.
--->
&lt;p>如果您在 gRPC 应用程序中 &lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto">实现&lt;/a> 此健康检查协议，那么可以使用标准或通用工具调用 &lt;code>Check()&lt;/code> 方法来确定服务器状态。&lt;/p>
&lt;!--
The next thing you need is the "standard tool", and it's the
[**grpc-health-probe**](https://github.com/grpc-ecosystem/grpc-health-probe/).
--->
&lt;p>接下来您需要的是 &amp;quot;标准工具&amp;quot; &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">&lt;strong>grpc-health-probe&lt;/strong>&lt;/a>。&lt;/p>
&lt;a href='https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/grpc_health_probe.png'>
&lt;img width="768" title='grpc-health-probe on kubernetes'
src='https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/grpc_health_probe.png'/>
&lt;/a>
&lt;!--
With this tool, you can use the same health check configuration in all your gRPC
applications. This approach requires you to:
--->
&lt;p>使用此工具，您可以在所有 gRPC 应用程序中使用相同的健康检查配置。这种方法有以下要求：&lt;/p>
&lt;!--
1. Find the gRPC "health" module in your favorite language and start using it
(example [Go library](https://godoc.org/github.com/grpc/grpc-go/health)).
2. Ship the
[grpc_health_probe](https://github.com/grpc-ecosystem/grpc-health-probe/)
binary in your container.
3. [Configure](https://github.com/grpc-ecosystem/grpc-health-probe/tree/1329d682b4232c102600b5e7886df8ffdcaf9e26#example-grpc-health-checking-on-kubernetes)
Kubernetes "exec" probe to invoke the "grpc_health_probe" tool in the
container.
--->
&lt;ol>
&lt;li>用您喜欢的语言找到 gRPC 的 &amp;quot;健康&amp;quot; 模块并开始使用它（例如 &lt;a href="https://godoc.org/github.com/grpc/grpc-go/health">Go 库&lt;/a>）。&lt;/li>
&lt;li>将二进制文件 &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">grpc_health_probe&lt;/a> 送到容器中。&lt;/li>
&lt;li>&lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/tree/1329d682b4232c102600b5e7886df8ffdcaf9e26#example-grpc-health-checking-on-kubernetes">配置&lt;/a> Kubernetes 的 &amp;quot;exec&amp;quot; 检查模块来调用容器中的 &amp;quot;grpc_health_probe&amp;quot; 工具。&lt;/li>
&lt;/ol>
&lt;!--
In this case, executing "grpc_health_probe" will call your gRPC server over
`localhost`, since they are in the same pod.
--->
&lt;p>在这种情况下，执行 &amp;quot;grpc_health_probe&amp;quot; 将通过 &lt;code>localhost&lt;/code> 调用您的 gRPC 服务器，因为它们位于同一个容器中。&lt;/p>
&lt;!--
## What's next
**grpc-health-probe** project is still in its early days and it needs your
feedback. It supports a variety of features like communicating with TLS servers
and configurable connection/RPC timeouts.
--->
&lt;h2 id="下一步工作">下一步工作&lt;/h2>
&lt;p>&lt;strong>grpc-health-probe&lt;/strong> 项目仍处于初期阶段，需要您的反馈。它支持多种功能，例如与 TLS 服务器通信和配置延时连接/RPC。&lt;/p>
&lt;!--
If you are running a gRPC server on Kubernetes today, try using the gRPC Health
Protocol and try the grpc-health-probe in your deployments, and [give
feedback](https://github.com/grpc-ecosystem/grpc-health-probe/).
--->
&lt;p>如果您最近要在 Kubernetes 上运行 gRPC 服务器，请尝试使用 gRPC Health Protocol，并在您的 Deployment 中尝试 grpc-health-probe，然后 &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">进行反馈&lt;/a>。&lt;/p>
&lt;!--
## Further reading
- Protocol: [GRPC Health Checking Protocol](https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md) ([health.proto](https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto))
- Documentation: [Kubernetes liveness and readiness probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)
- Article: [Advanced Kubernetes Health Check Patterns](https://ahmet.im/blog/advanced-kubernetes-health-checks/)
--->
&lt;h2 id="更多内容">更多内容&lt;/h2>
&lt;ul>
&lt;li>协议： &lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md">GRPC Health Checking Protocol&lt;/a> (&lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto">health.proto&lt;/a>)&lt;/li>
&lt;li>文档： &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">Kubernetes 存活和就绪探针&lt;/a>&lt;/li>
&lt;li>文章： &lt;a href="https://ahmet.im/blog/advanced-kubernetes-health-checks/">升级版 Kubernetes 健康检查模式&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: The Machines Can Do the Work, a Story of Kubernetes Testing, CI, and Automating the Contributor Experience</title><link>https://kubernetes.io/zh/blog/2018/08/29/the-machines-can-do-the-work-a-story-of-kubernetes-testing-ci-and-automating-the-contributor-experience/</link><pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/08/29/the-machines-can-do-the-work-a-story-of-kubernetes-testing-ci-and-automating-the-contributor-experience/</guid><description>
&lt;hr>
&lt;h2 id="date-2019-08-29">layout: blog
title: '机器可以完成这项工作，一个关于 kubernetes 测试、CI 和自动化贡献者体验的故事'
date: 2019-08-29&lt;/h2>
&lt;!--
**Author**: Aaron Crickenberger (Google) and Benjamin Elder (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>：Aaron Crickenberger（谷歌）和 Benjamin Elder（谷歌）&lt;/p>
&lt;!--
_“Large projects have a lot of less exciting, yet, hard work. We value time spent automating repetitive work more highly than toil. Where that work cannot be automated, it is our culture to recognize and reward all types of contributions. However, heroism is not sustainable.”_ - [Kubernetes Community Values](https://git.k8s.io/community/values.md#automation-over-process)
-->
&lt;p>&lt;em>”大型项目有很多不那么令人兴奋，但却很辛苦的工作。比起辛苦工作，我们更重视把时间花在自动化重复性工作上，如果这项工作无法实现自动化，我们的文化就是承认并奖励所有类型的贡献。然而，英雄主义是不可持续的。“&lt;/em> - &lt;a href="https://git.k8s.io/community/values.md#automation-over-process">Kubernetes Community Values&lt;/a>&lt;/p>
&lt;!--
Like many open source projects, Kubernetes is hosted on GitHub. We felt the barrier to participation would be lowest if the project lived where developers already worked, using tools and processes developers already knew. Thus the project embraced the service fully: it was the basis of our workflow, our issue tracker, our documentation, our blog platform, our team structure, and more.
-->
&lt;p>像许多开源项目一样，Kubernetes 托管在 GitHub 上。 如果项目位于在开发人员已经工作的地方，使用的开发人员已经知道的工具和流程，那么参与的障碍将是最低的。 因此，该项目完全接受了这项服务：它是我们工作流程，问题跟踪，文档，博客平台，团队结构等的基础。&lt;/p>
&lt;!--
This strategy worked. It worked so well that the project quickly scaled past its contributors’ capacity as humans. What followed was an incredible journey of automation and innovation. We didn’t just need to rebuild our airplane mid-flight without crashing, we needed to convert it into a rocketship and launch into orbit. We needed machines to do the work.
-->
&lt;p>这个策略奏效了。 它运作良好，以至于该项目迅速超越了其贡献者的人类能力。 接下来是一次令人难以置信的自动化和创新之旅。 我们不仅需要在飞行途中重建我们的飞机而不会崩溃，我们需要将其转换为火箭飞船并发射到轨道。 我们需要机器来完成这项工作。&lt;/p>
&lt;!--
## The Work
-->
&lt;p>##　工作&lt;/p>
&lt;!--
Initially, we focused on the fact that we needed to support the sheer volume of tests mandated by a complex distributed system such as Kubernetes. Real world failure scenarios had to be exercised via end-to-end (e2e) tests to ensure proper functionality. Unfortunately, e2e tests were susceptible to flakes (random failures) and took anywhere from an hour to a day to complete.
-->
&lt;p>最初，我们关注的事实是，我们需要支持复杂的分布式系统（如 Kubernetes）所要求的大量测试。 真实世界中的故障场景必须通过端到端（e2e）测试来执行，确保正确的功能。 不幸的是，e2e 测试容易受到薄片（随机故障）的影响，并且需要花费一个小时到一天才能完成。&lt;/p>
&lt;!--
Further experience revealed other areas where machines could do the work for us:
-->
&lt;p>进一步的经验揭示了机器可以为我们工作的其他领域：&lt;/p>
&lt;!--
* PR Workflow
* Did the contributor sign our CLA?
* Did the PR pass tests?
* Is the PR mergeable?
* Did the merge commit pass tests?
* Triage
* Who should be reviewing PRs?
* Is there enough information to route an issue to the right people?
* Is an issue still relevant?
* Project Health
* What is happening in the project?
* What should we be paying attention to?
-->
&lt;ul>
&lt;li>Pull Request 工作流程
&lt;ul>
&lt;li>贡献者是否签署了我们的 CLA？&lt;/li>
&lt;li>Pull Request 通过测试吗？&lt;/li>
&lt;li>Pull Request 可以合并吗？&lt;/li>
&lt;li>合并提交是否通过了测试？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>鉴别分类
&lt;ul>
&lt;li>谁应该审查 Pull Request？&lt;/li>
&lt;li>是否有足够的信息将问题发送给合适的人？&lt;/li>
&lt;li>问题是否依旧存在？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>项目健康
&lt;ul>
&lt;li>项目中发生了什么？&lt;/li>
&lt;li>我们应该注意什么？&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
As we developed automation to improve our situation, we followed a few guiding principles:
-->
&lt;p>当我们开发自动化来改善我们的情况时，我们遵循了以下几个指导原则：&lt;/p>
&lt;!--
* Follow the push/pull control loop patterns that worked well for Kubernetes
* Prefer stateless loosely coupled services that do one thing well
* Prefer empowering the entire community over empowering a few core contributors
* Eat our own dogfood and avoid reinventing wheels
-->
&lt;ul>
&lt;li>遵循适用于 Kubernetes 的推送/拉取控制循环模式&lt;/li>
&lt;li>首选无状态松散耦合服务&lt;/li>
&lt;li>更倾向于授权整个社区权利，而不是赋予少数核心贡献者权力&lt;/li>
&lt;li>做好自己的事，而不要重新造轮子&lt;/li>
&lt;/ul>
&lt;!--
## Enter Prow
-->
&lt;h2 id="了解-prow">了解 Prow&lt;/h2>
&lt;!--
This led us to create [Prow](https://git.k8s.io/test-infra/prow) as the central component for our automation. Prow is sort of like an [If This, Then That](https://ifttt.com/) for GitHub events, with a built-in library of [commands](https://prow.k8s.io/command-help), [plugins](https://prow.k8s.io/plugins), and utilities. We built Prow on top of Kubernetes to free ourselves from worrying about resource management and scheduling, and ensure a more pleasant operational experience.
-->
&lt;p>这促使我们创建 &lt;a href="https://git.k8s.io/test-infra/prow">Prow&lt;/a> 作为我们自动化的核心组件。 Prow有点像 &lt;a href="https://ifttt.com/">If This, Then That&lt;/a> 用于 GitHub 事件， 内置 &lt;a href="https://prow.k8s.io/command-help">commands&lt;/a>， &lt;a href="https://prow.k8s.io/plugins">plugins&lt;/a>， 和实用程序。 我们在 Kubernetes 之上建立了 Prow，让我们不必担心资源管理和日程安排，并确保更愉快的运营体验。&lt;/p>
&lt;!--
Prow lets us do things like:
-->
&lt;p>Prow 让我们做以下事情：&lt;/p>
&lt;!--
* Allow our community to triage issues/PRs by commenting commands such as “/priority critical-urgent”, “/assign mary” or “/close”
* Auto-label PRs based on how much code they change, or which files they touch
* Age out issues/PRs that have remained inactive for too long
* Auto-merge PRs that meet our PR workflow requirements
* Run CI jobs defined as [Knative Builds](https://github.com/knative/build), Kubernetes Pods, or Jenkins jobs
* Enforce org-wide and per-repo GitHub policies like [branch protection](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector) and [GitHub labels](https://github.com/kubernetes/test-infra/tree/master/label_sync)
-->
&lt;ul>
&lt;li>允许我们的社区通过评论诸如“/priority critical-urgent”，“/assign mary”或“/close”之类的命令对 issues/Pull Requests 进行分类&lt;/li>
&lt;li>根据用户更改的代码数量或创建的文件自动标记 Pull Requests&lt;/li>
&lt;li>标出长时间保持不活动状态 issues/Pull Requests&lt;/li>
&lt;li>自动合并符合我们PR工作流程要求的 Pull Requests&lt;/li>
&lt;li>运行定义为&lt;a href="https://github.com/knative/build">Knative Builds&lt;/a>的 Kubernetes Pods或 Jenkins jobs的 CI 作业&lt;/li>
&lt;li>实施组织范围和重构 GitHub 仓库策略，如&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector">Knative Builds&lt;/a>和&lt;a href="https://github.com/kubernetes/test-infra/tree/master/label_sync">GitHub labels&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Prow was initially developed by the engineering productivity team building Google Kubernetes Engine, and is actively contributed to by multiple members of Kubernetes SIG Testing. Prow has been adopted by several other open source projects, including Istio, JetStack, Knative and OpenShift. [Getting started with Prow](https://github.com/kubernetes/test-infra/tree/master/prow#getting-started) takes a Kubernetes cluster and `kubectl apply starter.yaml` (running pods on a Kubernetes cluster).
-->
&lt;p>Prow最初由构建 Google Kubernetes Engine 的工程效率团队开发，并由 Kubernetes SIG Testing 的多个成员积极贡献。 Prow 已被其他几个开源项目采用，包括 Istio，JetStack，Knative 和 OpenShift。 &lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow#getting-started">Getting started with Prow&lt;/a>需要一个 Kubernetes 集群和 &lt;code>kubectl apply starter.yaml&lt;/code>（在 Kubernetes 集群上运行 pod）。&lt;/p>
&lt;!--
Once we had Prow in place, we began to hit other scaling bottlenecks, and so produced additional tooling to support testing at the scale required by Kubernetes, including:
-->
&lt;p>一旦我们安装了 Prow，我们就开始遇到其他的问题，因此需要额外的工具以支持 Kubernetes 所需的规模测试，包括：&lt;/p>
&lt;!--
- [Boskos](https://github.com/kubernetes/test-infra/tree/master/boskos): manages job resources (such as GCP projects) in pools, checking them out for jobs and cleaning them up automatically ([with monitoring](http://velodrome.k8s.io/dashboard/db/boskos-dashboard?orgId=1))
- [ghProxy](https://github.com/kubernetes/test-infra/tree/master/ghproxy): a reverse proxy HTTP cache optimized for use with the GitHub API, to ensure our token usage doesn’t hit API limits ([with monitoring](http://velodrome.k8s.io/dashboard/db/github-cache?refresh=1m&amp;orgId=1))
- [Greenhouse](https://github.com/kubernetes/test-infra/tree/master/greenhouse): allows us to use a remote bazel cache to provide faster build and test results for PRs ([with monitoring](http://velodrome.k8s.io/dashboard/db/bazel-cache?orgId=1))
- [Splice](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/splice): allows us to test and merge PRs in a batch, ensuring our merge velocity is not limited to our test velocity
- [Tide](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide): allows us to merge PRs selected via GitHub queries rather than ordered in a queue, allowing for significantly higher merge velocity in tandem with splice
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/boskos">Boskos&lt;/a>: 管理池中的作业资源（例如 GCP 项目），检查它们是否有工作并自动清理它们 (&lt;a href="http://velodrome.k8s.io/dashboard/db/boskos-dashboard?orgId=1">with monitoring&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/ghproxy">ghProxy&lt;/a>: 优化用于 GitHub API 的反向代理 HTTP 缓存，以确保我们的令牌使用不会达到 API 限制 (&lt;a href="http://velodrome.k8s.io/dashboard/db/github-cache?refresh=1m&amp;amp;orgId=1">with monitoring&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/greenhouse">Greenhouse&lt;/a>: 允许我们使用远程 bazel 缓存为 Pull requests 提供更快的构建和测试结果 (&lt;a href="http://velodrome.k8s.io/dashboard/db/bazel-cache?orgId=1">with monitoring&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow/cmd/splice">Splice&lt;/a>: 允许我们批量测试和合并 Pull requests，确保我们的合并速度不仅限于我们的测试速度&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide">Tide&lt;/a>: 允许我们合并通过 GitHub 查询选择的 Pull requests，而不是在队列中排序，允许显着更高合并速度与拼接一起&lt;/li>
&lt;/ul>
&lt;!--
## Scaling Project Health
-->
&lt;p>##　关注项目健康状况&lt;/p>
&lt;!--
With workflow automation addressed, we turned our attention to project health. We chose to use Google Cloud Storage (GCS) as our source of truth for all test data, allowing us to lean on established infrastructure, and allowed the community to contribute results. We then built a variety of tools to help individuals and the project as a whole make sense of this data, including:
-->
&lt;p>随着工作流自动化的实施，我们将注意力转向了项目健康。我们选择使用 Google Cloud Storage (GCS)作为所有测试数据的真实来源，允许我们依赖已建立的基础设施，并允许社区贡献结果。然后，我们构建了各种工具来帮助个人和整个项目理解这些数据，包括：&lt;/p>
&lt;!--
* [Gubernator](https://github.com/kubernetes/test-infra/tree/master/gubernator): display the results and test history for a given PR
* [Kettle](https://github.com/kubernetes/test-infra/tree/master/kettle): transfer data from GCS to a publicly accessible bigquery dataset
* [PR dashboard](https://k8s-gubernator.appspot.com/pr): a workflow-aware dashboard that allows contributors to understand which PRs require attention and why
* [Triage](https://storage.googleapis.com/k8s-gubernator/triage/index.html): identify common failures that happen across all jobs and tests
* [Testgrid](https://k8s-testgrid.appspot.com/): display test results for a given job across all runs, summarize test results across groups of jobs
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/gubernator">Gubernator&lt;/a>: 显示给定 Pull Request 的结果和测试历史&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/kettle">Kettle&lt;/a>: 将数据从 GCS 传输到可公开访问的 bigquery 数据集&lt;/li>
&lt;li>&lt;a href="https://k8s-gubernator.appspot.com/pr">PR dashboard&lt;/a>: 一个工作流程识别仪表板，允许参与者了解哪些 Pull Request 需要注意以及为什么&lt;/li>
&lt;li>&lt;a href="https://storage.googleapis.com/k8s-gubernator/triage/index.html">Triage&lt;/a>: 识别所有作业和测试中发生的常见故障&lt;/li>
&lt;li>&lt;a href="https://k8s-testgrid.appspot.com/">Testgrid&lt;/a>: 显示所有运行中给定作业的测试结果，汇总各组作业的测试结果&lt;/li>
&lt;/ul>
&lt;!--
We approached the Cloud Native Computing Foundation (CNCF) to develop DevStats to glean insights from our GitHub events such as:
-->
&lt;p>我们与云计算本地计算基金会（CNCF）联系，开发 DevStats，以便从我们的 GitHub 活动中收集见解，例如：&lt;/p>
&lt;!--
* [Which prow commands are people most actively using](https://k8s.devstats.cncf.io/d/5/bot-commands-repository-groups?orgId=1)
* [PR reviews by contributor over time](https://k8s.devstats.cncf.io/d/46/pr-reviews-by-contributor?orgId=1&amp;var-period=d7&amp;var-repo_name=All&amp;var-reviewers=All)
* [Time spent in each phase of our PR workflow](https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1)
-->
&lt;ul>
&lt;li>&lt;a href="https://k8s.devstats.cncf.io/d/5/bot-commands-repository-groups?orgId=1">Which prow commands are people most actively using&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://k8s.devstats.cncf.io/d/46/pr-reviews-by-contributor?orgId=1&amp;amp;var-period=d7&amp;amp;var-repo_name=All&amp;amp;var-reviewers=All">PR reviews by contributor over time&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1">Time spent in each phase of our PR workflow&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Into the Beyond
-->
&lt;h2 id="into-the-beyond">Into the Beyond&lt;/h2>
&lt;!--
Today, the Kubernetes project spans over 125 repos across five orgs. There are 31 Special Interests Groups and 10 Working Groups coordinating development within the project. In the last year the project has had [participation from over 13,800 unique developers](https://k8s.devstats.cncf.io/d/13/developer-activity-counts-by-repository-group?orgId=1&amp;var-period_name=Last%20year&amp;var-metric=contributions&amp;var-repogroup_name=All) on GitHub.
-->
&lt;p>今天，Kubernetes 项目跨越了5个组织125个仓库。有31个特殊利益集团和10个工作组在项目内协调发展。在过去的一年里，该项目有 &lt;a href="https://k8s.devstats.cncf.io/d/13/developer-activity-counts-by-repository-group?orgId=1&amp;amp;var-period_name=Last%20year&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All">来自13800多名独立开发人员的参与&lt;/a>。&lt;/p>
&lt;!--
On any given weekday our Prow instance [runs over 10,000 CI jobs](http://velodrome.k8s.io/dashboard/db/bigquery-metrics?panelId=10&amp;fullscreen&amp;orgId=1&amp;from=now-6M&amp;to=now); from March 2017 to March 2018 it ran 4.3 million jobs. Most of these jobs involve standing up an entire Kubernetes cluster, and exercising it using real world scenarios. They allow us to ensure all supported releases of Kubernetes work across cloud providers, container engines, and networking plugins. They make sure the latest releases of Kubernetes work with various optional features enabled, upgrade safely, meet performance requirements, and work across architectures.
-->
&lt;p>在任何给定的工作日，我们的 Prow 实例&lt;a href="http://velodrome.k8s.io/dashboard/db/bigquery-metrics?panelId=10&amp;amp;fullscreen&amp;amp;orgId=1&amp;amp;from=now-6M&amp;amp;to=now">运行超过10,000个 CI 工作&lt;/a>; 从2017年3月到2018年3月，它有430万个工作岗位。 这些工作中的大多数涉及建立整个 Kubernetes 集群，并使用真实场景来实施它。 它们使我们能够确保所有受支持的 Kubernetes 版本跨云提供商，容器引擎和网络插件工作。 他们确保最新版本的 Kubernetes 能够启用各种可选功能，安全升级，满足性能要求，并跨架构工作。&lt;/p>
&lt;!--
With today’s [announcement from CNCF](https://www.cncf.io/announcement/2018/08/29/cncf-receives-9-million-cloud-credit-grant-from-google) – noting that Google Cloud has begun transferring ownership and management of the Kubernetes project’s cloud resources to CNCF community contributors, we are excited to embark on another journey. One that allows the project infrastructure to be owned and operated by the community of contributors, following the same open governance model that has worked for the rest of the project. Sound exciting to you? Come talk to us at #sig-testing on kubernetes.slack.com.
-->
&lt;p>今天&lt;a href="https://www.cncf.io/announcement/2018/08/29/cncf-receives-9-million-cloud-credit-grant-from-google">来自CNCF的公告&lt;/a> - 注意到 Google Cloud 有开始将 Kubernetes 项目的云资源的所有权和管理权转让给 CNCF 社区贡献者，我们很高兴能够开始另一个旅程。 允许项目基础设施由贡献者社区拥有和运营，遵循对项目其余部分有效的相同开放治理模型。 听起来令人兴奋。 请来 kubernetes.slack.com 上的 #sig-testing on kubernetes.slack.com 与我们联系。&lt;/p>
&lt;!--
Want to find out more? Come check out these resources:
-->
&lt;p>想了解更多？ 快来看看这些资源：&lt;/p>
&lt;!--
* [Prow: Testing the way to Kubernetes Next](https://elder.dev/posts/prow)
* [Automation and the Kubernetes Contributor Experience](https://www.youtube.com/watch?v=BsIC7gPkH5M)
-->
&lt;ul>
&lt;li>&lt;a href="https://elder.dev/posts/prow">Prow: Testing the way to Kubernetes Next&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=BsIC7gPkH5M">Automation and the Kubernetes Contributor Experience&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: 使用 CSI 和 Kubernetes 实现卷的动态扩容</title><link>https://kubernetes.io/zh/blog/2018/08/02/%E4%BD%BF%E7%94%A8-csi-%E5%92%8C-kubernetes-%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%9A%84%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9/</link><pubDate>Thu, 02 Aug 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/08/02/%E4%BD%BF%E7%94%A8-csi-%E5%92%8C-kubernetes-%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%9A%84%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9/</guid><description>
&lt;!--
---
layout: blog
title: 'Dynamically Expand Volume with CSI and Kubernetes'
date: 2018-08-02
---
-->
&lt;!--
**Author**: Orain Xiong (Co-Founder, WoquTech)
-->
&lt;p>&lt;strong>作者&lt;/strong>：Orain Xiong（联合创始人, WoquTech）&lt;/p>
&lt;!--
_There is a very powerful storage subsystem within Kubernetes itself, covering a fairly broad spectrum of use cases. Whereas, when planning to build a product-grade relational database platform with Kubernetes, we face a big challenge: coming up with storage. This article describes how to extend latest Container Storage Interface 0.2.0 and integrate with Kubernetes, and demonstrates the essential facet of dynamically expanding volume capacity._
-->
&lt;p>&lt;em>Kubernetes 本身有一个非常强大的存储子系统，涵盖了相当广泛的用例。而当我们计划使用 Kubernetes 构建产品级关系型数据库平台时，我们面临一个巨大的挑战：提供存储。本文介绍了如何扩展最新的 Container Storage Interface 0.2.0 和与 Kubernetes 集成，并演示了卷动态扩容的基本方面。&lt;/em>&lt;/p>
&lt;!--
## Introduction
-->
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;!--
As we focalize our customers, especially in financial space, there is a huge upswell in the adoption of container orchestration technology.
-->
&lt;p>当我们专注于客户时，尤其是在金融领域，采用容器编排技术的情况大大增加。&lt;/p>
&lt;!--
They are looking forward to open source solutions to redesign already existing monolithic applications, which have been running for several years on virtualization infrastructure or bare metal.
-->
&lt;p>他们期待着能用开源解决方案重新设计已经存在的整体应用程序，这些应用程序已经在虚拟化基础架构或裸机上运行了几年。&lt;/p>
&lt;!--
Considering extensibility and the extent of technical maturity, Kubernetes and Docker are at the very top of the list. But migrating monolithic applications to a distributed orchestration like Kubernetes is challenging, the relational database is critical for the migration.
-->
&lt;p>考虑到可扩展性和技术成熟程度，Kubernetes 和 Docker 排在我们选择列表的首位。但是将整体应用程序迁移到类似于 Kubernetes 之类的分布式容器编排平台上很具有挑战性，其中关系数据库对于迁移来说至关重要。&lt;/p>
&lt;!--
With respect to the relational database, we should pay attention to storage. There is a very powerful storage subsystem within Kubernetes itself. It is very useful and covers a fairly broad spectrum of use cases. When planning to run a relational database with Kubernetes in production, we face a big challenge: coming up with storage. There are still some fundamental functionalities which are left unimplemented. Specifically, dynamically expanding volume. It sounds boring but is highly required, except for actions like create and delete and mount and unmount.
-->
&lt;p>关于关系数据库，我们应该注意存储。Kubernetes 本身内部有一个非常强大的存储子系统。它非常有用，涵盖了相当广泛的用例。当我们计划在生产环境中使用 Kubernetes 运行关系型数据库时，我们面临一个巨大挑战：提供存储。目前，仍有一些基本功能尚未实现。特别是，卷的动态扩容。这听起来很无聊，但在除创建，删除，安装和卸载之类的操作外，它是非常必要的。&lt;/p>
&lt;!--
Currently, expanding volume is only available with those storage provisioners:
-->
&lt;p>目前，扩展卷仅适用于这些存储供应商：&lt;/p>
&lt;ul>
&lt;li>gcePersistentDisk&lt;/li>
&lt;li>awsElasticBlockStore&lt;/li>
&lt;li>OpenStack Cinder&lt;/li>
&lt;li>glusterfs&lt;/li>
&lt;li>rbd&lt;/li>
&lt;/ul>
&lt;!--
In order to enable this feature, we should set feature gate `ExpandPersistentVolumes` true and turn on the `PersistentVolumeClaimResize` admission plugin. Once `PersistentVolumeClaimResize` has been enabled, resizing will be allowed by a Storage Class whose `allowVolumeExpansion` field is set to true.
-->
&lt;p>为了启用此功能，我们应该将特性开关 &lt;code>ExpandPersistentVolumes&lt;/code> 设置为 true 并打开 &lt;code>PersistentVolumeClaimResize&lt;/code> 准入插件。 一旦启用了 &lt;code>PersistentVolumeClaimResize&lt;/code>，则其对应的 &lt;code>allowVolumeExpansion&lt;/code> 字段设置为 true 的存储类将允许调整大小。&lt;/p>
&lt;!--
Unfortunately, dynamically expanding volume through the Container Storage Interface (CSI) and Kubernetes is unavailable, even though the underlying storage providers have this feature.
-->
&lt;p>不幸的是，即使基础存储提供者具有此功能，也无法通过容器存储接口（CSI）和 Kubernetes 动态扩展卷。&lt;/p>
&lt;!--
This article will give a simplified view of CSI, followed by a walkthrough of how to introduce a new expanding volume feature on the existing CSI and Kubernetes. Finally, the article will demonstrate how to dynamically expand volume capacity.
-->
&lt;p>本文将给出 CSI 的简化视图，然后逐步介绍如何在现有 CSI 和 Kubernetes 上引入新的扩展卷功能。最后，本文将演示如何动态扩展卷容量。&lt;/p>
&lt;!--
## Container Storage Interface (CSI)
-->
&lt;h2 id="容器存储接口-csi">容器存储接口（CSI）&lt;/h2>
&lt;!--
To have a better understanding of what we're going to do, the first thing we need to know is what the Container Storage Interface is. Currently, there are still some problems for already existing storage subsystem within Kubernetes. Storage driver code is maintained in the Kubernetes core repository which is difficult to test. But beyond that, Kubernetes needs to give permissions to storage vendors to check code into the Kubernetes core repository. Ideally, that should be implemented externally.
-->
&lt;p>为了更好地了解我们将要做什么，我们首先需要知道什么是容器存储接口。当前，Kubernetes 中已经存在的存储子系统仍然存在一些问题。 存储驱动程序代码在 Kubernetes 核心存储库中维护，这很难测试。 但是除此之外，Kubernetes 还需要授予存储供应商许可，以将代码签入 Kubernetes 核心存储库。 理想情况下，这些应在外部实施。&lt;/p>
&lt;!--
CSI is designed to define an industry standard that will enable storage providers who enable CSI to be available across container orchestration systems that support CSI.
-->
&lt;p>CSI 旨在定义行业标准，该标准将使支持 CSI 的存储提供商能够在支持 CSI 的容器编排系统中使用。&lt;/p>
&lt;!--
This diagram depicts a kind of high-level Kubernetes archetypes integrated with CSI:
![csi diagram](/images/blog/2018-08-02-dynamically-expand-volume-csi/csi-diagram.png)
-->
&lt;p>该图描述了一种与 CSI 集成的高级 Kubernetes 原型：&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-08-02-dynamically-expand-volume-csi/csi-diagram.png" alt="csi diagram">&lt;/p>
&lt;!--
* Three new external components are introduced to decouple Kubernetes and Storage Provider logic
* Blue arrows present the conventional way to call against API Server
* Red arrows present gRPC to call against Volume Driver
-->
&lt;ul>
&lt;li>引入了三个新的外部组件以解耦 Kubernetes 和存储提供程序逻辑&lt;/li>
&lt;li>蓝色箭头表示针对 API 服务器进行调用的常规方法&lt;/li>
&lt;li>红色箭头显示 gRPC 以针对 Volume Driver 进行调用&lt;/li>
&lt;/ul>
&lt;!--
For more details, please visit: https://github.com/container-storage-interface/spec/blob/master/spec.md
-->
&lt;p>更多详细信息，请访问：https://github.com/container-storage-interface/spec/blob/master/spec.md&lt;/p>
&lt;!--
## Extend CSI and Kubernetes
-->
&lt;h2 id="扩展-csi-和-kubernetes">扩展 CSI 和 Kubernetes&lt;/h2>
&lt;!--
In order to enable the feature of expanding volume atop Kubernetes, we should extend several components including CSI specification, “in-tree” volume plugin, external-provisioner and external-attacher.
-->
&lt;p>为了实现在 Kubernetes 上扩展卷的功能，我们应该扩展几个组件，包括 CSI 规范，“in-tree” 卷插件，external-provisioner 和 external-attacher。&lt;/p>
&lt;!--
## Extend CSI spec
-->
&lt;h2 id="扩展csi规范">扩展CSI规范&lt;/h2>
&lt;!--
The feature of expanding volume is still undefined in latest CSI 0.2.0. The new 3 RPCs, including `RequiresFSResize` and `ControllerResizeVolume` and `NodeResizeVolume`, should be introduced.
-->
&lt;p>最新的 CSI 0.2.0 仍未定义扩展卷的功能。应该引入新的3个 RPC，包括 &lt;code>RequiresFSResize&lt;/code>， &lt;code>ControllerResizeVolume&lt;/code> 和 &lt;code>NodeResizeVolume&lt;/code>。&lt;/p>
&lt;pre>&lt;code>service Controller {
rpc CreateVolume (CreateVolumeRequest)
returns (CreateVolumeResponse) {}
……
rpc RequiresFSResize (RequiresFSResizeRequest)
returns (RequiresFSResizeResponse) {}
rpc ControllerResizeVolume (ControllerResizeVolumeRequest)
returns (ControllerResizeVolumeResponse) {}
}
service Node {
rpc NodeStageVolume (NodeStageVolumeRequest)
returns (NodeStageVolumeResponse) {}
……
rpc NodeResizeVolume (NodeResizeVolumeRequest)
returns (NodeResizeVolumeResponse) {}
}
&lt;/code>&lt;/pre>&lt;!--
## Extend “In-Tree” Volume Plugin
-->
&lt;h2 id="扩展-in-tree-卷插件">扩展 “In-Tree” 卷插件&lt;/h2>
&lt;!--
In addition to the extend CSI specification, the `csiPlugin﻿` interface within Kubernetes should also implement `expandablePlugin`. The `csiPlugin` interface will expand `PersistentVolumeClaim` representing for `ExpanderController`.
-->
&lt;p>除了扩展的 CSI 规范之外，Kubernetes 中的 &lt;code>csiPlugin&lt;/code> 接口还应该实现 &lt;code>expandablePlugin&lt;/code>。&lt;code>csiPlugin&lt;/code> 接口将扩展代表 &lt;code>ExpanderController&lt;/code> 的 &lt;code>PersistentVolumeClaim&lt;/code>。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> ExpandableVolumePlugin &lt;span style="color:#a2f;font-weight:bold">interface&lt;/span> {
VolumePlugin
&lt;span style="color:#00a000">ExpandVolumeDevice&lt;/span>(spec Spec, newSize resource.Quantity, oldSize resource.Quantity) (resource.Quantity, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>)
&lt;span style="color:#00a000">RequiresFSResize&lt;/span>() &lt;span style="color:#0b0;font-weight:bold">bool&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
### Implement Volume Driver
-->
&lt;h3 id="实现卷驱动程序">实现卷驱动程序&lt;/h3>
&lt;!--
Finally, to abstract complexity of the implementation, we should hard code the separate storage provider management logic into the following functions which is well-defined in the CSI specification:
-->
&lt;p>最后，为了抽象化实现的复杂性，我们应该将单独的存储提供程序管理逻辑硬编码为以下功能，这些功能在 CSI 规范中已明确定义：&lt;/p>
&lt;ul>
&lt;li>CreateVolume&lt;/li>
&lt;li>DeleteVolume&lt;/li>
&lt;li>ControllerPublishVolume&lt;/li>
&lt;li>ControllerUnpublishVolume&lt;/li>
&lt;li>ValidateVolumeCapabilities&lt;/li>
&lt;li>ListVolumes&lt;/li>
&lt;li>GetCapacity&lt;/li>
&lt;li>ControllerGetCapabilities&lt;/li>
&lt;li>RequiresFSResize&lt;/li>
&lt;li>ControllerResizeVolume&lt;/li>
&lt;/ul>
&lt;!--
## Demonstration
Let’s demonstrate this feature with a concrete user case.
* Create storage class for CSI storage provisioner
-->
&lt;h2 id="展示">展示&lt;/h2>
&lt;p>让我们以具体的用户案例来演示此功能。&lt;/p>
&lt;ul>
&lt;li>为 CSI 存储供应商创建存储类&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">allowVolumeExpansion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>StorageClass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-qcfs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csiProvisionerSecretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>orain-test&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csiProvisionerSecretNamespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">provisioner&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-qcfsplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">reclaimPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeBindingMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Immediate&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
* Deploy CSI Volume Driver including storage provisioner `csi-qcfsplugin` across Kubernetes cluster
* Create PVC `qcfs-pvc` which will be dynamically provisioned by storage class `csi-qcfs`
-->
&lt;ul>
&lt;li>
&lt;p>在 Kubernetes 集群上部署包括存储供应商 &lt;code>csi-qcfsplugin&lt;/code> 在内的 CSI 卷驱动&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 PVC &lt;code>qcfs-pvc&lt;/code>，它将由存储类 &lt;code>csi-qcfs&lt;/code> 动态配置&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>qcfs-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#00f;font-weight:bold">...&lt;/span>.&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>300Gi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-qcfs&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
* Create MySQL 5.7 instance to use PVC `qcfs-pvc`
* In order to mirror the exact same production-level scenario, there are actually two different types of workloads including:
* Batch insert to make MySQL consuming more file system capacity
* Surge query request
* Dynamically expand volume capacity through edit pvc `qcfs-pvc` configuration
-->
&lt;ul>
&lt;li>创建 MySQL 5.7 实例以使用 PVC &lt;code>qcfs-pvc&lt;/code>&lt;/li>
&lt;li>为了反映完全相同的生产级别方案，实际上有两种不同类型的工作负载，包括：
     * 批量插入使 MySQL 消耗更多的文件系统容量
     * 浪涌查询请求&lt;/li>
&lt;li>通过编辑 pvc &lt;code>qcfs-pvc&lt;/code> 配置动态扩展卷容量&lt;/li>
&lt;/ul>
&lt;!--
The Prometheus and Grafana integration allows us to visualize corresponding critical metrics.
![prometheus grafana](/images/blog/2018-08-02-dynamically-expand-volume-csi/prometheus-grafana.png)
We notice that the middle reading shows MySQL datafile size increasing slowly during bulk inserting. At the same time, the bottom reading shows file system expanding twice in about 20 minutes, from 300 GiB to 400 GiB and then 500 GiB. Meanwhile, the upper reading shows the whole process of expanding volume immediately completes and hardly impacts MySQL QPS.
-->
&lt;p>Prometheus 和 Grafana 的集成使我们可以可视化相应的关键指标。&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-08-02-dynamically-expand-volume-csi/prometheus-grafana.png" alt="prometheus grafana">&lt;/p>
&lt;p>我们注意到中间的读数显示在批量插入期间 MySQL 数据文件的大小缓慢增加。 同时，底部读数显示文件系统在大约20分钟内扩展了两次，从 300 GiB 扩展到 400 GiB，然后扩展到 500 GiB。 同时，上半部分显示，扩展卷的整个过程立即完成，几乎不会影响 MySQL QPS。&lt;/p>
&lt;!--
## Conclusion
Regardless of whatever infrastructure applications have been running on, the database is always a critical resource. It is essential to have a more advanced storage subsystem out there to fully support database requirements. This will help drive the more broad adoption of cloud native technology.
-->
&lt;h2 id="结论">结论&lt;/h2>
&lt;p>不管运行什么基础结构应用程序，数据库始终是关键资源。拥有更高级的存储子系统以完全支持数据库需求至关重要。这将有助于推动云原生技术的更广泛采用。&lt;/p></description></item><item><title>Blog: 动态 Kubelet 配置</title><link>https://kubernetes.io/zh/blog/2018/07/11/%E5%8A%A8%E6%80%81-kubelet-%E9%85%8D%E7%BD%AE/</link><pubDate>Wed, 11 Jul 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/07/11/%E5%8A%A8%E6%80%81-kubelet-%E9%85%8D%E7%BD%AE/</guid><description>
&lt;!--
---
layout: blog
title: 'Dynamic Kubelet Configuration'
date: 2018-07-11
---
-->
&lt;!--
**Author**: Michael Taufen (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Michael Taufen (Google)&lt;/p>
&lt;!--
**Editor’s note: this post is part of a [series of in-depth articles](https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/) on what’s new in Kubernetes 1.11**
-->
&lt;p>&lt;strong>编者注：这篇文章是&lt;a href="https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/">一系列深度文章&lt;/a> 的一部分，这个系列介绍了 Kubernetes 1.11 中的新增功能&lt;/strong>&lt;/p>
&lt;!--
## Why Dynamic Kubelet Configuration?
-->
&lt;h2 id="为什么要进行动态-kubelet-配置">为什么要进行动态 Kubelet 配置？&lt;/h2>
&lt;!--
Kubernetes provides API-centric tooling that significantly improves workflows for managing applications and infrastructure. Most Kubernetes installations, however, run the Kubelet as a native process on each host, outside the scope of standard Kubernetes APIs.
-->
&lt;p>Kubernetes 提供了以 API 为中心的工具，可显着改善用于管理应用程序和基础架构的工作流程。
但是，在大多数的 Kubernetes 安装中，kubelet 在每个主机上作为本机进程运行，因此
未被标准 Kubernetes API 覆盖。&lt;/p>
&lt;!--
In the past, this meant that cluster administrators and service providers could not rely on Kubernetes APIs to reconfigure Kubelets in a live cluster. In practice, this required operators to either ssh into machines to perform manual reconfigurations, use third-party configuration management automation tools, or create new VMs with the desired configuration already installed, then migrate work to the new machines. These approaches are environment-specific and can be expensive.
-->
&lt;p>过去，这意味着集群管理员和服务提供商无法依靠 Kubernetes API 在活动集群中重新配置 Kubelets。
实际上，这要求操作员要 SSH 登录到计算机以执行手动重新配置，要么使用第三方配置管理自动化工具，
或创建已经安装了所需配置的新 VM，然后将工作迁移到新计算机上。
这些方法是特定于环境的，并且可能很耗时费力。&lt;/p>
&lt;!--
Dynamic Kubelet configuration gives cluster administrators and service providers the ability to reconfigure Kubelets in a live cluster via Kubernetes APIs.
-->
&lt;p>动态 Kubelet 配置使集群管理员和服务提供商能够通过 Kubernetes API 在活动集群中重新配置 Kubelet。&lt;/p>
&lt;!--
## What is Dynamic Kubelet Configuration?
-->
&lt;h2 id="什么是动态-kubelet-配置">什么是动态 Kubelet 配置？&lt;/h2>
&lt;!--
Kubernetes v1.10 made it possible to configure the Kubelet via a beta [config file](/docs/tasks/administer-cluster/kubelet-config-file/) API. Kubernetes already provides the ConfigMap abstraction for storing arbitrary file data in the API server.
-->
&lt;p>Kubernetes v1.10 使得可以通过 Beta 版本的&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/kubelet-config-file/">配置文件&lt;/a>
API 配置 kubelet。
Kubernetes 已经提供了用于在 API 服务器中存储任意文件数据的 ConfigMap 抽象。&lt;/p>
&lt;!--
Dynamic Kubelet configuration extends the Node object so that a Node can refer to a ConfigMap that contains the same type of config file. When a Node is updated to refer to a new ConfigMap, the associated Kubelet will attempt to use the new configuration.
-->
&lt;p>动态 Kubelet 配置扩展了 Node 对象，以便 Node 可以引用包含相同类型配置文件的 ConfigMap。
当节点更新为引用新的 ConfigMap 时，关联的 Kubelet 将尝试使用新的配置。&lt;/p>
&lt;!--
## How does it work?
-->
&lt;h2 id="它是如何工作的">它是如何工作的？&lt;/h2>
&lt;!--
Dynamic Kubelet configuration provides the following core features:
-->
&lt;p>动态 Kubelet 配置提供以下核心功能：&lt;/p>
&lt;!--
* Kubelet attempts to use the dynamically assigned configuration.
* Kubelet "checkpoints" configuration to local disk, enabling restarts without API server access.
* Kubelet reports assigned, active, and last-known-good configuration sources in the Node status.
* When invalid configuration is dynamically assigned, Kubelet automatically falls back to a last-known-good configuration and reports errors in the Node status.
-->
&lt;ul>
&lt;li>Kubelet 尝试使用动态分配的配置。&lt;/li>
&lt;li>Kubelet 将其配置已检查点的形式保存到本地磁盘，无需 API 服务器访问即可重新启动。&lt;/li>
&lt;li>Kubelet 在 Node 状态中报告已指定的、活跃的和最近已知良好的配置源。&lt;/li>
&lt;li>当动态分配了无效的配置时，Kubelet 会自动退回到最后一次正确的配置，并在 Node 状态中报告错误。&lt;/li>
&lt;/ul>
&lt;!--
To use the dynamic Kubelet configuration feature, a cluster administrator or service provider will first post a ConfigMap containing the desired configuration, then set each Node.Spec.ConfigSource.ConfigMap reference to refer to the new ConfigMap. Operators can update these references at their preferred rate, giving them the ability to perform controlled rollouts of new configurations.
-->
&lt;p>要使用动态 Kubelet 配置功能，群集管理员或服务提供商将首先发布包含所需配置的 ConfigMap，
然后设置每个 Node.Spec.ConfigSource.ConfigMap 引用以指向新的 ConfigMap。
运营商可以以他们喜欢的速率更新这些参考，从而使他们能够执行新配置的受控部署。&lt;/p>
&lt;!--
Each Kubelet watches its associated Node object for changes. When the Node.Spec.ConfigSource.ConfigMap reference is updated, the Kubelet will "checkpoint" the new ConfigMap by writing the files it contains to local disk. The Kubelet will then exit, and the OS-level process manager will restart it. Note that if the Node.Spec.ConfigSource.ConfigMap reference is not set, the Kubelet uses the set of flags and config files local to the machine it is running on.
-->
&lt;p>每个 Kubelet 都会监视其关联的 Node 对象的更改。
更新 Node.Spec.ConfigSource.ConfigMap 引用后，
Kubelet 将通过将其包含的文件通过检查点机制写入本地磁盘保存新的 ConfigMap。
然后，Kubelet 将退出，而操作系统级进程管理器将重新启动它。
请注意，如果未设置 Node.Spec.ConfigSource.ConfigMap 引用，
则 Kubelet 将使用其正在运行的计算机本地的一组标志和配置文件。&lt;/p>
&lt;!--
Once restarted, the Kubelet will attempt to use the configuration from the new checkpoint. If the new configuration passes the Kubelet's internal validation, the Kubelet will update Node.Status.Config to reflect that it is using the new configuration. If the new configuration is invalid, the Kubelet will fall back to its last-known-good configuration and report an error in Node.Status.Config.
-->
&lt;p>重新启动后，Kubelet 将尝试使用来自新检查点的配置。
如果新配置通过了 Kubelet 的内部验证，则 Kubelet 将更新
Node.Status.Config 用以反映它正在使用新配置。
如果新配置无效，则 Kubelet 将退回到其最后一个正确的配置，并在 Node.Status.Config 中报告错误。&lt;/p>
&lt;!--
Note that the default last-known-good configuration is the combination of Kubelet command-line flags with the Kubelet's local configuration file. Command-line flags that overlap with the config file always take precedence over both the local configuration file and dynamic configurations, for backwards-compatibility.
-->
&lt;p>请注意，默认的最后一次正确配置是 Kubelet 命令行标志与 Kubelet 的本地配置文件的组合。
与配置文件重叠的命令行标志始终优先于本地配置文件和动态配置，以实现向后兼容。&lt;/p>
&lt;!--
See the following diagram for a high-level overview of a configuration update for a single Node:
-->
&lt;p>有关单个节点的配置更新的高级概述，请参见下图：&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-07-11-dynamic-kubelet-configuration/kubelet-diagram.png" alt="kubelet-diagram">&lt;/p>
&lt;!--
## How can I learn more?
-->
&lt;h2 id="我如何了解更多">我如何了解更多？&lt;/h2>
&lt;!--
Please see the official tutorial at /docs/tasks/administer-cluster/reconfigure-kubelet/, which contains more in-depth details on user workflow, how a configuration becomes "last-known-good," how the Kubelet "checkpoints" config, and possible failure modes.
-->
&lt;p>请参阅/docs/tasks/administer-cluster/reconfigure-kubelet/上的官方教程，
其中包含有关用户工作流，某配置如何成为“最新的正确的”配置，Kubelet 如何对配置执行“检查点”操作等，
更多详细信息，以及可能的故障模式。&lt;/p></description></item><item><title>Blog: 用于 Kubernetes 集群 DNS 的 CoreDNS GA 正式发布</title><link>https://kubernetes.io/zh/blog/2018/07/10/%E7%94%A8%E4%BA%8E-kubernetes-%E9%9B%86%E7%BE%A4-dns-%E7%9A%84-coredns-ga-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</link><pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/07/10/%E7%94%A8%E4%BA%8E-kubernetes-%E9%9B%86%E7%BE%A4-dns-%E7%9A%84-coredns-ga-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</guid><description>
&lt;!--
---
layout: blog
title: "CoreDNS GA for Kubernetes Cluster DNS"
date: 2018-07-10
---
--->
&lt;!--
**Author**: John Belamaric (Infoblox)
**Editor’s note: this post is part of a [series of in-depth articles](https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/) on what’s new in Kubernetes 1.11**
--->
&lt;p>&lt;strong>作者&lt;/strong>：John Belamaric (Infoblox)&lt;/p>
&lt;p>**编者注：这篇文章是 &lt;a href="https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/">系列深度文章&lt;/a> 中的一篇，介绍了 Kubernetes 1.11 新增的功能&lt;/p>
&lt;!--
## Introduction
In Kubernetes 1.11, [CoreDNS](https://coredns.io) has reached General Availability (GA) for DNS-based service discovery, as an alternative to the kube-dns addon. This means that CoreDNS will be offered as an option in upcoming versions of the various installation tools. In fact, the kubeadm team chose to make it the default option starting with Kubernetes 1.11.
--->
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;p>在 Kubernetes 1.11 中，&lt;a href="https://coredns.io">CoreDNS&lt;/a> 已经达到基于 DNS 服务发现的 General Availability (GA)，可以替代 kube-dns 插件。这意味着 CoreDNS 会作为即将发布的安装工具的选项之一上线。实际上，从 Kubernetes 1.11 开始，kubeadm 团队选择将它设为默认选项。&lt;/p>
&lt;!--
DNS-based service discovery has been part of Kubernetes for a long time with the kube-dns cluster addon. This has generally worked pretty well, but there have been some concerns around the reliability, flexibility and security of the implementation.
CoreDNS is a general-purpose, authoritative DNS server that provides a backwards-compatible, but extensible, integration with Kubernetes. It resolves the issues seen with kube-dns, and offers a number of unique features that solve a wider variety of use cases.
In this article, you will learn about the differences in the implementations of kube-dns and CoreDNS, and some of the helpful extensions offered by CoreDNS.
--->
&lt;p>很久以来， kube-dns 集群插件一直是 Kubernetes 的一部分，用来实现基于 DNS 的服务发现。
通常，此插件运行平稳，但对于实现的可靠性、灵活性和安全性仍存在一些疑虑。&lt;/p>
&lt;p>CoreDNS 是通用的、权威的 DNS 服务器，提供与 Kubernetes 向后兼容但可扩展的集成。它解决了 kube-dns 遇到的问题，并提供了许多独特的功能，可以解决各种用例。&lt;/p>
&lt;p>在本文中，您将了解 kube-dns 和 CoreDNS 的实现有何差异，以及 CoreDNS 提供的一些非常有用的扩展。&lt;/p>
&lt;!--
## Implementation differences
In kube-dns, several containers are used within a single pod: `kubedns`, `dnsmasq`, and `sidecar`. The `kubedns`
container watches the Kubernetes API and serves DNS records based on the [Kubernetes DNS specification](https://github.com/kubernetes/dns/blob/master/docs/specification.md), `dnsmasq` provides caching and stub domain support, and `sidecar` provides metrics and health checks.
--->
&lt;h2 id="实现差异">实现差异&lt;/h2>
&lt;p>在 kube-dns 中，一个 Pod 中使用多个 容器：&lt;code>kubedns&lt;/code>、&lt;code>dnsmasq&lt;/code>、和 &lt;code>sidecar&lt;/code>。&lt;code>kubedns&lt;/code> 容器监视 Kubernetes API 并根据 &lt;a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">Kubernetes DNS 规范&lt;/a> 提供 DNS 记录，&lt;code>dnsmasq&lt;/code> 提供缓存和存根域支持，&lt;code>sidecar&lt;/code> 提供指标和健康检查。&lt;/p>
&lt;!--
This setup leads to a few issues that have been seen over time. For one, security vulnerabilities in `dnsmasq` have led to the need
for a security-patch release of Kubernetes in the past. Additionally, because `dnsmasq` handles the stub domains,
but `kubedns` handles the External Services, you cannot use a stub domain in an external service, which is very
limiting to that functionality (see [dns#131](https://github.com/kubernetes/dns/issues/131)).
All of these functions are done in a single container in CoreDNS, which is running a process written in Go. The
different plugins that are enabled replicate (and enhance) the functionality found in kube-dns.
--->
&lt;p>随着时间的推移，此设置会导致一些问题。一方面，以往 &lt;code>dnsmasq&lt;/code> 中的安全漏洞需要通过发布 Kubernetes 的安全补丁来解决。但是，由于 &lt;code>dnsmasq&lt;/code> 处理存根域，而 &lt;code>kubedns&lt;/code> 处理外部服务，因此您不能在外部服务中使用存根域，导致这个功能具有局限性（请参阅 &lt;a href="https://github.com/kubernetes/dns/issues/131">dns#131&lt;/a>）。&lt;/p>
&lt;p>在 CoreDNS 中，所有这些功能都是在一个容器中完成的，该容器运行用 Go 编写的进程。所启用的不同插件可复制（并增强）在 kube-dns 中存在的功能。&lt;/p>
&lt;!--
## Configuring CoreDNS
In kube-dns, you can [modify a ConfigMap](https://kubernetes.io/blog/2017/04/configuring-private-dns-zones-upstream-nameservers-kubernetes/) to change the behavior of your service discovery. This allows the addition of
features such as serving stub domains, modifying upstream nameservers, and enabling federation.
--->
&lt;h2 id="配置-coredns">配置 CoreDNS&lt;/h2>
&lt;p>在 kube-dns 中，您可以 &lt;a href="https://kubernetes.io/blog/2017/04/configuring-private-dns-zones-upstream-nameservers-kubernetes/">修改 ConfigMap&lt;/a> 来更改服务发现的行为。用户可以添加诸如为存根域提供服务、修改上游名称服务器以及启用联盟之类的功能。&lt;/p>
&lt;!--
In CoreDNS, you similarly can modify the ConfigMap for the CoreDNS [Corefile](https://coredns.io/2017/07/23/corefile-explained/) to change how service discovery
works. This Corefile configuration offers many more options than you will find in kube-dns, since it is the
primary configuration file that CoreDNS uses for configuration of all of its features, even those that are not
Kubernetes related.
When upgrading from kube-dns to CoreDNS using `kubeadm`, your existing ConfigMap will be used to generate the
customized Corefile for you, including all of the configuration for stub domains, federation, and upstream nameservers. See [Using CoreDNS for Service Discovery](/docs/tasks/administer-cluster/coredns/) for more details.
--->
&lt;p>在 CoreDNS 中，您可以类似地修改 CoreDNS &lt;a href="https://coredns.io/2017/07/23/corefile-explained/">Corefile&lt;/a> 的 ConfigMap，以更改服务发现的工作方式。这种 Corefile 配置提供了比 kube-dns 中更多的选项，因为它是 CoreDNS 用于配置所有功能的主要配置文件，即使与 Kubernetes 不相关的功能也可以操作。&lt;/p>
&lt;p>使用 &lt;code>kubeadm&lt;/code> 将 kube-dns 升级到 CoreDNS 时，现有的 ConfigMap 将被用来为您生成自定义的 Corefile，包括存根域、联盟和上游名称服务器的所有配置。更多详细信息，请参见
&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/coredns/">使用 CoreDNS 进行服务发现&lt;/a>。&lt;/p>
&lt;!--
## Bug fixes and enhancements
There are several open issues with kube-dns that are resolved in CoreDNS, either in default configuration or with some customized configurations.
--->
&lt;h2 id="错误修复和增强">错误修复和增强&lt;/h2>
&lt;p>在 CoreDNS 中解决了 kube-dn 的多个未解决问题，无论是默认配置还是某些自定义配置。&lt;/p>
&lt;!--
* [dns#55 - Custom DNS entries for kube-dns](https://github.com/kubernetes/dns/issues/55) may be handled using the "fallthrough" mechanism in the [kubernetes plugin](https://coredns.io/plugins/kubernetes), using the [rewrite plugin](https://coredns.io/plugins/rewrite), or simply serving a subzone with a different plugin such as the [file plugin](https://coredns.io/plugins/file).
* [dns#116 - Only one A record set for headless service with pods having single hostname](https://github.com/kubernetes/dns/issues/116). This issue is fixed without any additional configuration.
* [dns#131 - externalName not using stubDomains settings](https://github.com/kubernetes/dns/issues/131). This issue is fixed without any additional configuration.
* [dns#167 - enable skyDNS round robin A/AAAA records](https://github.com/kubernetes/dns/issues/167). The equivalent functionality can be configured using the [load balance plugin](https://coredns.io/plugins/loadbalance).
* [dns#190 - kube-dns cannot run as non-root user](https://github.com/kubernetes/dns/issues/190). This issue is solved today by using a non-default image, but it will be made the default CoreDNS behavior in a future release.
* [dns#232 - fix pod hostname to be podname for dns srv records](https://github.com/kubernetes/dns/issues/232) is an enhancement that is supported through the "endpoint_pod_names" feature described below.
--->
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/55">dns#55 - kube-dns 的自定义 DNS 条目&lt;/a> 可以使用 &lt;a href="https://coredns.io/plugins/kubernetes">kubernetes 插件&lt;/a> 中的 &amp;quot;fallthrough&amp;quot; 机制，使用 &lt;a href="https://coredns.io/plugins/rewrite">rewrite 插件&lt;/a>，或者分区使用不同的插件，例如 &lt;a href="https://coredns.io/plugins/file">file 插件&lt;/a>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/116">dns#116 - 对具有相同主机名的、提供无头服务服务的 Pod 仅设置了一个 A 记录&lt;/a>。无需任何其他配置即可解决此问题。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/131">dns#131 - externalName 未使用 stubDomains 设置&lt;/a>。无需任何其他配置即可解决此问题。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/167">dns#167 - 允许 skyDNS 为 A/AAAA 记录提供轮换&lt;/a>。可以使用 &lt;a href="https://coredns.io/plugins/loadbalance">负载均衡插件&lt;/a> 配置等效功能。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/190">dns#190 - kube-dns 无法以非 root 用户身份运行&lt;/a>。今天，通过使用 non-default 镜像解决了此问题，但是在将来的版本中，它将成为默认的 CoreDNS 行为。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/232">dns#232 - 在 dns srv 记录中修复 pod hostname 为 podname&lt;/a> 是通过下面提到的 &amp;quot;endpoint_pod_names&amp;quot; 功能进行支持的增强功能。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;!--
## Metrics
The functional behavior of the default CoreDNS configuration is the same as kube-dns. However,
one difference you need to be aware of is that the published metrics are not the same. In kube-dns,
you get separate metrics for `dnsmasq` and `kubedns` (skydns). In CoreDNS there is a completely
different set of metrics, since it is all a single process. You can find more details on these
metrics on the CoreDNS [Prometheus plugin](https://coredns.io/plugins/metrics/) page.
--->
&lt;h2 id="指标">指标&lt;/h2>
&lt;p>CoreDNS 默认配置的功能性行为与 kube-dns 相同。但是，你需要了解的差别之一是二者发布的指标是不同的。在 kube-dns 中，您将分别获得 &lt;code>dnsmasq&lt;/code> 和 &lt;code>kubedns&lt;/code>（skydns）的度量值。在 CoreDNS 中，存在一组完全不同的指标，因为它们在同一个进程中。您可以在 CoreDNS &lt;a href="https://coredns.io/plugins/metrics/">Prometheus 插件&lt;/a> 页面上找到有关这些指标的更多详细信息。&lt;/p>
&lt;!--
## Some special features
The standard CoreDNS Kubernetes configuration is designed to be backwards compatible with the prior
kube-dns behavior. But with some configuration changes, CoreDNS can allow you to modify how the
DNS service discovery works in your cluster. A number of these features are intended to still be
compliant with the [Kubernetes DNS specification](https://github.com/kubernetes/dns/blob/master/docs/specification.md);
they enhance functionality but remain backward compatible. Since CoreDNS is not
*only* made for Kubernetes, but is instead a general-purpose DNS server, there are many things you
can do beyond that specification.
--->
&lt;h2 id="一些特殊功能">一些特殊功能&lt;/h2>
&lt;p>标准的 CoreDNS Kubernetes 配置旨在与以前的 kube-dns 在行为上向后兼容。但是，通过进行一些配置更改，CoreDNS 允许您修改 DNS 服务发现在群集中的工作方式。这些功能中的许多功能仍要符合 &lt;a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">Kubernetes DNS规范&lt;/a>；它们在增强了功能的同时保持向后兼容。由于 CoreDNS 并非 &lt;em>仅&lt;/em> 用于 Kubernetes，而是通用的 DNS 服务器，因此您可以做很多超出该规范的事情。&lt;/p>
&lt;!--
### Pods verified mode
In kube-dns, pod name records are "fake". That is, any "a-b-c-d.namespace.pod.cluster.local" query will
return the IP address "a.b.c.d". In some cases, this can weaken the identity guarantees offered by TLS. So,
CoreDNS offers a "pods verified" mode, which will only return the IP address if there is a pod in the
specified namespace with that IP address.
--->
&lt;h3 id="pod-验证模式">Pod 验证模式&lt;/h3>
&lt;p>在 kube-dns 中，Pod 名称记录是 &amp;quot;伪造的&amp;quot;。也就是说，任何 &amp;quot;a-b-c-d.namespace.pod.cluster.local&amp;quot; 查询都将返回 IP 地址 &amp;quot;a.b.c.d&amp;quot;。在某些情况下，这可能会削弱 TLS 提供的身份确认。因此，CoreDNS 提供了一种 &amp;quot;Pod 验证&amp;quot; 的模式，该模式仅在指定名称空间中存在具有该 IP 地址的 Pod 时才返回 IP 地址。&lt;/p>
&lt;!--
### Endpoint names based on pod names
In kube-dns, when using a headless service, you can use an SRV request to get a list of
all endpoints for the service:
--->
&lt;h3 id="基于-pod-名称的端点名称">基于 Pod 名称的端点名称&lt;/h3>
&lt;p>在 kube-dns 中，使用无头服务时，可以使用 SRV 请求获取该服务的所有端点的列表：&lt;/p>
&lt;pre>&lt;code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 10 33 0 6234396237313665.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 10 33 0 6662363165353239.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 10 33 0 6338633437303230.headless.default.svc.cluster.local.
dnstools#
&lt;/code>&lt;/pre>&lt;!--
However, the endpoint DNS names are (for practical purposes) random. In CoreDNS, by default, you get endpoint
DNS names based upon the endpoint IP address:
--->
&lt;p>但是，端点 DNS 名称（出于实际目的）是随机的。在 CoreDNS 中，默认情况下，您所获得的端点 DNS 名称是基于端点 IP 地址生成的：&lt;/p>
&lt;pre>&lt;code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-14.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-18.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-4.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-9.headless.default.svc.cluster.local.
&lt;/code>&lt;/pre>&lt;!--
For some applications, it is desirable to have the pod name for this, rather than the pod IP
address (see for example [kubernetes#47992](https://github.com/kubernetes/kubernetes/issues/47992) and [coredns#1190](https://github.com/coredns/coredns/pull/1190)). To enable this in CoreDNS, you specify the "endpoint_pod_names" option in your Corefile, which results in this:
--->
&lt;p>对于某些应用程序，你会希望在这里使用 Pod 名称，而不是 Pod IP 地址（例如，参见 &lt;a href="https://github.com/kubernetes/kubernetes/issues/47992">kubernetes#47992&lt;/a> 和 &lt;a href="https://github.com/coredns/coredns/pull/1190">coredns#1190&lt;/a>）。要在 CoreDNS 中启用此功能，请在 Corefile 中指定 &amp;quot;endpoint_pod_names&amp;quot; 选项，结果如下：&lt;/p>
&lt;pre>&lt;code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-qv84p.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-zc8lx.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-q7lf2.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-566rt.headless.default.svc.cluster.local.
&lt;/code>&lt;/pre>&lt;!--
### Autopath
CoreDNS also has a special feature to improve latency in DNS requests for external names. In Kubernetes, the
DNS search path for pods specifies a long list of suffixes. This enables the use of short names when requesting
services in the cluster - for example, "headless" above, rather than "headless.default.svc.cluster.local". However,
when requesting an external name - "infoblox.com", for example - several invalid DNS queries are made by the client,
requiring a roundtrip from the client to kube-dns each time (actually to `dnsmasq` and then to `kubedns`, since [negative caching is disabled](https://github.com/kubernetes/dns/issues/121)):
--->
&lt;h3 id="自动路径">自动路径&lt;/h3>
&lt;p>CoreDNS 还具有一项特殊功能，可以改善 DNS 中外部名称请求的延迟。在 Kubernetes 中，Pod 的 DNS 搜索路径指定了一长串后缀。这一特点使得你可以针对集群中服务使用短名称 - 例如，上面的 &amp;quot;headless&amp;quot;，而不是 &amp;quot;headless.default.svc.cluster.local&amp;quot;。但是，当请求一个外部名称（例如 &amp;quot;infoblox.com&amp;quot;）时，客户端会进行几个无效的 DNS 查询，每次都需要从客户端到 kube-dns 往返（实际上是到 &lt;code>dnsmasq&lt;/code>，然后到 &lt;code>kubedns&lt;/code>），因为 &lt;a href="https://github.com/kubernetes/dns/issues/121">禁用了负缓存&lt;/a>）&lt;/p>
&lt;ul>
&lt;li>infoblox.com.default.svc.cluster.local -&amp;gt; NXDOMAIN&lt;/li>
&lt;li>infoblox.com.svc.cluster.local -&amp;gt; NXDOMAIN&lt;/li>
&lt;li>infoblox.com.cluster.local -&amp;gt; NXDOMAIN&lt;/li>
&lt;li>infoblox.com.your-internal-domain.com -&amp;gt; NXDOMAIN&lt;/li>
&lt;/ul>
&lt;!--
* infoblox.com -> returns a valid record
--->
&lt;ul>
&lt;li>infoblox.com -&amp;gt; 返回有效记录&lt;/li>
&lt;/ul>
&lt;!--
In CoreDNS, an optional feature called [autopath](https://coredns.io/plugins/autopath) can be enabled that will cause this search path to be followed
*in the server*. That is, CoreDNS will figure out from the source IP address which namespace the client pod is in,
and it will walk this search list until it gets a valid answer. Since the first 3 of these are resolved internally
within CoreDNS itself, it cuts out all of the back and forth between the client and server, reducing latency.
--->
&lt;p>在 CoreDNS 中，可以启用 &lt;a href="https://coredns.io/plugins/autopath">autopath&lt;/a> 的可选功能，该功能使搜索路径在 &lt;em>服务器端&lt;/em> 遍历。也就是说，CoreDNS 将基于源 IP 地址判断客户端 Pod 所在的命名空间，并且遍历此搜索列表，直到获得有效答案为止。由于其中的前三个是在 CoreDNS 本身内部解决的，因此它消除了客户端和服务器之间所有的来回通信，从而减少了延迟。&lt;/p>
&lt;!--
### A few other Kubernetes specific features
In CoreDNS, you can use standard DNS zone transfer to export the entire DNS record set. This is useful for
debugging your services as well as importing the cluster zone into other DNS servers.
You can also filter by namespaces or a label selector. This can allow you to run specific CoreDNS instances that will only server records that match the filters, exposing only a limited set of your services via DNS.
--->
&lt;h3 id="其他一些特定于-kubernetes-的功能">其他一些特定于 Kubernetes 的功能&lt;/h3>
&lt;p>在 CoreDNS 中，您可以使用标准 DNS 区域传输来导出整个 DNS 记录集。这对于调试服务以及将集群区导入其他 DNS 服务器很有用。&lt;/p>
&lt;p>您还可以按名称空间或标签选择器进行过滤。这样，您可以运行特定的 CoreDNS 实例，该实例仅服务与过滤器匹配的记录，从而通过 DNS 公开受限的服务集。&lt;/p>
&lt;!--
## Extensibility
In addition to the features described above, CoreDNS is easily extended. It is possible to build custom versions
of CoreDNS that include your own features. For example, this ability has been used to extend CoreDNS to do recursive resolution
with the [unbound plugin](https://coredns.io/explugins/unbound), to server records directly from a database with the [pdsql plugin](https://coredns.io/explugins/pdsql), and to allow multiple CoreDNS instances to share a common level 2 cache with the [redisc plugin](https://coredns.io/explugins/redisc).
Many other interesting extensions have been added, which you will find on the [External Plugins](https://coredns.io/explugins/) page of the CoreDNS site. One that is really interesting for Kubernetes and Istio users is the [kubernetai plugin](https://coredns.io/explugins/kubernetai), which allows a single CoreDNS instance to connect to multiple Kubernetes clusters and provide service discovery across all of them.
--->
&lt;h2 id="可扩展性">可扩展性&lt;/h2>
&lt;p>除了上述功能之外，CoreDNS 还可轻松扩展，构建包含您独有的功能的自定义版本的 CoreDNS。例如，这一能力已被用于扩展 CoreDNS 来使用 &lt;a href="https://coredns.io/explugins/unbound">unbound 插件&lt;/a> 进行递归解析、使用 &lt;a href="https://coredns.io/explugins/pdsql">pdsql 插件&lt;/a> 直接从数据库提供记录，以及使用 &lt;a href="https://coredns.io/explugins/redisc">redisc 插件&lt;/a> 与多个 CoreDNS 实例共享一个公共的 2 级缓存。&lt;/p>
&lt;p>已添加的还有许多其他有趣的扩展，您可以在 CoreDNS 站点的 &lt;a href="https://coredns.io/explugins/">外部插件&lt;/a> 页面上找到这些扩展。Kubernetes 和 Istio 用户真正感兴趣的是 &lt;a href="https://coredns.io/explugins/kubernetai">kubernetai 插件&lt;/a>，它允许单个 CoreDNS 实例连接到多个 Kubernetes 集群并在所有集群中提供服务发现 。&lt;/p>
&lt;!--
## What's Next?
CoreDNS is an independent project, and as such is developing many features that are not directly
related to Kubernetes. However, a number of these will have applications within Kubernetes. For example,
the upcoming integration with policy engines will allow CoreDNS to make intelligent choices about which endpoint
to return when a headless service is requested. This could be used to route traffic to a local pod, or
to a more responsive pod. Many other features are in development, and of course as an open source project, we welcome you to suggest and contribute your own features!
The features and differences described above are a few examples. There is much more you can do with CoreDNS.
You can find out more on the [CoreDNS Blog](https://coredns.io/blog).
--->
&lt;h2 id="下一步工作">下一步工作&lt;/h2>
&lt;p>CoreDNS 是一个独立的项目，许多与 Kubernetes 不直接相关的功能正在开发中。但是，其中许多功能将在 Kubernetes 中具有对应的应用。例如，与策略引擎完成集成后，当请求无头服务时，CoreDNS 能够智能地选择返回哪个端点。这可用于将流量分流到到本地 Pod 或响应更快的 Pod。更多的其他功能正在开发中，当然作为一个开源项目，我们欢迎您提出建议并贡献自己的功能特性！&lt;/p>
&lt;p>上述特征和差异是几个示例。CoreDNS 还可以做更多的事情。您可以在 &lt;a href="https://coredns.io/blog">CoreDNS 博客&lt;/a> 上找到更多信息。&lt;/p>
&lt;!--
### Get involved with CoreDNS
CoreDNS is an incubated [CNCF](https:://cncf.io) project.
We're most active on Slack (and GitHub):
--->
&lt;h3 id="参与-coredns">参与 CoreDNS&lt;/h3>
&lt;p>CoreDNS 是一个 &lt;a href="https:://cncf.io">CNCF&lt;/a> 孵化项目。&lt;/p>
&lt;p>我们在 Slack（和 GitHub）上最活跃：&lt;/p>
&lt;ul>
&lt;li>Slack: #coredns on &lt;a href="https://slack.cncf.io">https://slack.cncf.io&lt;/a>&lt;/li>
&lt;li>GitHub: &lt;a href="https://github.com/coredns/coredns">https://github.com/coredns/coredns&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
More resources can be found:
--->
&lt;p>更多资源请浏览：&lt;/p>
&lt;ul>
&lt;li>Website: &lt;a href="https://coredns.io">https://coredns.io&lt;/a>&lt;/li>
&lt;li>Blog: &lt;a href="https://blog.coredns.io">https://blog.coredns.io&lt;/a>&lt;/li>
&lt;li>Twitter: &lt;a href="https://twitter.com/corednsio">@corednsio&lt;/a>&lt;/li>
&lt;li>Mailing list/group: &lt;a href="mailto:coredns-discuss@googlegroups.com">coredns-discuss@googlegroups.com&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: IPVS-Based In-Cluster Load Balancing Deep Dive</title><link>https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</link><pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</guid><description>
&lt;!--
Author: Jun Du(Huawei), Haibin Xie(Huawei), Wei Liang(Huawei)
Editor’s note: this post is part of a series of in-depth articles on what’s new in Kubernetes 1.11
-->
&lt;p>作者: Jun Du(华为), Haibin Xie(华为), Wei Liang(华为)&lt;/p>
&lt;p>注意: 这篇文章出自 系列深度文章 介绍 Kubernetes 1.11 的新特性&lt;/p>
&lt;!--
Introduction
Per the Kubernetes 1.11 release blog post , we announced that IPVS-Based In-Cluster Service Load Balancing graduates to General Availability. In this blog, we will take you through a deep dive of the feature.
-->
&lt;p>介绍&lt;/p>
&lt;p>根据 Kubernetes 1.11 发布的博客文章, 我们宣布基于 IPVS 的集群内部服务负载均衡已达到一般可用性。 在这篇博客中，我们将带您深入了解该功能。&lt;/p>
&lt;!--
What Is IPVS?
IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel.
IPVS is incorporated into the LVS (Linux Virtual Server), where it runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service.
-->
&lt;p>什么是 IPVS ?&lt;/p>
&lt;p>IPVS (IP Virtual Server)是在 Netfilter 上层构建的，并作为 Linux 内核的一部分，实现传输层负载均衡。&lt;/p>
&lt;p>IPVS 集成在 LVS（Linux Virtual Server，Linux 虚拟服务器）中，它在主机上运行，并在物理服务器集群前作为负载均衡器。IPVS 可以将基于 TCP 和 UDP 服务的请求定向到真实服务器，并使真实服务器的服务在单个IP地址上显示为虚拟服务。 因此，IPVS 自然支持 Kubernetes 服务。&lt;/p>
&lt;!--
Why IPVS for Kubernetes?
As Kubernetes grows in usage, the scalability of its resources becomes more and more important. In particular, the scalability of services is paramount to the adoption of Kubernetes by developers/companies running large workloads.
Kube-proxy, the building block of service routing has relied on the battle-hardened iptables to implement the core supported Service types such as ClusterIP and NodePort. However, iptables struggles to scale to tens of thousands of Services because it is designed purely for firewalling purposes and is based on in-kernel rule lists.
Even though Kubernetes already support 5000 nodes in release v1.6, the kube-proxy with iptables is actually a bottleneck to scale the cluster to 5000 nodes. One example is that with NodePort Service in a 5000-node cluster, if we have 2000 services and each services have 10 pods, this will cause at least 20000 iptable records on each worker node, and this can make the kernel pretty busy.
On the other hand, using IPVS-based in-cluster service load balancing can help a lot for such cases. IPVS is specifically designed for load balancing and uses more efficient data structures (hash tables) allowing for almost unlimited scale under the hood.
-->
&lt;p>为什么为 Kubernetes 选择 IPVS ?&lt;/p>
&lt;p>随着 Kubernetes 的使用增长，其资源的可扩展性变得越来越重要。特别是，服务的可扩展性对于运行大型工作负载的开发人员/公司采用 Kubernetes 至关重要。&lt;/p>
&lt;p>Kube-proxy 是服务路由的构建块，它依赖于经过强化攻击的 iptables 来实现支持核心的服务类型，如 ClusterIP 和 NodePort。 但是，iptables 难以扩展到成千上万的服务，因为它纯粹是为防火墙而设计的，并且基于内核规则列表。&lt;/p>
&lt;p>尽管 Kubernetes 在版本v1.6中已经支持5000个节点，但使用 iptables 的 kube-proxy 实际上是将集群扩展到5000个节点的瓶颈。 一个例子是，在5000节点集群中使用 NodePort 服务，如果我们有2000个服务并且每个服务有10个 pod，这将在每个工作节点上至少产生20000个 iptable 记录，这可能使内核非常繁忙。&lt;/p>
&lt;p>另一方面，使用基于 IPVS 的集群内服务负载均衡可以为这种情况提供很多帮助。 IPVS 专门用于负载均衡，并使用更高效的数据结构（哈希表），允许几乎无限的规模扩张。&lt;/p>
&lt;!--
IPVS-based Kube-proxy
Parameter Changes
Parameter: --proxy-mode In addition to existing userspace and iptables modes, IPVS mode is configured via --proxy-mode=ipvs. It implicitly uses IPVS NAT mode for service port mapping.
-->
&lt;p>基于 IPVS 的 Kube-proxy&lt;/p>
&lt;p>参数更改&lt;/p>
&lt;p>参数: --proxy-mode 除了现有的用户空间和 iptables 模式，IPVS 模式通过--proxy-mode = ipvs 进行配置。 它隐式使用 IPVS NAT 模式进行服务端口映射。&lt;/p>
&lt;!--
Parameter: --ipvs-scheduler
A new kube-proxy parameter has been added to specify the IPVS load balancing algorithm, with the parameter being --ipvs-scheduler. If it’s not configured, then round-robin (rr) is the default value.
- rr: round-robin
- lc: least connection
- dh: destination hashing
- sh: source hashing
- sed: shortest expected delay
- nq: never queue
In the future, we can implement Service specific scheduler (potentially via annotation), which has higher priority and overwrites the value.
-->
&lt;p>参数: --ipvs-scheduler&lt;/p>
&lt;p>添加了一个新的 kube-proxy 参数来指定 IPVS 负载均衡算法，参数为 --ipvs-scheduler。 如果未配置，则默认为 round-robin 算法（rr）。&lt;/p>
&lt;ul>
&lt;li>rr: round-robin&lt;/li>
&lt;li>lc: least connection&lt;/li>
&lt;li>dh: destination hashing&lt;/li>
&lt;li>sh: source hashing&lt;/li>
&lt;li>sed: shortest expected delay&lt;/li>
&lt;li>nq: never queue&lt;/li>
&lt;/ul>
&lt;p>将来，我们可以实现特定于服务的调度程序（可能通过注释），该调度程序具有更高的优先级并覆盖该值。&lt;/p>
&lt;!--
Parameter: --cleanup-ipvs Similar to the --cleanup-iptables parameter, if true, cleanup IPVS configuration and IPTables rules that are created in IPVS mode.
Parameter: --ipvs-sync-period Maximum interval of how often IPVS rules are refreshed (e.g. '5s', '1m'). Must be greater than 0.
Parameter: --ipvs-min-sync-period Minimum interval of how often the IPVS rules are refreshed (e.g. '5s', '1m'). Must be greater than 0.
-->
&lt;p>参数: --cleanup-ipvs 类似于 --cleanup-iptables 参数，如果为 true，则清除在 IPVS 模式下创建的 IPVS 配置和 IPTables 规则。&lt;/p>
&lt;p>参数: --ipvs-sync-period 刷新 IPVS 规则的最大间隔时间（例如'5s'，'1m'）。 必须大于0。&lt;/p>
&lt;p>参数: --ipvs-min-sync-period 刷新 IPVS 规则的最小间隔时间间隔（例如'5s'，'1m'）。 必须大于0。&lt;/p>
&lt;!--
Parameter: --ipvs-exclude-cidrs A comma-separated list of CIDR's which the IPVS proxier should not touch when cleaning up IPVS rules because IPVS proxier can't distinguish kube-proxy created IPVS rules from user original IPVS rules. If you are using IPVS proxier with your own IPVS rules in the environment, this parameter should be specified, otherwise your original rule will be cleaned.
-->
&lt;p>参数: --ipvs-exclude-cidrs 清除 IPVS 规则时 IPVS 代理不应触及的 CIDR 的逗号分隔列表，因为 IPVS 代理无法区分 kube-proxy 创建的 IPVS 规则和用户原始规则 IPVS 规则。 如果您在环境中使用 IPVS proxier 和您自己的 IPVS 规则，则应指定此参数，否则将清除原始规则。&lt;/p>
&lt;!--
Design Considerations
IPVS Service Network Topology
When creating a ClusterIP type Service, IPVS proxier will do the following three things:
- Make sure a dummy interface exists in the node, defaults to kube-ipvs0
- Bind Service IP addresses to the dummy interface
- Create IPVS virtual servers for each Service IP address respectively
-->
&lt;p>设计注意事项&lt;/p>
&lt;p>IPVS 服务网络拓扑&lt;/p>
&lt;p>创建 ClusterIP 类型服务时，IPVS proxier 将执行以下三项操作：&lt;/p>
&lt;ul>
&lt;li>确保节点中存在虚拟接口，默认为 kube-ipvs0&lt;/li>
&lt;li>将服务 IP 地址绑定到虚拟接口&lt;/li>
&lt;li>分别为每个服务 IP 地址创建 IPVS 虚拟服务器&lt;/li>
&lt;/ul>
&lt;!--
Here comes an example:
# kubectl describe svc nginx-service
Name: nginx-service
...
Type: ClusterIP
IP: 10.102.128.4
Port: http 3080/TCP
Endpoints: 10.244.0.235:8080,10.244.1.237:8080
Session Affinity: None
# ip addr
...
73: kube-ipvs0: &lt;BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN qlen 1000
link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
inet 10.102.128.4/32 scope global kube-ipvs0
valid_lft forever preferred_lft forever
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-> RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 10.102.128.4:3080 rr
-> 10.244.0.235:8080 Masq 1 0 0
-> 10.244.1.237:8080 Masq 1 0 0
-->
&lt;p>这是一个例子:&lt;/p>
&lt;pre>&lt;code># kubectl describe svc nginx-service
Name: nginx-service
...
Type: ClusterIP
IP: 10.102.128.4
Port: http 3080/TCP
Endpoints: 10.244.0.235:8080,10.244.1.237:8080
Session Affinity: None
# ip addr
...
73: kube-ipvs0: &amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state DOWN qlen 1000
link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
inet 10.102.128.4/32 scope global kube-ipvs0
valid_lft forever preferred_lft forever
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-&amp;gt; RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 10.102.128.4:3080 rr
-&amp;gt; 10.244.0.235:8080 Masq 1 0 0
-&amp;gt; 10.244.1.237:8080 Masq 1 0 0
&lt;/code>&lt;/pre>
&lt;!--
Please note that the relationship between a Kubernetes Service and IPVS virtual servers is 1:N. For example, consider a Kubernetes Service that has more than one IP address. An External IP type Service has two IP addresses - ClusterIP and External IP. Then the IPVS proxier will create 2 IPVS virtual servers - one for Cluster IP and another one for External IP. The relationship between a Kubernetes Endpoint (each IP+Port pair) and an IPVS virtual server is 1:1.
Deleting of a Kubernetes service will trigger deletion of the corresponding IPVS virtual server, IPVS real servers and its IP addresses bound to the dummy interface.
Port Mapping
There are three proxy modes in IPVS: NAT (masq), IPIP and DR. Only NAT mode supports port mapping. Kube-proxy leverages NAT mode for port mapping. The following example shows IPVS mapping Service port 3080 to Pod port 8080.
-->
&lt;p>请注意，Kubernetes 服务和 IPVS 虚拟服务器之间的关系是“1：N”。 例如，考虑具有多个 IP 地址的 Kubernetes 服务。 外部 IP 类型服务有两个 IP 地址 - 集群IP和外部 IP。 然后，IPVS 代理将创建2个 IPVS 虚拟服务器 - 一个用于集群 IP，另一个用于外部 IP。 Kubernetes 的 endpoint（每个IP +端口对）与 IPVS 虚拟服务器之间的关系是“1：1”。&lt;/p>
&lt;p>删除 Kubernetes 服务将触发删除相应的 IPVS 虚拟服务器，IPVS 物理服务器及其绑定到虚拟接口的 IP 地址。&lt;/p>
&lt;p>端口映射&lt;/p>
&lt;p>IPVS 中有三种代理模式：NAT（masq），IPIP 和 DR。 只有 NAT 模式支持端口映射。 Kube-proxy 利用 NAT 模式进行端口映射。 以下示例显示 IPVS 服务端口3080到Pod端口8080的映射。&lt;/p>
&lt;pre>&lt;code>TCP 10.102.128.4:3080 rr
-&amp;gt; 10.244.0.235:8080 Masq 1 0 0
-&amp;gt; 10.244.1.237:8080 Masq 1 0
&lt;/code>&lt;/pre>
&lt;!--
Session Affinity
IPVS supports client IP session affinity (persistent connection). When a Service specifies session affinity, the IPVS proxier will set a timeout value (180min=10800s by default) in the IPVS virtual server. For example:
-->
&lt;p>会话关系&lt;/p>
&lt;p>IPVS 支持客户端 IP 会话关联（持久连接）。 当服务指定会话关系时，IPVS 代理将在 IPVS 虚拟服务器中设置超时值（默认为180分钟= 10800秒）。 例如：&lt;/p>
&lt;pre>&lt;code># kubectl describe svc nginx-service
Name: nginx-service
...
IP: 10.102.128.4
Port: http 3080/TCP
Session Affinity: ClientIP
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-&amp;gt; RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 10.102.128.4:3080 rr persistent 10800
&lt;/code>&lt;/pre>
&lt;!--
Iptables &amp; Ipset in IPVS Proxier
IPVS is for load balancing and it can't handle other workarounds in kube-proxy, e.g. packet filtering, hairpin-masquerade tricks, SNAT, etc.
IPVS proxier leverages iptables in the above scenarios. Specifically, ipvs proxier will fall back on iptables in the following 4 scenarios:
- kube-proxy start with --masquerade-all=true
- Specify cluster CIDR in kube-proxy startup
- Support Loadbalancer type service
- Support NodePort type service
However, we don't want to create too many iptables rules. So we adopt ipset for the sake of decreasing iptables rules. The following is the table of ipset sets that IPVS proxier maintains:
-->
&lt;p>IPVS 代理中的 Iptables 和 Ipset&lt;/p>
&lt;p>IPVS 用于负载均衡，它无法处理 kube-proxy 中的其他问题，例如 包过滤，数据包欺骗，SNAT 等&lt;/p>
&lt;p>IPVS proxier 在上述场景中利用 iptables。 具体来说，ipvs proxier 将在以下4种情况下依赖于 iptables：&lt;/p>
&lt;ul>
&lt;li>kube-proxy 以 --masquerade-all = true 开头&lt;/li>
&lt;li>在 kube-proxy 启动中指定集群 CIDR&lt;/li>
&lt;li>支持 Loadbalancer 类型服务&lt;/li>
&lt;li>支持 NodePort 类型的服务&lt;/li>
&lt;/ul>
&lt;p>但是，我们不想创建太多的 iptables 规则。 所以我们采用 ipset 来减少 iptables 规则。 以下是 IPVS proxier 维护的 ipset 集表：&lt;/p>
&lt;!--
set name members usage
KUBE-CLUSTER-IP All Service IP + port masquerade for cases that masquerade-all=true or clusterCIDR specified
KUBE-LOOP-BACK All Service IP + port + IP masquerade for resolving hairpin issue
KUBE-EXTERNAL-IP Service External IP + port masquerade for packets to external IPs
KUBE-LOAD-BALANCER Load Balancer ingress IP + port masquerade for packets to Load Balancer type service
KUBE-LOAD-BALANCER-LOCAL Load Balancer ingress IP + port with externalTrafficPolicy=local accept packets to Load Balancer with externalTrafficPolicy=local
KUBE-LOAD-BALANCER-FW Load Balancer ingress IP + port with loadBalancerSourceRanges Drop packets for Load Balancer type Service with loadBalancerSourceRanges specified
KUBE-LOAD-BALANCER-SOURCE-CIDR Load Balancer ingress IP + port + source CIDR accept packets for Load Balancer type Service with loadBalancerSourceRanges specified
KUBE-NODE-PORT-TCP NodePort type Service TCP port masquerade for packets to NodePort(TCP)
KUBE-NODE-PORT-LOCAL-TCP NodePort type Service TCP port with externalTrafficPolicy=local accept packets to NodePort Service with externalTrafficPolicy=local
KUBE-NODE-PORT-UDP NodePort type Service UDP port masquerade for packets to NodePort(UDP)
KUBE-NODE-PORT-LOCAL-UDP NodePort type service UDP port with externalTrafficPolicy=local accept packets to NodePort Service with externalTrafficPolicy=local
-->
&lt;p>设置名称 成员 用法
KUBE-CLUSTER-IP 所有服务 IP + 端口 masquerade-all=true 或 clusterCIDR 指定的情况下进行伪装
KUBE-LOOP-BACK 所有服务 IP +端口+ IP 解决数据包欺骗问题
KUBE-EXTERNAL-IP 服务外部 IP +端口 将数据包伪装成外部 IP
KUBE-LOAD-BALANCER 负载均衡器入口 IP +端口 将数据包伪装成 Load Balancer 类型的服务
KUBE-LOAD-BALANCER-LOCAL 负载均衡器入口 IP +端口 以及 externalTrafficPolicy=local 接受数据包到 Load Balancer externalTrafficPolicy=local
KUBE-LOAD-BALANCER-FW 负载均衡器入口 IP +端口 以及 loadBalancerSourceRanges 使用指定的 loadBalancerSourceRanges 丢弃 Load Balancer类型Service的数据包
KUBE-LOAD-BALANCER-SOURCE-CIDR 负载均衡器入口 IP +端口 + 源 CIDR 接受 Load Balancer 类型 Service 的数据包，并指定loadBalancerSourceRanges
KUBE-NODE-PORT-TCP NodePort 类型服务 TCP 将数据包伪装成 NodePort（TCP）
KUBE-NODE-PORT-LOCAL-TCP NodePort 类型服务 TCP 端口，带有 externalTrafficPolicy=local 接受数据包到 NodePort 服务 使用 externalTrafficPolicy=local
KUBE-NODE-PORT-UDP NodePort 类型服务 UDP 端口 将数据包伪装成 NodePort(UDP)
KUBE-NODE-PORT-LOCAL-UDP NodePort 类型服务 UDP 端口 使用 externalTrafficPolicy=local 接受数据包到NodePort服务 使用 externalTrafficPolicy=local&lt;/p>
&lt;!--
In general, for IPVS proxier, the number of iptables rules is static, no matter how many Services/Pods we have.
-->
&lt;p>通常，对于 IPVS proxier，无论我们有多少 Service/ Pod，iptables 规则的数量都是静态的。&lt;/p>
&lt;!--
Run kube-proxy in IPVS Mode
Currently, local-up scripts, GCE scripts, and kubeadm support switching IPVS proxy mode via exporting environment variables (KUBE_PROXY_MODE=ipvs) or specifying flag (--proxy-mode=ipvs). Before running IPVS proxier, please ensure IPVS required kernel modules are already installed.
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
Finally, for Kubernetes v1.10, feature gate SupportIPVSProxyMode is set to true by default. For Kubernetes v1.11, the feature gate is entirely removed. However, you need to enable --feature-gates=SupportIPVSProxyMode=true explicitly for Kubernetes before v1.10.
-->
&lt;p>在 IPVS 模式下运行 kube-proxy&lt;/p>
&lt;p>目前，本地脚本，GCE 脚本和 kubeadm 支持通过导出环境变量（KUBE_PROXY_MODE=ipvs）或指定标志（--proxy-mode=ipvs）来切换 IPVS 代理模式。 在运行IPVS 代理之前，请确保已安装 IPVS 所需的内核模块。&lt;/p>
&lt;pre>&lt;code>ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
&lt;/code>&lt;/pre>
&lt;p>最后，对于 Kubernetes v1.10，“SupportIPVSProxyMode” 默认设置为 “true”。 对于 Kubernetes v1.11 ，该选项已完全删除。 但是，您需要在v1.10之前为Kubernetes 明确启用 --feature-gates = SupportIPVSProxyMode = true。&lt;/p>
&lt;!--
Get Involved
The simplest way to get involved with Kubernetes is by joining one of the many Special Interest Groups (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly community meeting, and through the channels below.
Thank you for your continued feedback and support.
Post questions (or answer questions) on Stack Overflow
Join the community portal for advocates on K8sPort
Follow us on Twitter @Kubernetesio for latest updates
Chat with the community on Slack
Share your Kubernetes story
-->
&lt;p>参与其中&lt;/p>
&lt;p>参与 Kubernetes 的最简单方法是加入众多&lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">特别兴趣小组&lt;/a> (SIG）中与您的兴趣一致的小组。 你有什么想要向 Kubernetes 社区广播的吗？ 在我们的每周&lt;a href="https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting">社区会议&lt;/a>或通过以下渠道分享您的声音。&lt;/p>
&lt;p>感谢您的持续反馈和支持。
在&lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>上发布问题（或回答问题）&lt;/p>
&lt;p>加入&lt;a href="http://k8sport.org/">K8sPort&lt;/a>的倡导者社区门户网站&lt;/p>
&lt;p>在 Twitter 上关注我们 &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a>获取最新更新&lt;/p>
&lt;p>在&lt;a href="http://slack.k8s.io/">Slack&lt;/a>上与社区聊天&lt;/p>
&lt;p>分享您的 Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">故事&lt;/a>&lt;/p></description></item><item><title>Blog: Airflow在Kubernetes中的使用（第一部分）：一种不同的操作器</title><link>https://kubernetes.io/zh/blog/2018/06/28/airflow%E5%9C%A8kubernetes%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%80%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E6%93%8D%E4%BD%9C%E5%99%A8/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/06/28/airflow%E5%9C%A8kubernetes%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%80%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E6%93%8D%E4%BD%9C%E5%99%A8/</guid><description>
&lt;!--
Author: Daniel Imberman (Bloomberg LP)
-->
&lt;p>作者: Daniel Imberman (Bloomberg LP)&lt;/p>
&lt;!--
## Introduction
As part of Bloomberg's continued commitment to developing the Kubernetes ecosystem, we are excited to announce the Kubernetes Airflow Operator; a mechanism for Apache Airflow, a popular workflow orchestration framework to natively launch arbitrary Kubernetes Pods using the Kubernetes API.
-->
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;p>作为Bloomberg [继续致力于开发Kubernetes生态系统]的一部分（https://www.techatbloomberg.com/blog/bloomberg-awarded-first-cncf-end-user-award-contributions-kubernetes/），我们很高兴能够宣布Kubernetes Airflow Operator的发布; &lt;a href="https://airflow.apache.org/">Apache Airflow&lt;/a>的机制，一种流行的工作流程编排框架，使用Kubernetes API可以在本机启动任意的Kubernetes Pod。&lt;/p>
&lt;!--
## What Is Airflow?
Apache Airflow is one realization of the DevOps philosophy of "Configuration As Code." Airflow allows users to launch multi-step pipelines using a simple Python object DAG (Directed Acyclic Graph). You can define dependencies, programmatically construct complex workflows, and monitor scheduled jobs in an easy to read UI.
-->
&lt;h2 id="什么是airflow">什么是Airflow?&lt;/h2>
&lt;p>Apache Airflow是DevOps“Configuration As Code”理念的一种实现。 Airflow允许用户使用简单的Python对象DAG（有向无环图）启动多步骤流水线。 您可以在易于阅读的UI中定义依赖关系，以编程方式构建复杂的工作流，并监视调度的作业。&lt;/p>
&lt;p>&amp;lt;img src =“/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow_dags.png”width =“85％”alt =“Airflow DAGs”/&amp;gt;&lt;/p>
&lt;p>&amp;lt;img src =“/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow.png”width =“85％”alt =“Airflow UI”/&amp;gt;&lt;/p>
&lt;!--
## Why Airflow on Kubernetes?
Since its inception, Airflow's greatest strength has been its flexibility. Airflow offers a wide range of integrations for services ranging from Spark and HBase, to services on various cloud providers. Airflow also offers easy extensibility through its plug-in framework. However, one limitation of the project is that Airflow users are confined to the frameworks and clients that exist on the Airflow worker at the moment of execution. A single organization can have varied Airflow workflows ranging from data science pipelines to application deployments. This difference in use-case creates issues in dependency management as both teams might use vastly different libraries for their workflows.
To address this issue, we've utilized Kubernetes to allow users to launch arbitrary Kubernetes pods and configurations. Airflow users can now have full power over their run-time environments, resources, and secrets, basically turning Airflow into an "any job you want" workflow orchestrator.
-->
&lt;h2 id="为什么在kubernetes上使用airflow">为什么在Kubernetes上使用Airflow?&lt;/h2>
&lt;p>自成立以来，Airflow的最大优势在于其灵活性。 Airflow提供广泛的服务集成，包括Spark和HBase，以及各种云提供商的服务。 Airflow还通过其插件框架提供轻松的可扩展性。但是，该项目的一个限制是Airflow用户仅限于执行时Airflow站点上存在的框架和客户端。单个组织可以拥有各种Airflow工作流程，范围从数据科学流到应用程序部署。用例中的这种差异会在依赖关系管理中产生问题，因为两个团队可能会在其工作流程使用截然不同的库。&lt;/p>
&lt;p>为了解决这个问题，我们使Kubernetes允许用户启动任意Kubernetes pod和配置。 Airflow用户现在可以在其运行时环境，资源和机密上拥有全部权限，基本上将Airflow转变为“您想要的任何工作”工作流程协调器。&lt;/p>
&lt;!--
## The Kubernetes Operator
Before we move any further, we should clarify that an Operator in Airflow is a task definition. When a user creates a DAG, they would use an operator like the "SparkSubmitOperator" or the "PythonOperator" to submit/monitor a Spark job or a Python function respectively. Airflow comes with built-in operators for frameworks like Apache Spark, BigQuery, Hive, and EMR. It also offers a Plugins entrypoint that allows DevOps engineers to develop their own connectors.
Airflow users are always looking for ways to make deployments and ETL pipelines simpler to manage. Any opportunity to decouple pipeline steps, while increasing monitoring, can reduce future outages and fire-fights. The following is a list of benefits provided by the Airflow Kubernetes Operator:
-->
&lt;h2 id="kubernetes运营商">Kubernetes运营商&lt;/h2>
&lt;p>在进一步讨论之前，我们应该澄清Airflow中的&lt;a href="https://airflow.apache.org/concepts.html#operators">Operator&lt;/a>是一个任务定义。 当用户创建DAG时，他们将使用像“SparkSubmitOperator”或“PythonOperator”这样的operator分别提交/监视Spark作业或Python函数。 Airflow附带了Apache Spark，BigQuery，Hive和EMR等框架的内置运算符。 它还提供了一个插件入口点，允许DevOps工程师开发自己的连接器。&lt;/p>
&lt;p>Airflow用户一直在寻找更易于管理部署和ETL流的方法。 在增加监控的同时，任何解耦流程的机会都可以减少未来的停机等问题。 以下是Airflow Kubernetes Operator提供的好处：&lt;/p>
&lt;!--
* Increased flexibility for deployments:
Airflow's plugin API has always offered a significant boon to engineers wishing to test new functionalities within their DAGs. On the downside, whenever a developer wanted to create a new operator, they had to develop an entirely new plugin. Now, any task that can be run within a Docker container is accessible through the exact same operator, with no extra Airflow code to maintain.
-->
&lt;ul>
&lt;li>提高部署灵活性：&lt;/li>
&lt;/ul>
&lt;p>Airflow的插件API一直为希望在其DAG中测试新功能的工程师提供了重要的福利。 不利的一面是，每当开发人员想要创建一个新的operator时，他们就必须开发一个全新的插件。 现在，任何可以在Docker容器中运行的任务都可以通过完全相同的运算符访问，而无需维护额外的Airflow代码。&lt;/p>
&lt;!--
* Flexibility of configurations and dependencies:
For operators that are run within static Airflow workers, dependency management can become quite difficult. If a developer wants to run one task that requires SciPy and another that requires NumPy, the developer would have to either maintain both dependencies within all Airflow workers or offload the task to an external machine (which can cause bugs if that external machine changes in an untracked manner). Custom Docker images allow users to ensure that the tasks environment, configuration, and dependencies are completely idempotent.
-->
&lt;ul>
&lt;li>配置和依赖的灵活性：&lt;/li>
&lt;/ul>
&lt;p>对于在静态Airflow工作程序中运行的operator，依赖关系管理可能变得非常困难。 如果开发人员想要运行一个需要&lt;a href="https://www.scipy.org">SciPy&lt;/a>的任务和另一个需要&lt;a href="http://www.numpy.org">NumPy&lt;/a>的任务，开发人员必须维护所有Airflow节点中的依赖关系或将任务卸载到其他计算机（如果外部计算机以未跟踪的方式更改，则可能导致错误）。 自定义Docker镜像允许用户确保任务环境，配置和依赖关系完全是幂等的。&lt;/p>
&lt;!--
* Usage of kubernetes secrets for added security:
Handling sensitive data is a core responsibility of any DevOps engineer. At every opportunity, Airflow users want to isolate any API keys, database passwords, and login credentials on a strict need-to-know basis. With the Kubernetes operator, users can utilize the Kubernetes Vault technology to store all sensitive data. This means that the Airflow workers will never have access to this information, and can simply request that pods be built with only the secrets they need.
-->
&lt;ul>
&lt;li>使用kubernetes Secret以增加安全性：&lt;/li>
&lt;/ul>
&lt;p>处理敏感数据是任何开发工程师的核心职责。 Airflow用户总有机会在严格条款的基础上隔离任何API密钥，数据库密码和登录凭据。 使用Kubernetes运算符，用户可以利用Kubernetes Vault技术存储所有敏感数据。 这意味着Airflow工作人员将永远无法访问此信息，并且可以容易地请求仅使用他们需要的密码信息构建pod。&lt;/p>
&lt;!--
# Architecture
The Kubernetes Operator uses the Kubernetes Python Client to generate a request that is processed by the APIServer (1). Kubernetes will then launch your pod with whatever specs you've defined (2). Images will be loaded with all the necessary environment variables, secrets and dependencies, enacting a single command. Once the job is launched, the operator only needs to monitor the health of track logs (3). Users will have the choice of gathering logs locally to the scheduler or to any distributed logging service currently in their Kubernetes cluster.
-->
&lt;p>＃架构&lt;/p>
&lt;p>&amp;lt;img src =“/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow-architecture.png”width =“85％”alt =“Airflow Architecture”/&amp;gt;&lt;/p>
&lt;p>Kubernetes Operator使用&lt;a href="https://github.com/kubernetes-client/Python">Kubernetes Python客户端&lt;/a>生成由APIServer处理的请求（1）。 然后，Kubernetes将使用您定义的需求启动您的pod（2）。映像文件中将加载环境变量，Secret和依赖项，执行单个命令。 一旦启动作业，operator只需要监视跟踪日志的状况（3）。 用户可以选择将日志本地收集到调度程序或当前位于其Kubernetes集群中的任何分布式日志记录服务。&lt;/p>
&lt;!--
# Using the Kubernetes Operator
## A Basic Example
The following DAG is probably the simplest example we could write to show how the Kubernetes Operator works. This DAG creates two pods on Kubernetes: a Linux distro with Python and a base Ubuntu distro without it. The Python pod will run the Python request correctly, while the one without Python will report a failure to the user. If the Operator is working correctly, the passing-task pod should complete, while the failing-task pod returns a failure to the Airflow webserver.
-->
&lt;p>＃使用Kubernetes Operator&lt;/p>
&lt;p>##一个基本的例子&lt;/p>
&lt;p>以下DAG可能是我们可以编写的最简单的示例，以显示Kubernetes Operator的工作原理。 这个DAG在Kubernetes上创建了两个pod：一个带有Python的Linux发行版和一个没有它的基本Ubuntu发行版。 Python pod将正确运行Python请求，而没有Python的那个将向用户报告失败。 如果Operator正常工作，则应该完成“passing-task”pod，而“falling-task”pod则向Airflow网络服务器返回失败。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-Python" data-lang="Python">
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">airflow&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> DAG
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">datetime&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> datetime, timedelta
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">airflow.contrib.operators.kubernetes_pod_operator&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> KubernetesPodOperator
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">airflow.operators.dummy_operator&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> DummyOperator
default_args &lt;span style="color:#666">=&lt;/span> {
&lt;span style="color:#b44">&amp;#39;owner&amp;#39;&lt;/span>: &lt;span style="color:#b44">&amp;#39;airflow&amp;#39;&lt;/span>,
&lt;span style="color:#b44">&amp;#39;depends_on_past&amp;#39;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">False&lt;/span>,
&lt;span style="color:#b44">&amp;#39;start_date&amp;#39;&lt;/span>: datetime&lt;span style="color:#666">.&lt;/span>utcnow(),
&lt;span style="color:#b44">&amp;#39;email&amp;#39;&lt;/span>: [&lt;span style="color:#b44">&amp;#39;airflow@example.com&amp;#39;&lt;/span>],
&lt;span style="color:#b44">&amp;#39;email_on_failure&amp;#39;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">False&lt;/span>,
&lt;span style="color:#b44">&amp;#39;email_on_retry&amp;#39;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">False&lt;/span>,
&lt;span style="color:#b44">&amp;#39;retries&amp;#39;&lt;/span>: &lt;span style="color:#666">1&lt;/span>,
&lt;span style="color:#b44">&amp;#39;retry_delay&amp;#39;&lt;/span>: timedelta(minutes&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">5&lt;/span>)
}
dag &lt;span style="color:#666">=&lt;/span> DAG(
&lt;span style="color:#b44">&amp;#39;kubernetes_sample&amp;#39;&lt;/span>, default_args&lt;span style="color:#666">=&lt;/span>default_args, schedule_interval&lt;span style="color:#666">=&lt;/span>timedelta(minutes&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">10&lt;/span>))
start &lt;span style="color:#666">=&lt;/span> DummyOperator(task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;run_this_first&amp;#39;&lt;/span>, dag&lt;span style="color:#666">=&lt;/span>dag)
passing &lt;span style="color:#666">=&lt;/span> KubernetesPodOperator(namespace&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;default&amp;#39;&lt;/span>,
image&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;Python:3.6&amp;#34;&lt;/span>,
cmds&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;Python&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>],
arguments&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span>],
labels&lt;span style="color:#666">=&lt;/span>{&lt;span style="color:#b44">&amp;#34;foo&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;bar&amp;#34;&lt;/span>},
name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;passing-test&amp;#34;&lt;/span>,
task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;passing-task&amp;#34;&lt;/span>,
get_logs&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">True&lt;/span>,
dag&lt;span style="color:#666">=&lt;/span>dag
)
failing &lt;span style="color:#666">=&lt;/span> KubernetesPodOperator(namespace&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;default&amp;#39;&lt;/span>,
image&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;ubuntu:1604&amp;#34;&lt;/span>,
cmds&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;Python&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>],
arguments&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span>],
labels&lt;span style="color:#666">=&lt;/span>{&lt;span style="color:#b44">&amp;#34;foo&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;bar&amp;#34;&lt;/span>},
name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;fail&amp;#34;&lt;/span>,
task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;failing-task&amp;#34;&lt;/span>,
get_logs&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">True&lt;/span>,
dag&lt;span style="color:#666">=&lt;/span>dag
)
passing&lt;span style="color:#666">.&lt;/span>set_upstream(start)
failing&lt;span style="color:#666">.&lt;/span>set_upstream(start)
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
## But how does this relate to my workflow?
While this example only uses basic images, the magic of Docker is that this same DAG will work for any image/command pairing you want. The following is a recommended CI/CD pipeline to run production-ready code on an Airflow DAG.
### 1: PR in github
Use Travis or Jenkins to run unit and integration tests, bribe your favorite team-mate into PR'ing your code, and merge to the master branch to trigger an automated CI build.
### 2: CI/CD via Jenkins -> Docker Image
Generate your Docker images and bump release version within your Jenkins build.
### 3: Airflow launches task
Finally, update your DAGs to reflect the new release version and you should be ready to go!
-->
&lt;p>##但这与我的工作流程有什么关系？&lt;/p>
&lt;p>虽然这个例子只使用基本映像，但Docker的神奇之处在于，这个相同的DAG可以用于您想要的任何图像/命令配对。 以下是推荐的CI / CD管道，用于在Airflow DAG上运行生产就绪代码。&lt;/p>
&lt;h3 id="1-github中的pr">1：github中的PR&lt;/h3>
&lt;p>使用Travis或Jenkins运行单元和集成测试，请您的朋友PR您的代码，并合并到主分支以触发自动CI构建。&lt;/p>
&lt;h3 id="2-ci-cd构建jenkins-docker-image">2：CI / CD构建Jenkins - &amp;gt; Docker Image&lt;/h3>
&lt;p>&lt;a href="https://getintodevops.com/blog/building-your-first-Docker-image-with-jenkins-2-guide-for-developers">在Jenkins构建中生成Docker镜像和缓冲版本&lt;/a>。&lt;/p>
&lt;h3 id="3-airflow启动任务">3：Airflow启动任务&lt;/h3>
&lt;p>最后，更新您的DAG以反映新版本，您应该准备好了！&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-Python" data-lang="Python">
production_task &lt;span style="color:#666">=&lt;/span> KubernetesPodOperator(namespace&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;default&amp;#39;&lt;/span>,
&lt;span style="color:#080;font-style:italic"># image=&amp;#34;my-production-job:release-1.0.1&amp;#34;, &amp;lt;-- old release&lt;/span>
image&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;my-production-job:release-1.0.2&amp;#34;&lt;/span>,
cmds&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;Python&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>],
arguments&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span>],
name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;fail&amp;#34;&lt;/span>,
task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;failing-task&amp;#34;&lt;/span>,
get_logs&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">True&lt;/span>,
dag&lt;span style="color:#666">=&lt;/span>dag
)
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
# Launching a test deployment
Since the Kubernetes Operator is not yet released, we haven't released an official helm chart or operator (however both are currently in progress). However, we are including instructions for a basic deployment below and are actively looking for foolhardy beta testers to try this new feature. To try this system out please follow these steps:
## Step 1: Set your kubeconfig to point to a kubernetes cluster
## Step 2: Clone the Airflow Repo:
Run git clone https://github.com/apache/incubator-airflow.git to clone the official Airflow repo.
## Step 3: Run
To run this basic deployment, we are co-opting the integration testing script that we currently use for the Kubernetes Executor (which will be explained in the next article of this series). To launch this deployment, run these three commands:
-->
&lt;p>＃启动测试部署&lt;/p>
&lt;p>由于Kubernetes运营商尚未发布，我们尚未发布官方&lt;a href="https://helm.sh/">helm&lt;/a> 图表或operator（但两者目前都在进行中）。 但是，我们在下面列出了基本部署的说明，并且正在积极寻找测试人员来尝试这一新功能。 要试用此系统，请按以下步骤操作：&lt;/p>
&lt;p>##步骤1：将kubeconfig设置为指向kubernetes集群&lt;/p>
&lt;p>##步骤2：clone Airflow 仓库：&lt;/p>
&lt;p>运行git clone https：// github.com / apache / incubator-airflow.git来clone官方Airflow仓库。&lt;/p>
&lt;p>##步骤3：运行&lt;/p>
&lt;p>为了运行这个基本Deployment，我们正在选择我们目前用于Kubernetes Executor的集成测试脚本（将在本系列的下一篇文章中对此进行解释）。 要启动此部署，请运行以下三个命令：&lt;/p>
&lt;pre>&lt;code>
sed -ie &amp;quot;s/KubernetesExecutor/LocalExecutor/g&amp;quot; scripts/ci/kubernetes/kube/configmaps.yaml
./scripts/ci/kubernetes/Docker/build.sh
./scripts/ci/kubernetes/kube/deploy.sh
&lt;/code>&lt;/pre>&lt;!--
Before we move on, let's discuss what these commands are doing:
### sed -ie "s/KubernetesExecutor/LocalExecutor/g" scripts/ci/kubernetes/kube/configmaps.yaml
The Kubernetes Executor is another Airflow feature that allows for dynamic allocation of tasks as idempotent pods. The reason we are switching this to the LocalExecutor is simply to introduce one feature at a time. You are more then welcome to skip this step if you would like to try the Kubernetes Executor, however we will go into more detail in a future article.
### ./scripts/ci/kubernetes/Docker/build.sh
This script will tar the Airflow master source code build a Docker container based on the Airflow distribution
### ./scripts/ci/kubernetes/kube/deploy.sh
Finally, we create a full Airflow deployment on your cluster. This includes Airflow configs, a postgres backend, the webserver + scheduler, and all necessary services between. One thing to note is that the role binding supplied is a cluster-admin, so if you do not have that level of permission on the cluster, you can modify this at scripts/ci/kubernetes/kube/airflow.yaml
## Step 4: Log into your webserver
Now that your Airflow instance is running let's take a look at the UI! The UI lives in port 8080 of the Airflow pod, so simply run
-->
&lt;p>在我们继续之前，让我们讨论这些命令正在做什么：&lt;/p>
&lt;h3 id="sed-ie-s-kubernetesexecutor-localexecutor-g-scripts-ci-kubernetes-kube-configmaps-yaml">sed -ie“s / KubernetesExecutor / LocalExecutor / g”scripts / ci / kubernetes / kube / configmaps.yaml&lt;/h3>
&lt;p>Kubernetes Executor是另一种Airflow功能，允许动态分配任务已解决幂等pod的问题。我们将其切换到LocalExecutor的原因只是一次引入一个功能。如果您想尝试Kubernetes Executor，欢迎您跳过此步骤，但我们将在以后的文章中详细介绍。&lt;/p>
&lt;h3 id="scripts-ci-kubernetes-docker-build-sh">./scripts/ci/kubernetes/Docker/build.sh&lt;/h3>
&lt;p>此脚本将对Airflow主分支代码进行打包，以根据Airflow的发行文件构建Docker容器&lt;/p>
&lt;h3 id="scripts-ci-kubernetes-kube-deploy-sh">./scripts/ci/kubernetes/kube/deploy.sh&lt;/h3>
&lt;p>最后，我们在您的群集上创建完整的Airflow部署。这包括Airflow配置，postgres后端，webserver +调度程序以及之间的所有必要服务。需要注意的一点是，提供的角色绑定是集群管理员，因此如果您没有该集群的权限级别，可以在scripts / ci / kubernetes / kube / airflow.yaml中进行修改。&lt;/p>
&lt;p>##步骤4：登录您的网络服务器&lt;/p>
&lt;p>现在您的Airflow实例正在运行，让我们来看看UI！用户界面位于Airflow pod的8080端口，因此只需运行即可&lt;/p>
&lt;pre>&lt;code>
WEB=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}' | grep &amp;quot;airflow&amp;quot; | head -1)
kubectl port-forward $WEB 8080:8080
&lt;/code>&lt;/pre>&lt;!--
Now the Airflow UI will exist on http://localhost:8080. To log in simply enter airflow/airflow and you should have full access to the Airflow web UI.
## Step 5: Upload a test document
To modify/add your own DAGs, you can use kubectl cp to upload local files into the DAG folder of the Airflow scheduler. Airflow will then read the new DAG and automatically upload it to its system. The following command will upload any local file into the correct directory:
-->
&lt;p>现在，Airflow UI将存在于http://localhost:8080上。 要登录，只需输入airflow /airflow，您就可以完全访问Airflow Web UI。&lt;/p>
&lt;p>##步骤5：上传测试文档&lt;/p>
&lt;p>要修改/添加自己的DAG，可以使用kubectl cp将本地文件上传到Airflow调度程序的DAG文件夹中。 然后，Airflow将读取新的DAG并自动将其上传到其系统。 以下命令将任何本地文件上载到正确的目录中：&lt;/p>
&lt;p>kubectl cp &lt;local file> &lt;namespace>/&lt;pod>:/root/airflow/dags -c scheduler&lt;/p>
&lt;!--
## Step 6: Enjoy!
# So when will I be able to use this?
While this feature is still in the early stages, we hope to see it released for wide release in the next few months.
# Get Involved
This feature is just the beginning of multiple major efforts to improves Apache Airflow integration into Kubernetes. The Kubernetes Operator has been merged into the 1.10 release branch of Airflow (the executor in experimental mode), along with a fully k8s native scheduler called the Kubernetes Executor (article to come). These features are still in a stage where early adopters/contributors can have a huge influence on the future of these features.
For those interested in joining these efforts, I'd recommend checkint out these steps:
* Join the airflow-dev mailing list at dev@airflow.apache.org.
* File an issue in Apache Airflow JIRA
* Join our SIG-BigData meetings on Wednesdays at 10am PST.
* Reach us on slack at #sig-big-data on kubernetes.slack.com
Special thanks to the Apache Airflow and Kubernetes communities, particularly Grant Nicholas, Ben Goldberg, Anirudh Ramanathan, Fokko Dreisprong, and Bolke de Bruin, for your awesome help on these features as well as our future efforts.
-->
&lt;p>##步骤6：使用它！&lt;/p>
&lt;p>#那么我什么时候可以使用它？&lt;/p>
&lt;p>虽然此功能仍处于早期阶段，但我们希望在未来几个月内发布该功能以进行广泛发布。&lt;/p>
&lt;p>#参与其中&lt;/p>
&lt;p>此功能只是将Apache Airflow集成到Kubernetes中的多项主要工作的开始。 Kubernetes Operator已合并到&lt;a href="https://github.com/apache/incubator-airflow/tree/v1-10-test">Airflow的1.10发布分支&lt;/a>（实验模式中的执行模块），以及完整的k8s本地调度程序称为Kubernetes Executor（即将发布文章）。这些功能仍处于早期采用者/贡献者可能对这些功能的未来产生巨大影响的阶段。&lt;/p>
&lt;p>对于有兴趣加入这些工作的人，我建议按照以下步骤：&lt;/p>
&lt;p>*加入airflow-dev邮件列表dev@airflow.apache.org。&lt;/p>
&lt;p>*在[Apache Airflow JIRA]中提出问题（https://issues.apache.org/jira/projects/AIRFLOW/issues/）&lt;/p>
&lt;p>*周三上午10点太平洋标准时间加入我们的SIG-BigData会议。&lt;/p>
&lt;p>*在kubernetes.slack.com上的＃sig-big-data找到我们。&lt;/p>
&lt;p>特别感谢Apache Airflow和Kubernetes社区，特别是Grant Nicholas，Ben Goldberg，Anirudh Ramanathan，Fokko Dreisprong和Bolke de Bruin，感谢您对这些功能的巨大帮助以及我们未来的努力。&lt;/p></description></item><item><title>Blog: Kubernetes 内的动态 Ingress</title><link>https://kubernetes.io/zh/blog/2018/06/07/dynamic-ingress-kubernetes/</link><pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/06/07/dynamic-ingress-kubernetes/</guid><description>
&lt;!--
title: Dynamic Ingress in Kubernetes
date: 2018-06-07
Author: Richard Li (Datawire)
-->
&lt;p>作者: Richard Li (Datawire)&lt;/p>
&lt;!--
Kubernetes makes it easy to deploy applications that consist of many microservices, but one of the key challenges with this type of architecture is dynamically routing ingress traffic to each of these services. One approach is Ambassador, a Kubernetes-native open source API Gateway built on the Envoy Proxy. Ambassador is designed for dynamic environment where services may come and go frequently.
Ambassador is configured using Kubernetes annotations. Annotations are used to configure specific mappings from a given Kubernetes service to a particular URL. A mapping can include a number of annotations for configuring a route. Examples include rate limiting, protocol, cross-origin request sharing, traffic shadowing, and routing rules.
-->
&lt;p>Kubernetes 可以轻松部署由许多微服务组成的应用程序，但这种架构的关键挑战之一是动态地将流量路由到这些服务中的每一个。
一种方法是使用 &lt;a href="https://www.getambassador.io">Ambassador&lt;/a>，
一个基于 &lt;a href="https://www.envoyproxy.io">Envoy Proxy&lt;/a> 构建的 Kubernetes 原生开源 API 网关。
Ambassador 专为动态环境而设计，这类环境中的服务可能被频繁添加或删除。&lt;/p>
&lt;p>Ambassador 使用 Kubernetes 注解进行配置。
注解用于配置从给定 Kubernetes 服务到特定 URL 的具体映射关系。
每个映射中可以包括多个注解，用于配置路由。
注解的例子有速率限制、协议、跨源请求共享（CORS）、流量影射和路由规则等。&lt;/p>
&lt;!--
## A Basic Ambassador Example
Ambassador is typically installed as a Kubernetes deployment, and is also available as a Helm chart. To configure Ambassador, create a Kubernetes service with the Ambassador annotations. Here is an example that configures Ambassador to route requests to /httpbin/ to the public httpbin.org service:
-->
&lt;h2 id="一个简单的-ambassador-示例">一个简单的 Ambassador 示例&lt;/h2>
&lt;p>Ambassador 通常作为 Kubernetes Deployment 来安装，也可以作为 Helm Chart 使用。
配置 Ambassador 时，请使用 Ambassador 注解创建 Kubernetes 服务。
下面是一个例子，用来配置 Ambassador，将针对 /httpbin/ 的请求路由到公共的 httpbin.org 服务：&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Service
metadata:
name: httpbin
annotations:
getambassador.io/config: |
---
apiVersion: ambassador/v0
kind: Mapping
name: httpbin_mapping
prefix: /httpbin/
service: httpbin.org:80
host_rewrite: httpbin.org
spec:
type: ClusterIP
ports:
- port: 80
&lt;/code>&lt;/pre>&lt;!--
A mapping object is created with a prefix of /httpbin/ and a service name of httpbin.org. The host_rewrite annotation specifies that the HTTP host header should be set to httpbin.org.
-->
&lt;p>例子中创建了一个 Mapping 对象，其 prefix 设置为 /httpbin/，service 名称为 httpbin.org。
其中的 host_rewrite 注解指定 HTTP 的 host 头部字段应设置为 httpbin.org。&lt;/p>
&lt;!--
## Kubeflow
Kubeflow provides a simple way to easily deploy machine learning infrastructure on Kubernetes. The Kubeflow team needed a proxy that provided a central point of authentication and routing to the wide range of services used in Kubeflow, many of which are ephemeral in nature.
&lt;center>&lt;i>Kubeflow architecture, pre-Ambassador&lt;/center>&lt;/i>
-->
&lt;h2 id="kubeflow">Kubeflow&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubeflow/kubeflow">Kubeflow&lt;/a> 提供了一种简单的方法，用于在 Kubernetes 上轻松部署机器学习基础设施。
Kubeflow 团队需要一个代理，为 Kubeflow 中所使用的各种服务提供集中化的认证和路由能力；Kubeflow 中许多服务本质上都是生命期很短的。&lt;/p>
&lt;center>&lt;i>Kubeflow architecture, pre-Ambassador&lt;/center>&lt;/i>
&lt;!--
## Service configuration
With Ambassador, Kubeflow can use a distributed model for configuration. Instead of a central configuration file, Ambassador allows each service to configure its route in Ambassador via Kubernetes annotations. Here is a simplified example configuration:
-->
&lt;h2 id="服务配置">服务配置&lt;/h2>
&lt;p>有了 Ambassador，Kubeflow 可以使用分布式模型进行配置。
Ambassador 不使用集中的配置文件，而是允许每个服务通过 Kubernetes 注解在 Ambassador 中配置其路由。
下面是一个简化的配置示例：&lt;/p>
&lt;pre>&lt;code>---
apiVersion: ambassador/v0
kind: Mapping
name: tfserving-mapping-test-post
prefix: /models/test/
rewrite: /model/test/:predict
method: POST
service: test.kubeflow:8000
&lt;/code>&lt;/pre>&lt;!--
In this example, the “test” service uses Ambassador annotations to dynamically configure a route to the service, triggered only when the HTTP method is a POST, and the annotation also specifies a rewrite rule.
-->
&lt;p>示例中，“test” 服务使用 Ambassador 注解来为服务动态配置路由。
所配置的路由仅在 HTTP 方法是 POST 时触发；注解中同时还给出了一条重写规则。&lt;/p>
&lt;!--
With Ambassador, Kubeflow manages routing easily with Kubernetes annotations. Kubeflow configures a single ingress object that directs traffic to Ambassador, then creates services with Ambassador annotations as needed to direct traffic to specific backends. For example, when deploying TensorFlow services, Kubeflow creates and and annotates a K8s service so that the model will be served at https://&lt;ingress host>/models/&lt;model name>/. Kubeflow can also use the Envoy Proxy to do the actual L7 routing. Using Ambassador, Kubeflow takes advantage of additional routing configuration like URL rewriting and method-based routing.
If you’re interested in using Ambassador with Kubeflow, the standard Kubeflow install automatically installs and configures Ambassador.
If you’re interested in using Ambassador as an API Gateway or Kubernetes ingress solution for your non-Kubeflow services, check out the Getting Started with Ambassador guide.
## Kubeflow and Ambassador
-->
&lt;h2 id="kubeflow-和-ambassador">Kubeflow 和 Ambassador&lt;/h2>
&lt;p>通过 Ambassador，Kubeflow 可以使用 Kubernetes 注解轻松管理路由。
Kubeflow 配置同一个 Ingress 对象，将流量定向到 Ambassador，然后根据需要创建具有 Ambassador 注解的服务，以将流量定向到特定后端。
例如，在部署 TensorFlow 服务时，Kubeflow 会创建 Kubernetes 服务并为其添加注解，
以便用户能够在 &lt;code>https://&amp;lt;ingress主机&amp;gt;/models/&amp;lt;模型名称&amp;gt;/&lt;/code> 处访问到模型本身。
Kubeflow 还可以使用 Envoy Proxy 来进行实际的 L7 路由。
通过 Ambassador，Kubeflow 能够更充分地利用 URL 重写和基于方法的路由等额外的路由配置能力。&lt;/p>
&lt;p>如果您对在 Kubeflow 中使用 Ambassador 感兴趣，标准的 Kubeflow 安装会自动安装和配置 Ambassador。&lt;/p>
&lt;p>如果您有兴趣将 Ambassador 用作 API 网关或 Kubernetes 的 Ingress 解决方案，
请参阅 &lt;a href="https://www.getambassador.io/user-guide/getting-started">Ambassador 入门指南&lt;/a>。&lt;/p></description></item><item><title>Blog: Kubernetes 这四年</title><link>https://kubernetes.io/zh/blog/2018/06/06/kubernetes-%E8%BF%99%E5%9B%9B%E5%B9%B4/</link><pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/06/06/kubernetes-%E8%BF%99%E5%9B%9B%E5%B9%B4/</guid><description>
&lt;!--
**Author**: Joe Beda (CTO and Founder, Heptio)
On June 6, 2014 I checked in the [first commit](https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56) of what would become the public repository for Kubernetes. Many would assume that is where the story starts. It is the beginning of history, right? But that really doesn’t tell the whole story.
-->
&lt;p>&lt;strong>作者&lt;/strong>：Joe Beda( Heptio 首席技术官兼创始人)
2014 年 6 月 6 日，我检查了 Kubernetes 公共代码库的&lt;a href="https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56">第一次 commit&lt;/a> 。许多人会认为这是故事开始的地方。这难道不是一切开始的地方吗？但这的确不能把整个过程说清楚。&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-06-06-4-years-of-k8s/k8s-first-commit.png" alt="k8s_first_commit">&lt;/p>
&lt;!--
The cast leading up to that commit was large and the success for Kubernetes since then is owed to an ever larger cast.
Kubernetes was built on ideas that had been proven out at Google over the previous ten years with Borg. And Borg, itself, owed its existence to even earlier efforts at Google and beyond.
Concretely, Kubernetes started as some prototypes from Brendan Burns combined with ongoing work from me and Craig McLuckie to better align the internal Google experience with the Google Cloud experience. Brendan, Craig, and I really wanted people to use this, so we made the case to build out this prototype as an open source project that would bring the best ideas from Borg out into the open.
After we got the nod, it was time to actually build the system. We took Brendan’s prototype (in Java), rewrote it in Go, and built just enough to get the core ideas across. By this time the team had grown to include Ville Aikas, Tim Hockin, Brian Grant, Dawn Chen and Daniel Smith. Once we had something working, someone had to sign up to clean things up to get it ready for public launch. That ended up being me. Not knowing the significance at the time, I created a new repo, moved things over, and checked it in. So while I have the first public commit to the repo, there was work underway well before that.
The version of Kubernetes at that point was really just a shadow of what it was to become. The core concepts were there but it was very raw. For example, Pods were called Tasks. That was changed a day before we went public. All of this led up to the public announcement of Kubernetes on June 10th, 2014 in a keynote from Eric Brewer at the first DockerCon. You can watch that video here:
-->
&lt;p>第一次 commit 涉及的人员众多，自那以后 Kubernetes 的成功归功于更大的开发者阵容。
Kubernetes 建立在过去十年曾经在 Google 的 Borg 集群管理系统中验证过的思路之上。而 Borg 本身也是 Google 和其他公司早期努力的结果。
具体而言，Kubernetes 最初是从 Brendan Burns 的一些原型开始，结合我和 Craig McLuckie 正在进行的工作，以更好地将 Google 内部实践与 Google Cloud 的经验相结合。 Brendan，Craig 和我真的希望人们使用它，所以我们建议将这个原型构建为一个开源项目，将 Borg 的最佳创意带给大家。
在我们所有人同意后，就开始着手构建这个系统了。我们采用了 Brendan 的原型（Java 语言），用 Go 语言重写了它，并且以上述核心思想去构建该系统。到这个时候，团队已经成长为包括 Ville Aikas，Tim Hockin，Brian Grant，Dawn Chen 和 Daniel Smith。一旦我们有了一些工作需求，有人必须承担一些脱敏的工作，以便为公开发布做好准备。这个角色最终由我承担。当时，我不知道这件事情的重要性，我创建了一个新的仓库，把代码搬过来，然后进行了检查。所以在我第一次提交 public commit 之前，就有工作已经启动了。
那时 Kubernetes 的版本只是现在版本的简单雏形。核心概念已经有了，但非常原始。例如，Pods 被称为 Tasks，这在我们推广前一天就被替换。2014年6月10日 Eric Brewe 在第一届 DockerCon 上的演讲中正式发布了 Kubernetes 。您可以在此处观看该视频：&lt;/p>
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/YrxnVKZeqK8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
&lt;!--
But, however raw, that modest start was enough to pique the interest of a community that started strong and has only gotten stronger. Over the past four years Kubernetes has exceeded the expectations of all of us that were there early on. We owe the Kubernetes community a huge debt. The success the project has seen is based not just on code and technology but also the way that an amazing group of people have come together to create something special. The best expression of this is the [set of Kubernetes values](https://github.com/kubernetes/steering/blob/master/values.md) that Sarah Novotny helped curate.
Here is to another 4 years and beyond! 🎉🎉🎉
-->
&lt;p>但是，无论多么原始，这小小的一步足以激起一个开始强大而且变得更强大的社区的兴趣。在过去的四年里，Kubernetes 已经超出了我们所有人的期望。我们对 Kubernetes 社区的所有人员表示感谢。该项目所取得的成功不仅基于代码和技术，还基于一群出色的人聚集在一起所做的有意义的事情。Sarah Novotny 策划的一套 &lt;a href="https://github.com/kubernetes/steering/blob/master/values.md">Kubernetes 价值观&lt;/a>是以上最好的表现形式。
让我们一起期待下一个4年！🎉🎉🎉&lt;/p></description></item><item><title>Blog: Kubernetes 1 11：向 discuss kubernetes 问好</title><link>https://kubernetes.io/zh/blog/2018/05/30/kubernetes-1-11%E5%90%91-discuss-kubernetes-%E9%97%AE%E5%A5%BD/</link><pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/05/30/kubernetes-1-11%E5%90%91-discuss-kubernetes-%E9%97%AE%E5%A5%BD/</guid><description>
&lt;!--
---
title: ' Kubernetes 1 11：say-hello-to-discuss-kubernetes '
cn-approvers:
- congfairy
layout: blog
date: 2018-05-30
---
-->
&lt;!--
Author: Jorge Castro (Heptio)
-->
&lt;p>作者: Jorge Castro (Heptio)&lt;/p>
&lt;!--
Communication is key when it comes to engaging a community of over 35,000 people in a global and remote environment. Keeping track of everything in the Kubernetes community can be an overwhelming task. On one hand we have our official resources, like Stack Overflow, GitHub, and the mailing lists, and on the other we have more ephemeral resources like Slack, where you can hop in, chat with someone, and then go on your merry way.
-->
&lt;p>就一个超过 35,000 人的全球性社区而言，参与其中时沟通是非常关键的。 跟踪 Kubernetes 社区中的所有内容可能是一项艰巨的任务。 一方面，我们有官方资源，如 Stack Overflow，GitHub 和邮件列表，另一方面，我们有更多瞬时性的资源，如 Slack，你可以加入进去、与某人聊天然后各走各路。&lt;/p>
&lt;!--
Slack is great for casual and timely conversations and keeping up with other community members, but communication can't be easily referenced in the future. Plus it can be hard to raise your hand in a room filled with 35,000 participants and find a voice. Mailing lists are useful when trying to reach a specific group of people with a particular ask and want to keep track of responses on the thread, but can be daunting with a large amount of people. Stack Overflow and GitHub are ideal for collaborating on projects or questions that involve code and need to be searchable in the future, but certain topics like "What's your favorite CI/CD tool" or "Kubectl tips and tricks" are offtopic there.
While our current assortment of communication channels are valuable in their own rights, we found that there was still a gap between email and real time chat. Across the rest of the web, many other open source projects like Docker, Mozilla, Swift, Ghost, and Chef have had success building communities on top of Discourse, an open source discussion platform. So what if we could use this tool to bring our discussions together under a modern roof, with an open API, and perhaps not let so much of our information fade into the ether? There's only one way to find out: Welcome to discuss.kubernetes.io
-->
&lt;p>Slack 非常适合随意和及时的对话，并与其他社区成员保持联系，但未来很难轻易引用通信。此外，在35,000名参与者中提问并得到回答很难。邮件列表在有问题尝试联系特定人群并且想要跟踪大家的回应时非常有用，但是对于大量人员来说可能是麻烦的。 Stack Overflow 和 GitHub 非常适合在涉及代码的项目或问题上进行协作，并且如果在将来要进行搜索也很有用，但某些主题如“你最喜欢的 CI/CD 工具是什么”或“&lt;a href="http://discuss.kubernetes.io/t/kubectl-tips-and-tricks/192">Kubectl提示和技巧&lt;/a>“在那里是没有意义的。&lt;/p>
&lt;p>虽然我们目前的各种沟通渠道对他们自己来说都很有价值，但我们发现电子邮件和实时聊天之间仍然存在差距。在网络的其他部分，许多其他开源项目，如 Docker、Mozilla、Swift、Ghost 和 Chef，已经成功地在&lt;a href="http://www.discourse.org/features">Discourse&lt;/a>之上构建社区，一个开放的讨论平台。那么，如果我们可以使用这个工具将我们的讨论结合在一个平台下，使用开放的API，或许也不会让我们的大部分信息消失在网络中呢？只有一种方法可以找到：欢迎来到&lt;a href="http://discuss.kubernetes.io">discuss.kubernetes.io&lt;/a>&lt;/p>
&lt;!--
Right off the bat we have categories that users can browse. Checking and posting in these categories allow users to participate in things they might be interested in without having to commit to subscribing to a list. Granular notification controls allow the users to subscribe to just the category or tag they want, and allow for responding to topics via email.
Ecosystem partners and developers now have a place where they can [announce projects](https://discuss.kubernetes.io/c/announcements) that they're working on to users without wondering if it would be offtopic on an official list. We can make this place be not just about core Kubernetes, but about the hundreds of wonderful tools our community is building.
This new community forum gives people a place to go where they can discuss Kubernetes, and a sounding board for developers to make announcements of things happening around Kubernetes, all while being searchable and easily accessible to a wider audience.
Hop in and take a look. We're just getting started, so you might want to begin by [introducing yourself](https://discuss.kubernetes.io/t/introduce-yourself-here/56) and then browsing around. Apps are also available for [Android](https://play.google.com/store/apps/details?id=com.discourse&amp;hl=en_US&amp;rdid=com.discourse&amp;pli=1)and [iOS](https://itunes.apple.com/us/app/discourse-app/id1173672076?mt=8).
-->
&lt;p>马上，我们有用户可以浏览的类别。检查和发布这些类别允许用户参与他们可能感兴趣的事情，而无需订阅列表。精细的通知控件允许用户只订阅他们想要的类别或标签，并允许通过电子邮件回复主题。&lt;/p>
&lt;p>生态系统合作伙伴和开发人员现在有一个地方可以&lt;a href="http://discuss.kubernetes.io/c/announcements">宣布项目&lt;/a>，他们正在为用户工作，而不会想知道它是否会在官方列表中脱离主题。我们可以让这个地方不仅仅是关于核心 Kubernetes，而是关于我们社区正在建设的数百个精彩工具。&lt;/p>
&lt;p>这个新的社区论坛为人们提供了一个可以讨论 Kubernetes 的地方，也是开发人员在 Kubernetes 周围发布事件的声音板，同时可以搜索并且更容易被更广泛的用户访问。&lt;/p>
&lt;p>进来看看。我们刚刚开始，所以，您可能希望从&lt;a href="http://discuss.kubernetes.io/t/introduce-yourself-here/56">自我介绍&lt;/a>开始，到处浏览。也有 &lt;a href="http://play.google.com/store/apps/details?id=com.discourse&amp;amp;hl=en_US&amp;amp;rdid=com.discourse&amp;amp;pli=1">Android&lt;/a> 和 &lt;a href="http://itunes.apple.com/us/app/discourse-app/id1173672076?mt=8">iOS&lt;/a> 应用下载。&lt;/p></description></item><item><title>Blog: 在 Kubernetes 上开发</title><link>https://kubernetes.io/zh/blog/2018/05/01/developing-on-kubernetes/</link><pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/05/01/developing-on-kubernetes/</guid><description>
&lt;!--
---
title: Developing on Kubernetes
date: 2018-05-01
slug: developing-on-kubernetes
---
-->
&lt;!--**Authors**:-->
&lt;p>&lt;strong>作者&lt;/strong>： &lt;a href="https://twitter.com/mhausenblas">Michael Hausenblas&lt;/a> (Red Hat), &lt;a href="https://twitter.com/errordeveloper">Ilya Dmitrichenko&lt;/a> (Weaveworks)&lt;/p>
&lt;!--
How do you develop a Kubernetes app? That is, how do you write and test an app that is supposed to run on Kubernetes? This article focuses on the challenges, tools and methods you might want to be aware of to successfully write Kubernetes apps alone or in a team setting.
-->
&lt;p>您将如何开发一个 Kubernates 应用？也就是说，您如何编写并测试一个要在 Kubernates 上运行的应用程序？本文将重点介绍在独自开发或者团队协作中，您可能希望了解到的为了成功编写 Kubernetes 应用程序而需面临的挑战，工具和方法。&lt;/p>
&lt;!--
We’re assuming you are a developer, you have a favorite programming language, editor/IDE, and a testing framework available. The overarching goal is to introduce minimal changes to your current workflow when developing the app for Kubernetes. For example, if you’re a Node.js developer and are used to a hot-reload setup—that is, on save in your editor the running app gets automagically updated—then dealing with containers and container images, with container registries, Kubernetes deployments, triggers, and more can not only be overwhelming but really take all the fun out if it.
-->
&lt;p>我们假定您是一位开发人员，有您钟爱的编程语言，编辑器/IDE（集成开发环境），以及可用的测试框架。在针对 Kubernates 开发应用时，最重要的目标是减少对当前工作流程的影响，改变越少越好，尽量做到最小。举个例子，如果您是 Node.js 开发人员，习惯于那种热重载的环境 - 也就是说您在编辑器里一做保存，正在运行的程序就会自动更新 - 那么跟容器、容器镜像或者镜像仓库打交道，又或是跟 Kubernetes 部署、triggers 以及更多头疼东西打交道，不仅会让人难以招架也真的会让开发过程完全失去乐趣。&lt;/p>
&lt;!--
In the following, we’ll first discuss the overall development setup, then review tools of the trade, and last but not least do a hands-on walkthrough of three exemplary tools that allow for iterative, local app development against Kubernetes.
-->
&lt;p>在下文中，我们将首先讨论 Kubernetes 总体开发环境，然后回顾常用工具，最后进行三个示例性工具的实践演练。这些工具允许针对 Kubernetes 进行本地应用程序的开发和迭代。&lt;/p>
&lt;!--
## Where to run your cluster?
-->
&lt;h2 id="您的集群运行在哪里">您的集群运行在哪里？&lt;/h2>
&lt;!--
As a developer you want to think about where the Kubernetes cluster you’re developing against runs as well as where the development environment sits. Conceptually there are four development modes:
-->
&lt;p>作为开发人员，您既需要考虑所针对开发的 Kubernetes 集群运行在哪里，也需要思考开发环境如何配置。概念上，有四种开发模式：&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-devmodes_preview.png" alt="Dev Modes">&lt;/p>
&lt;!--
A number of tools support pure offline development including Minikube, Docker for Mac/Windows, Minishift, and the ones we discuss in detail below. Sometimes, for example, in a microservices setup where certain microservices already run in the cluster, a proxied setup (forwarding traffic into and from the cluster) is preferable and Telepresence is an example tool in this category. The live mode essentially means you’re building and/or deploying against a remote cluster and, finally, the pure online mode means both your development environment and the cluster are remote, as this is the case with, for example, [Eclipse Che](https://www.eclipse.org/che/docs/kubernetes-single-user.html) or [Cloud 9](https://github.com/errordeveloper/k9c). Let’s now have a closer look at the basics of offline development: running Kubernetes locally.
-->
&lt;p>许多工具支持纯 offline 开发，包括 Minikube、Docker（Mac 版/Windows 版）、Minishift 以及下文中我们将详细讨论的几种。有时，比如说在一个微服务系统中，已经有若干微服务在运行，proxied 模式（通过转发把数据流传进传出集群）就非常合适，Telepresence 就是此类工具的一个实例。live 模式，本质上是您基于一个远程集群进行构建和部署。最后，纯 online 模式意味着您的开发环境和运行集群都是远程的，典型的例子是 &lt;a href="https://www.eclipse.org/che/docs/kubernetes-single-user.html">Eclipse Che&lt;/a> 或者 &lt;a href="https://github.com/errordeveloper/k9c">Cloud 9&lt;/a>。现在让我们仔细看看离线开发的基础：在本地运行 Kubernetes。&lt;/p>
&lt;!--
[Minikube](/docs/getting-started-guides/minikube/) is a popular choice for those who prefer to run Kubernetes in a local VM. More recently Docker for [Mac](https://docs.docker.com/docker-for-mac/kubernetes/) and [Windows](https://docs.docker.com/docker-for-windows/kubernetes/) started shipping Kubernetes as an experimental package (in the “edge” channel). Some reasons why you may want to prefer using Minikube over the Docker desktop option are:
-->
&lt;p>&lt;a href="https://kubernetes.io/docs/getting-started-guides/minikube/">Minikube&lt;/a> 在更加喜欢于本地 VM 上运行 Kubernetes 的开发人员中，非常受欢迎。不久前，Docker 的 &lt;a href="https://docs.docker.com/docker-for-mac/kubernetes/">Mac&lt;/a> 版和 &lt;a href="https://docs.docker.com/docker-for-windows/kubernetes/">Windows&lt;/a> 版，都试验性地开始自带 Kubernetes（需要下载 “edge” 安装包）。在两者之间，以下原因也许会促使您选择 Minikube 而不是 Docker 桌面版：&lt;/p>
&lt;!--
* You already have Minikube installed and running
* You prefer to wait until Docker ships a stable package
* You’re a Linux desktop user
* You are a Windows user who doesn’t have Windows 10 Pro with Hyper-V
-->
&lt;ul>
&lt;li>您已经安装了 Minikube 并且它运行良好&lt;/li>
&lt;li>您想等到 Docker 出稳定版本&lt;/li>
&lt;li>您是 Linux 桌面用户&lt;/li>
&lt;li>您是 Windows 用户，但是没有配有 Hyper-V 的 Windows 10 Pro&lt;/li>
&lt;/ul>
&lt;!--
Running a local cluster allows folks to work offline and that you don’t have to pay for using cloud resources. Cloud provider costs are often rather affordable and free tiers exists, however some folks prefer to avoid having to approve those costs with their manager as well as potentially incur unexpected costs, for example, when leaving cluster running over the weekend.
-->
&lt;p>运行一个本地集群，开发人员可以离线工作，不用支付云服务。云服务收费一般不会太高，并且免费的等级也有，但是一些开发人员不喜欢为了使用云服务而必须得到经理的批准，也不愿意支付意想不到的费用，比如说忘了下线而集群在周末也在运转。&lt;/p>
&lt;!--
Some developers prefer to use a remote Kubernetes cluster, and this is usually to allow for larger compute and storage capacity and also enable collaborative workflows more easily. This means it’s easier for you to pull in a colleague to help with debugging or share access to an app in the team. Additionally, for some developers it can be critical to mirror production environment as closely as possible, especially when it comes down to external cloud services, say, proprietary databases, object stores, message queues, external load balancer, or mail delivery systems.
-->
&lt;p>有些开发人员却更喜欢远程的 Kubernetes 集群，这样他们通常可以获得更大的计算能力和存储容量，也简化了协同工作流程。您可以更容易的拉上一个同事来帮您调试，或者在团队内共享一个应用的使用。再者，对某些开发人员来说，尽可能的让开发环境类似生产环境至关重要，尤其是您依赖外部厂商的云服务时，如：专有数据库、云对象存储、消息队列、外商的负载均衡器或者邮件投递系统。&lt;/p>
&lt;!--
In summary, there are good reasons for you to develop against a local cluster as well as a remote one. It very much depends on in which phase you are: from early prototyping and/or developing alone to integrating a set of more stable microservices.
-->
&lt;p>总之，无论您选择本地或者远程集群，理由都足够多。这很大程度上取决于您所处的阶段：从早期的原型设计/单人开发到后期面对一批稳定微服务的集成。&lt;/p>
&lt;!--
Now that you have a basic idea of the options around the runtime environment, let’s move on to how to iteratively develop and deploy your app.
-->
&lt;p>既然您已经了解到运行环境的基本选项，那么我们就接着讨论如何迭代式的开发并部署您的应用。&lt;/p>
&lt;!--
## The tools of the trade
-->
&lt;h2 id="常用工具">常用工具&lt;/h2>
&lt;!--
We are now going to review tooling allowing you to develop apps on Kubernetes with the focus on having minimal impact on your existing workflow. We strive to provide an unbiased description including implications of using each of the tools in general terms.
-->
&lt;p>我们现在回顾既可以允许您可以在 Kubernetes 上开发应用程序又尽可能最小地改变您现有的工作流程的一些工具。我们致力于提供一份不偏不倚的描述，也会提及使用某个工具将会意味着什么。&lt;/p>
&lt;!--
Note that this is a tricky area since even for established technologies such as, for example, JSON vs YAML vs XML or REST vs gRPC vs SOAP a lot depends on your background, your preferences and organizational settings. It’s even harder to compare tooling in the Kubernetes ecosystem as things evolve very rapidly and new tools are announced almost on a weekly basis; during the preparation of this post alone, for example, [Gitkube](https://gitkube.sh/) and [Watchpod](https://github.com/MinikubeAddon/watchpod) came out. To cover these new tools as well as related, existing tooling such as [Weave Flux](https://github.com/weaveworks/flux) and OpenShift’s [S2I](https://docs.openshift.com/container-platform/3.9/creating_images/s2i.html) we are planning a follow-up blog post to the one you’re reading.
-->
&lt;p>请注意这很棘手，因为即使在成熟定型的技术中做选择，比如说在 JSON、YAML、XML、REST、gRPC 或者 SOAP 之间做选择，很大程度也取决于您的背景、喜好以及公司环境。在 Kubernetes 生态系统内比较各种工具就更加困难，因为技术发展太快，几乎每周都有新工具面市；举个例子，仅在准备这篇博客的期间，&lt;a href="https://gitkube.sh/">Gitkube&lt;/a> 和 &lt;a href="https://github.com/MinikubeAddon/watchpod">Watchpod&lt;/a> 相继出品。为了进一步覆盖到这些新的，以及一些相关的已推出的工具，例如 &lt;a href="https://github.com/weaveworks/flux">Weave Flux&lt;/a> 和 OpenShift 的 &lt;a href="https://docs.openshift.com/container-platform/3.9/creating_images/s2i.html">S2I&lt;/a>，我们计划再写一篇跟进的博客。&lt;/p>
&lt;h3 id="draft">Draft&lt;/h3>
&lt;!--
[Draft](https://github.com/Azure/draft) aims to help you get started deploying any app to Kubernetes. It is capable of applying heuristics as to what programming language your app is written in and generates a Dockerfile along with a Helm chart. It then runs the build for you and deploys resulting image to the target cluster via the Helm chart. It also allows user to setup port forwarding to localhost very easily.
-->
&lt;p>&lt;a href="https://github.com/Azure/draft">Draft&lt;/a> 旨在帮助您将任何应用程序部署到 Kubernetes。它能够检测到您的应用所使用的编程语言，并且生成一份 Dockerfile 和 Helm 图表。然后它替您启动构建并且依照 Helm 图表把所生产的镜像部署到目标集群。它也可以让您很容易地设置到 localhost 的端口映射。&lt;/p>
&lt;!--
Implications:
-->
&lt;p>这意味着：&lt;/p>
&lt;!--
* User can customise the chart and Dockerfile templates however they like, or even create a [custom pack](https://github.com/Azure/draft/blob/master/docs/reference/dep-003.md) (with Dockerfile, the chart and more) for future use
-->
&lt;ul>
&lt;li>用户可以任意地自定义 Helm 图表和 Dockerfile 模版，或者甚至创建一个 &lt;a href="https://github.com/Azure/draft/blob/master/docs/reference/dep-003.md">custom pack&lt;/a>（使用 Dockerfile、Helm 图表以及其他）以备后用&lt;/li>
&lt;/ul>
&lt;!--
* It’s not very simple to guess how just any app is supposed to be built, in some cases user may need to tweak Dockerfile and the Helm chart that Draft generates
-->
&lt;ul>
&lt;li>要想理解一个应用应该怎么构建并不容易，在某些情况下，用户也许需要修改 Draft 生成的 Dockerfile 和 Heml 图表&lt;/li>
&lt;/ul>
&lt;!--
* With [Draft version 0.12.0](https://github.com/Azure/draft/releases/tag/v0.12.0) or older, every time user wants to test a change, they need to wait for Draft to copy the code to the cluster, then run the build, push the image and release updated chart; this can timely, but it results in an image being for every single change made by the user (whether it was committed to git or not)
-->
&lt;ul>
&lt;li>如果使用 &lt;a href="https://github.com/Azure/draft/releases/tag/v0.12.0">Draft version 0.12.0&lt;/a>&lt;sup>1&lt;/sup> 或者更老版本，每一次用户想要测试一个改动，他们需要等 Draft 把代码拷贝到集群，运行构建，推送镜像并且发布更新后的图表；这些步骤可能进行得很快，但是每一次用户的改动都会产生一个镜像（无论是否提交到 git ）&lt;/li>
&lt;/ul>
&lt;!--
* As of Draft version 0.12.0, builds are executed locally
* User doesn’t have an option to choose something other than Helm for deployment
* It can watch local changes and trigger deployments, but this feature is not enabled by default
-->
&lt;ul>
&lt;li>在 Draft 0.12.0版本，构建是本地进行的&lt;/li>
&lt;li>用户不能选择 Helm 以外的工具进行部署&lt;/li>
&lt;li>它可以监控本地的改动并且触发部署，但是这个功能默认是关闭的&lt;/li>
&lt;/ul>
&lt;!--
* It allows developer to use either local or remote Kubernetes cluster
* Deploying to production is up to the user, Draft authors recommend their other project – Brigade
* Can be used instead of Skaffold, and along the side of Squash
-->
&lt;ul>
&lt;li>它允许开发人员使用本地或者远程的 Kubernates 集群&lt;/li>
&lt;li>如何部署到生产环境取决于用户， Draft 的作者推荐了他们的另一个项目 - Brigade&lt;/li>
&lt;li>可以代替 Skaffold， 并且可以和 Squash 一起使用&lt;/li>
&lt;/ul>
&lt;!--
More info:
-->
&lt;p>更多信息：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/blog/2017/05/draft-kubernetes-container-development">Draft: Kubernetes container development made easy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Azure/draft/blob/master/docs/getting-started.md">Getting Started Guide&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>【1】：此处疑为 0.11.0，因为 0.12.0 已经支持本地构建，见下一条&lt;/p>
&lt;h3 id="skaffold">Skaffold&lt;/h3>
&lt;!--
[Skaffold](https://github.com/GoogleCloudPlatform/skaffold) is a tool that aims to provide portability for CI integrations with different build system, image registry and deployment tools. It is different from Draft, yet somewhat comparable. It has a basic capability for generating manifests, but it’s not a prominent feature. Skaffold is extendible and lets user pick tools for use in each of the steps in building and deploying their app.
-->
&lt;p>&lt;a href="https://github.com/GoogleCloudPlatform/skaffold">Skaffold&lt;/a> 让 CI 集成具有可移植性的，它允许用户采用不同的构建系统，镜像仓库和部署工具。它不同于 Draft，同时也具有一定的可比性。它具有生成系统清单的基本能力，但那不是一个重要功能。Skaffold 易于扩展，允许用户在构建和部署应用的每一步选取相应的工具。&lt;/p>
&lt;!--
Implications:
-->
&lt;p>这意味着：&lt;/p>
&lt;!--
* Modular by design
* Works independently of CI vendor, user doesn’t need Docker or Kubernetes plugin
* Works without CI as such, i.e. from the developer’s laptop
* It can watch local changes and trigger deployments
-->
&lt;ul>
&lt;li>模块化设计&lt;/li>
&lt;li>不依赖于 CI，用户不需要 Docker 或者 Kubernetes 插件&lt;/li>
&lt;li>没有 CI 也可以工作，也就是说，可以在开发人员的电脑上工作&lt;/li>
&lt;li>它可以监控本地的改动并且触发部署&lt;/li>
&lt;/ul>
&lt;!--
* It allows developer to use either local or remote Kubernetes cluster
* It can be used to deploy to production, user can configure how exactly they prefer to do it and provide different kind of pipeline for each target environment
* Can be used instead of Draft, and along the side with most other tools
-->
&lt;ul>
&lt;li>它允许开发人员使用本地或者远程的 Kubernetes 集群&lt;/li>
&lt;li>它可以用于部署生产环境，用户可以精确配置，也可以为每一套目标环境提供不同的生产线&lt;/li>
&lt;li>可以代替 Draft，并且和其他工具一起使用&lt;/li>
&lt;/ul>
&lt;!--
More info:
-->
&lt;p>更多信息：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://cloudplatform.googleblog.com/2018/03/introducing-Skaffold-Easy-and-repeatable-Kubernetes-development.html">Introducing Skaffold: Easy and repeatable Kubernetes development&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/GoogleCloudPlatform/skaffold#getting-started-with-local-tooling">Getting Started Guide&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="squash">Squash&lt;/h3>
&lt;!--
[Squash](https://github.com/solo-io/squash) consists of a debug server that is fully integrated with Kubernetes, and a IDE plugin. It allows you to insert breakpoints and do all the fun stuff you are used to doing when debugging an application using an IDE. It bridges IDE debugging experience with your Kubernetes cluster by allowing you to attach the debugger to a pod running in your Kubernetes cluster.
-->
&lt;p>&lt;a href="https://github.com/solo-io/squash">Squash&lt;/a> 包含一个与 Kubernetes 全面集成的调试服务器，以及一个 IDE 插件。它允许您插入断点和所有的调试操作，就像您所习惯的使用 IDE 调试一个程序一般。它允许您将调试器应用到 Kubernetes 集群中运行的 pod 上，从而让您可以使用 IDE 调试 Kubernetes 集群。&lt;/p>
&lt;!--
Implications:
-->
&lt;p>这意味着：&lt;/p>
&lt;!--
* Can be used independently of other tools you chose
* Requires a privileged DaemonSet
* Integrates with popular IDEs
* Supports Go, Python, Node.js, Java and gdb
-->
&lt;ul>
&lt;li>不依赖您选择的其它工具&lt;/li>
&lt;li>需要一组特权 DaemonSet&lt;/li>
&lt;li>可以和流行 IDE 集成&lt;/li>
&lt;li>支持 Go、Python、Node.js、Java 和 gdb&lt;/li>
&lt;/ul>
&lt;!--
* User must ensure application binaries inside the container image are compiled with debug symbols
* Can be used in combination with any other tools described here
* It can be used with either local or remote Kubernetes cluster
-->
&lt;ul>
&lt;li>用户必须确保容器中的应用程序使编译时使用了调试符号&lt;/li>
&lt;li>可与此处描述的任何其他工具结合使用&lt;/li>
&lt;li>它可以与本地或远程 Kubernetes 集群一起使用&lt;/li>
&lt;/ul>
&lt;!--
More info:
-->
&lt;p>更多信息：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=5TrV3qzXlgI">Squash: A Debugger for Kubernetes Apps&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/solo-io/squash/blob/master/docs/getting-started.md">Getting Started Guide&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="telepresence">Telepresence&lt;/h3>
&lt;!--
[Telepresence](https://www.telepresence.io/) connects containers running on developer’s workstation with a remote Kubernetes cluster using a two-way proxy and emulates in-cluster environment as well as provides access to config maps and secrets. It aims to improve iteration time for container app development by eliminating the need for deploying app to the cluster and leverages local container to abstract network and filesystem interface in order to make it appear as if the app was running in the cluster.
-->
&lt;p>&lt;a href="https://www.telepresence.io/">Telepresence&lt;/a> 使用双向代理将开发人员工作站上运行的容器与远程 Kubernetes 集群连接起来，并模拟集群内环境以及提供对配置映射和机密的访问。它消除了将应用部署到集群的需要，并利用本地容器抽象出网络和文件系统接口，以使其看起来应用好像就在集群中运行，从而改进容器应用程序开发的迭代时间。&lt;/p>
&lt;!--
Implications:
-->
&lt;p>这意味着：&lt;/p>
&lt;!--
* It can be used independently of other tools you chose
* Using together with Squash is possible, although Squash would have to be used for pods in the cluster, while conventional/local debugger would need to be used for debugging local container that’s connected to the cluster via Telepresence
* Telepresence imposes some network latency
-->
&lt;ul>
&lt;li>它不依赖于其它您选取的工具&lt;/li>
&lt;li>可以同 Squash 一起使用，但是 Squash 必须用于调试集群中的 pods，而传统/本地调试器需要用于调试通过 Telepresence 连接到集群的本地容器&lt;/li>
&lt;li>Telepresence 会产生一些网络延迟&lt;/li>
&lt;/ul>
&lt;!--
* It provides connectivity via a side-car process - sshuttle, which is based on SSH
* More intrusive dependency injection mode with LD_PRELOAD/DYLD_INSERT_LIBRARIES is also available
* It is most commonly used with a remote Kubernetes cluster, but can be used with a local one also
-->
&lt;ul>
&lt;li>它通过辅助进程提供连接 - sshuttle，基于SSH的一个工具&lt;/li>
&lt;li>还提供了使用 LD_PRELOAD/DYLD_INSERT_LIBRARIES 的更具侵入性的依赖注入模式&lt;/li>
&lt;li>它最常用于远程 Kubernetes 集群，但也可以与本地集群一起使用&lt;/li>
&lt;/ul>
&lt;!--
More info:
-->
&lt;p>更多信息：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.telepresence.io/">Telepresence: fast, realistic local development for Kubernetes microservices&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.telepresence.io/tutorials/docker">Getting Started Guide&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.telepresence.io/discussion/how-it-works">How It Works&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="ksync">Ksync&lt;/h3>
&lt;!--
[Ksync](https://github.com/vapor-ware/ksync) synchronizes application code (and configuration) between your local machine and the container running in Kubernetes, akin to what [oc rsync](https://docs.openshift.com/container-platform/3.9/dev_guide/copy_files_to_container.html) does in OpenShift. It aims to improve iteration time for app development by eliminating build and deployment steps.
-->
&lt;p>&lt;a href="https://github.com/vapor-ware/ksync">Ksync&lt;/a> 在本地计算机和运行在 Kubernetes 中的容器之间同步应用程序代码（和配置），类似于 &lt;a href="https://docs.openshift.com/container-platform/3.9/dev_guide/copy_files_to_container.html">oc rsync&lt;/a> 在 OpenShift 中的角色。它旨在通过消除构建和部署步骤来缩短应用程序开发的迭代时间。&lt;/p>
&lt;!--
Implications:
-->
&lt;p>这意味着：&lt;/p>
&lt;!--
* It bypasses container image build and revision control
* Compiled language users have to run builds inside the pod (TBC)
* Two-way sync – remote files are copied to local directory
* Container is restarted each time remote filesystem is updated
* No security features – development only
-->
&lt;ul>
&lt;li>它绕过容器图像构建和修订控制&lt;/li>
&lt;li>使用编译语言的用户必须在 pod（TBC）内运行构建&lt;/li>
&lt;li>双向同步 - 远程文件会复制到本地目录&lt;/li>
&lt;li>每次更新远程文件系统时都会重启容器&lt;/li>
&lt;li>无安全功能 - 仅限开发&lt;/li>
&lt;/ul>
&lt;!--
* Utilizes [Syncthing](https://github.com/syncthing/syncthing), a Go library for peer-to-peer sync
* Requires a privileged DaemonSet running in the cluster
* Node has to use Docker with overlayfs2 – no other CRI implementations are supported at the time of writing
-->
&lt;ul>
&lt;li>使用 &lt;a href="https://github.com/syncthing/syncthing">Syncthing&lt;/a>，一个用于点对点同步的 Go 语言库&lt;/li>
&lt;li>需要一个在集群中运行的特权 DaemonSet&lt;/li>
&lt;li>Node 必须使用带有 overlayfs2 的 Docker - 在写作本文时，尚不支持其他 CRI 实现&lt;/li>
&lt;/ul>
&lt;!--
More info:
-->
&lt;p>更多信息：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/vapor-ware/ksync#getting-started">Getting Started Guide&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/vapor-ware/ksync/blob/master/docs/architecture.md">How It Works&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.katacoda.com/vaporio/scenarios/ksync">Katacoda scenario to try out ksync in your browser&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.syncthing.net/specs/">Syncthing Specification&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Hands-on walkthroughs
-->
&lt;h2 id="实践演练">实践演练&lt;/h2>
&lt;!--
The app we will be using for the hands-on walkthroughs of the tools in the following is a simple [stock market simulator](https://github.com/kubernauts/dok-example-us), consisting of two microservices:
-->
&lt;p>我们接下来用于练习使用工具的应用是一个简单的&lt;a href="https://github.com/kubernauts/dok-example-us">股市模拟器&lt;/a>，包含两个微服务：&lt;/p>
&lt;!--
* The `stock-gen` microservice is written in Go and generates stock data randomly and exposes it via HTTP endpoint `/stockdata`.
‎* A second microservice, `stock-con` is a Node.js app that consumes the stream of stock data from `stock-gen` and provides an aggregation in form of a moving average via the HTTP endpoint `/average/$SYMBOL` as well as a health-check endpoint at `/healthz`.
-->
&lt;ul>
&lt;li>&lt;code>stock-gen&lt;/code>（股市数据生成器）微服务是用 Go 编写的，随机生成股票数据并通过 HTTP 端点 &lt;code>/ stockdata&lt;/code> 公开&lt;/li>
&lt;li>第二个微服务，&lt;code>stock-con&lt;/code>（股市数据消费者）是一个 Node.js 应用程序，它使用来自 &lt;code>stock-gen&lt;/code> 的股票数据流，并通过 HTTP 端点 &lt;code>/average/$SYMBOL&lt;/code> 提供股价移动平均线，也提供一个健康检查端点 &lt;code>/healthz&lt;/code>。&lt;/li>
&lt;/ul>
&lt;!--
Overall, the default setup of the app looks as follows:
-->
&lt;p>总体上，此应用的默认配置如下图所示：&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-architecture_preview.png" alt="Default Setup">&lt;/p>
&lt;!--
In the following we’ll do a hands-on walkthrough for a representative selection of tools discussed above: ksync, Minikube with local build, as well as Skaffold. For each of the tools we do the following:
-->
&lt;p>在下文中，我们将选取以上讨论的代表性工具进行实践演练：ksync，具有本地构建的 Minikube 以及 Skaffold。对于每个工具，我们执行以下操作：&lt;/p>
&lt;!--
* Set up the respective tool incl. preparations for the deployment and local consumption of the `stock-con` microservice.
* Perform a code update, that is, change the source code of the `/healthz` endpoint in the `stock-con` microservice and observe the updates.
-->
&lt;ul>
&lt;li>设置相应的工具，包括部署准备和 &lt;code>stock-con&lt;/code> 微服务数据的本地读取&lt;/li>
&lt;li>执行代码更新，即更改 &lt;code>stock-con&lt;/code> 微服务的 &lt;code>/healthz&lt;/code> 端点的源代码并观察网页刷新&lt;/li>
&lt;/ul>
&lt;!--
Note that for the target Kubernetes cluster we’ve been using Minikube locally, but you can also a remote cluster for ksync and Skaffold if you want to follow along.
-->
&lt;p>请注意，我们一直使用 Minikube 的本地 Kubernetes 集群，但是您也可以使用 ksync 和 Skaffold 的远程集群跟随练习。&lt;/p>
&lt;!--
### Walkthrough: ksync
-->
&lt;h3 id="实践演练-ksync">实践演练：ksync&lt;/h3>
&lt;!--
As a preparation, install [ksync](https://vapor-ware.github.io/ksync/#installation) and then carry out the following steps to prepare the development setup:
-->
&lt;p>作为准备，安装 &lt;a href="https://vapor-ware.github.io/ksync/#installation">ksync&lt;/a>，然后执行以下步骤配置开发环境：&lt;/p>
&lt;pre>&lt;code>$ mkdir -p $(pwd)/ksync
$ kubectl create namespace dok
$ ksync init -n dok
&lt;/code>&lt;/pre>&lt;!--
With the basic setup completed we're ready to tell ksync’s local client to watch a certain Kubernetes namespace and then we create a spec to define what we want to sync (the directory `$(pwd)/ksync` locally with `/app` in the container). Note that target pod is specified via the selector parameter:
-->
&lt;p>完成基本设置后，我们可以告诉 ksync 的本地客户端监控 Kubernetes 的某个命名空间，然后我们创建一个规范来定义我们想要同步的文件夹（本地的 &lt;code>$(pwd)/ksync&lt;/code> 和容器中的 &lt;code>/ app&lt;/code> ）。请注意，目标 pod 是用 selector 参数指定：&lt;/p>
&lt;pre>&lt;code>$ ksync watch -n dok
$ ksync create -n dok --selector=app=stock-con $(pwd)/ksync /app
$ ksync get -n dok
&lt;/code>&lt;/pre>&lt;!--
Now we deploy the stock generator and the stock consumer microservice:
-->
&lt;p>现在我们部署股价数据生成器和股价数据消费者微服务：&lt;/p>
&lt;pre>&lt;code>$ kubectl -n=dok apply \
-f https://raw.githubusercontent.com/kubernauts/dok-example-us/master/stock-gen/app.yaml
$ kubectl -n=dok apply \
-f https://raw.githubusercontent.com/kubernauts/dok-example-us/master/stock-con/app.yaml
&lt;/code>&lt;/pre>&lt;!--
Once both deployments are created and the pods are running, we forward the `stock-con` service for local consumption (in a separate terminal session):
-->
&lt;p>一旦两个部署建好并且 pod 开始运行，我们转发 &lt;code>stock-con&lt;/code> 服务以供本地读取（另开一个终端窗口）：&lt;/p>
&lt;pre>&lt;code>$ kubectl get -n dok po --selector=app=stock-con \
-o=custom-columns=:metadata.name --no-headers | \
xargs -IPOD kubectl -n dok port-forward POD 9898:9898
&lt;/code>&lt;/pre>&lt;!--
With that we should be able to consume the `stock-con` service from our local machine; we do this by regularly checking the response of the `healthz` endpoint like so (in a separate terminal session):
-->
&lt;p>这样，通过定期查询 &lt;code>healthz&lt;/code> 端点，我们就应该能够从本地机器上读取 &lt;code>stock-con&lt;/code> 服务，查询命令如下（在一个单独的终端窗口）：&lt;/p>
&lt;pre>&lt;code>$ watch curl localhost:9898/healthz
&lt;/code>&lt;/pre>&lt;!--
Now change the code in the `ksync/stock-con`directory, for example update the [`/healthz` endpoint code in `service.js`](https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52) by adding a field to the JSON response and observe how the pod gets updated and the response of the `curl localhost:9898/healthz` command changes. Overall you should have something like the following in the end:
-->
&lt;p>现在，改动 &lt;code>ksync/stock-con&lt;/code> 目录中的代码，例如改动 &lt;a href="https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52">&lt;code>service.js&lt;/code> 中定义的 &lt;code>/healthz&lt;/code> 端点代码&lt;/a>，在其 JSON 形式的响应中新添一个字段并观察 pod 如何更新以及 &lt;code>curl localhost：9898/healthz&lt;/code> 命令的输出发生何种变化。总的来说，您最后应该看到类似的内容：&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-ksync_preview.png" alt="Preview">&lt;/p>
&lt;!--
### Walkthrough: Minikube with local build
-->
&lt;h3 id="实践演练-带本地构建的-minikube">实践演练：带本地构建的 Minikube&lt;/h3>
&lt;!--
For the following you will need to have Minikube up and running and we will leverage the Minikube-internal Docker daemon for building images, locally. As a preparation, do the following
-->
&lt;p>对于以下内容，您需要启动并运行 Minikube，我们将利用 Minikube 自带的 Docker daemon 在本地构建镜像。作为准备，请执行以下操作&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/kubernauts/dok-example-us.git &amp;amp;&amp;amp; cd dok-example-us
$ eval $(minikube docker-env)
$ kubectl create namespace dok
&lt;/code>&lt;/pre>&lt;!--
Now we deploy the stock generator and the stock consumer microservice:
-->
&lt;p>现在我们部署股价数据生成器和股价数据消费者微服务：&lt;/p>
&lt;pre>&lt;code>$ kubectl -n=dok apply -f stock-gen/app.yaml
$ kubectl -n=dok apply -f stock-con/app.yaml
&lt;/code>&lt;/pre>&lt;!--
Once both deployments are created and the pods are running, we forward the `stock-con` service for local consumption (in a separate terminal session):
-->
&lt;p>一旦两个部署建好并且 pod 开始运行，我们转发 &lt;code>stock-con&lt;/code> 服务以供本地读取（另开一个终端窗口）：&lt;/p>
&lt;pre>&lt;code>$ kubectl get -n dok po --selector=app=stock-con \
-o=custom-columns=:metadata.name --no-headers | \
xargs -IPOD kubectl -n dok port-forward POD 9898:9898 &amp;amp;
$ watch curl localhost:9898/healthz
&lt;/code>&lt;/pre>&lt;!--
Now change the code in the `stock-con`directory, for example, update the [`/healthz` endpoint code in `service.js`](https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52) by adding a field to the JSON response. Once you’re done with your code update, the last step is to build a new container image and kick off a new deployment like shown below:
-->
&lt;p>现在，改一下 &lt;code>ksync/stock-con&lt;/code> 目录中的代码，例如修改 &lt;a href="https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52">&lt;code>service.js&lt;/code> 中定义的 &lt;code>/healthz&lt;/code> 端点代码&lt;/a>，在其 JSON 形式的响应中添加一个字段。在您更新完代码后，最后一步是构建新的容器镜像并启动新部署，如下所示：&lt;/p>
&lt;pre>&lt;code>$ docker build -t stock-con:dev -f Dockerfile .
$ kubectl -n dok set image deployment/stock-con *=stock-con:dev
&lt;/code>&lt;/pre>&lt;!--
Overall you should have something like the following in the end:
-->
&lt;p>总的来说，您最后应该看到类似的内容：&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-minikube-localdev_preview.png" alt="Local Preview">&lt;/p>
&lt;!--
### Walkthrough: Skaffold
-->
&lt;h3 id="实践演练-skaffold">实践演练：Skaffold&lt;/h3>
&lt;!--
To perform this walkthrough you first need to install [Skaffold](https://github.com/GoogleContainerTools/skaffold#installation). Once that is done, you can do the following steps to prepare the development setup:
-->
&lt;p>要进行此演练，首先需要安装 &lt;a href="https://github.com/GoogleContainerTools/skaffold#installation">Skaffold&lt;/a>。完成后，您可以执行以下步骤来配置开发环境：&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/kubernauts/dok-example-us.git &amp;amp;&amp;amp; cd dok-example-us
$ kubectl create namespace dok
&lt;/code>&lt;/pre>&lt;!--
Now we deploy the stock generator (but not the stock consumer microservice, that is done via Skaffold):
-->
&lt;p>现在我们部署股价数据生成器（但是暂不部署股价数据消费者，此服务将使用 Skaffold 完成）：&lt;/p>
&lt;pre>&lt;code>$ kubectl -n=dok apply -f stock-gen/app.yaml
&lt;/code>&lt;/pre>&lt;!--
Note that initially we experienced an authentication error when doing `skaffold dev` and needed to apply a fix as described in [Issue 322](https://github.com/GoogleContainerTools/skaffold/issues/322). Essentially it means changing the content of `~/.docker/config.json` to:
-->
&lt;p>请注意，最初我们在执行 &lt;code>skaffold dev&lt;/code> 时发生身份验证错误，为避免此错误需要安装&lt;a href="https://github.com/GoogleContainerTools/skaffold/issues/322">问题322&lt;/a> 中所述的修复。本质上，需要将 &lt;code>〜/.docker/config.json&lt;/code> 的内容改为：&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;auths&amp;quot;: {}
}
&lt;/code>&lt;/pre>&lt;!--
Next, we had to patch `stock-con/app.yaml` slightly to make it work with Skaffold:
-->
&lt;p>接下来，我们需要略微改动 &lt;code>stock-con/app.yaml&lt;/code>，这样 Skaffold 才能正常使用此文件：&lt;/p>
&lt;!--
Add a `namespace` field to both the `stock-con` deployment and the service with the value of `dok`.
Change the `image` field of the container spec to `quay.io/mhausenblas/stock-con` since Skaffold manages the container image tag on the fly.
-->
&lt;p>在 &lt;code>stock-con&lt;/code> 部署和服务中添加一个 &lt;code>namespace&lt;/code> 字段，其值为 &lt;code>dok&lt;/code>&lt;/p>
&lt;p>将容器规范的 &lt;code>image&lt;/code> 字段更改为 &lt;code>quay.io/mhausenblas/stock-con&lt;/code>，因为 Skaffold 可以即时管理容器镜像标签。&lt;/p>
&lt;!--
The resulting `app.yaml` file stock-con looks as follows:
-->
&lt;p>最终的 stock-con 的 &lt;code>app.yaml&lt;/code> 文件看起来如下：&lt;/p>
&lt;pre>&lt;code>apiVersion: apps/v1beta1
kind: Deployment
metadata:
labels:
app: stock-con
name: stock-con
namespace: dok
spec:
replicas: 1
template:
metadata:
labels:
app: stock-con
spec:
containers:
- name: stock-con
image: quay.io/mhausenblas/stock-con
env:
- name: DOK_STOCKGEN_HOSTNAME
value: stock-gen
- name: DOK_STOCKGEN_PORT
value: &amp;quot;9999&amp;quot;
ports:
- containerPort: 9898
protocol: TCP
livenessProbe:
initialDelaySeconds: 2
periodSeconds: 5
httpGet:
path: /healthz
port: 9898
readinessProbe:
initialDelaySeconds: 2
periodSeconds: 5
httpGet:
path: /healthz
port: 9898
---
apiVersion: v1
kind: Service
metadata:
labels:
app: stock-con
name: stock-con
namespace: dok
spec:
type: ClusterIP
ports:
- name: http
port: 80
protocol: TCP
targetPort: 9898
selector:
app: stock-con
&lt;/code>&lt;/pre>&lt;!--
The final step before we can start development is to configure Skaffold. So, create a file `skaffold.yaml` in the `stock-con/` directory with the following content:
-->
&lt;p>我们能够开始开发之前的最后一步是配置 Skaffold。因此，在 &lt;code>stock-con/&lt;/code> 目录中创建文件 &lt;code>skaffold.yaml&lt;/code>，其中包含以下内容：&lt;/p>
&lt;pre>&lt;code>apiVersion: skaffold/v1alpha2
kind: Config
build:
artifacts:
- imageName: quay.io/mhausenblas/stock-con
workspace: .
docker: {}
local: {}
deploy:
kubectl:
manifests:
- app.yaml
&lt;/code>&lt;/pre>&lt;!--
Now we’re ready to kick off the development. For that execute the following in the `stock-con/` directory:
-->
&lt;p>现在我们准备好开始开发了。为此，在 &lt;code>stock-con/&lt;/code> 目录中执行以下命令：&lt;/p>
&lt;pre>&lt;code>$ skaffold dev
&lt;/code>&lt;/pre>&lt;!--
Above command triggers a build of the `stock-con` image and then a deployment. Once the pod of the `stock-con` deployment is running, we again forward the `stock-con` service for local consumption (in a separate terminal session) and check the response of the `healthz` endpoint:
-->
&lt;p>上面的命令将触发 &lt;code>stock-con&lt;/code> 图像的构建和部署。一旦 &lt;code>stock-con&lt;/code> 部署的 pod 开始运行，我们再次转发 &lt;code>stock-con&lt;/code> 服务以供本地读取（在单独的终端窗口中）并检查 &lt;code>healthz&lt;/code> 端点的响应：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl get -n dok po --selector&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b8860b">app&lt;/span>&lt;span style="color:#666">=&lt;/span>stock-con &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> -o&lt;span style="color:#666">=&lt;/span>custom-columns&lt;span style="color:#666">=&lt;/span>:metadata.name --no-headers | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> xargs -IPOD kubectl -n dok port-forward POD 9898:9898 &amp;amp;
$ watch curl localhost:9898/healthz
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
If you now change the code in the `stock-con`directory, for example, by updating the [`/healthz` endpoint code in `service.js`](https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52) by adding a field to the JSON response, you should see Skaffold noticing the change and create a new image as well as deploy it. The resulting screen would look something like this:
-->
&lt;p>现在，如果您修改一下 &lt;code>stock-con&lt;/code> 目录中的代码，例如 &lt;a href="https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52">&lt;code>service.js&lt;/code> 中定义的 &lt;code>/healthz&lt;/code> 端点代码&lt;/a>，在其 JSON 形式的响应中添加一个字段，您应该看到 Skaffold 可以检测到代码改动并创建新图像以及部署它。您的屏幕看起来应该类似这样：&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-skaffold_preview.png" alt="Skaffold Preview">&lt;/p>
&lt;!--
By now you should have a feeling how different tools enable you to develop apps on Kubernetes and if you’re interested to learn more about tools and or methods, check out the following resources:
-->
&lt;p>至此，您应该对不同的工具如何帮您在 Kubernetes 上开发应用程序有了一定的概念，如果您有兴趣了解有关工具和/或方法的更多信息，请查看以下资源：&lt;/p>
&lt;ul>
&lt;li>Blog post by Shahidh K Muhammed on &lt;a href="https://blog.hasura.io/draft-vs-gitkube-vs-helm-vs-ksonnet-vs-metaparticle-vs-skaffold-f5aa9561f948">Draft vs Gitkube vs Helm vs Ksonnet vs Metaparticle vs Skaffold&lt;/a> (03/2018)&lt;/li>
&lt;li>Blog post by Gergely Nemeth on &lt;a href="https://nemethgergely.com/using-kubernetes-for-local-development/index.html">Using Kubernetes for Local Development&lt;/a>, with a focus on Skaffold (03/2018)&lt;/li>
&lt;li>Blog post by Richard Li on &lt;a href="https://hackernoon.com/locally-developing-kubernetes-services-without-waiting-for-a-deploy-f63995de7b99">Locally developing Kubernetes services (without waiting for a deploy)&lt;/a>, with a focus on Telepresence&lt;/li>
&lt;li>Blog post by Abhishek Tiwari on &lt;a href="https://abhishek-tiwari.com/local-development-environment-for-kubernetes-using-minikube/">Local Development Environment for Kubernetes using Minikube&lt;/a> (09/2017)&lt;/li>
&lt;li>Blog post by Aymen El Amri on &lt;a href="https://medium.com/devopslinks/using-kubernetes-minikube-for-local-development-c37c6e56e3db">Using Kubernetes for Local Development — Minikube&lt;/a> (08/2017)&lt;/li>
&lt;li>Blog post by Alexis Richardson on &lt;a href="https://www.weave.works/blog/gitops-operations-by-pull-request">​GitOps - Operations by Pull Request&lt;/a> (08/2017)&lt;/li>
&lt;li>Slide deck &lt;a href="https://docs.google.com/presentation/d/1d3PigRVt_m5rO89Ob2XZ16bW8lRSkHHH5k816-oMzZo/">GitOps: Drive operations through git&lt;/a>, with a focus on Gitkube by Tirumarai Selvan (03/2018)&lt;/li>
&lt;li>Slide deck &lt;a href="https://speakerdeck.com/mhausenblas/developing-apps-on-kubernetes">Developing apps on Kubernetes&lt;/a>, a talk Michael Hausenblas gave at a CNCF Paris meetup (04/2018)&lt;/li>
&lt;li>YouTube videos:
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=QW85Y0Ug3KY">TGI Kubernetes 029: Developing Apps with Ksync&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=McwwWhCXMxc">TGI Kubernetes 030: Exploring Skaffold&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=zezeBAJ_3w8">TGI Kubernetes 031: Connecting with Telepresence&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=8B1D7cTMPgA">TGI Kubernetes 033: Developing with Draft&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Raw responses to the &lt;a href="https://docs.google.com/spreadsheets/d/12ilRCly2eHKPuicv1P_BD6z__PXAqpiaR-tDYe2eudE/edit">Kubernetes Application Survey&lt;/a> 2018 by SIG Apps&lt;/li>
&lt;/ul>
&lt;!--
With that we wrap up this post on how to go about developing apps on Kubernetes, we hope you learned something and if you have feedback and/or want to point out a tool that you found useful, please let us know via Twitter: [Ilya](https://twitter.com/errordeveloper) and [Michael](https://twitter.com/mhausenblas).
-->
&lt;p>有了这些，我们这篇关于如何在 Kubernetes 上开发应用程序的博客就可以收尾了，希望您有所收获，如果您有反馈和/或想要指出您认为有用的工具，请通过 Twitter 告诉我们：&lt;a href="https://twitter.com/errordeveloper">Ilya&lt;/a> 和 &lt;a href="https://twitter.com/mhausenblas">Michael&lt;/a>&lt;/p></description></item><item><title>Blog: Kubernetes 社区 - 2017 年开源排行榜榜首</title><link>https://kubernetes.io/zh/blog/2018/04/25/open-source-charts-2017/</link><pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/04/25/open-source-charts-2017/</guid><description>
&lt;!--
---
title: Kubernetes Community - Top of the Open Source Charts in 2017
date: 2018-04-25
slug: open-source-charts-2017
---
--->
&lt;!--
2017 was a huge year for Kubernetes, and GitHub’s latest [Octoverse report](https://octoverse.github.com) illustrates just how much attention this project has been getting.
Kubernetes, an [open source platform for running application containers](/docs/concepts/overview/what-is-kubernetes/), provides a consistent interface that enables developers and ops teams to automate the deployment, management, and scaling of a wide variety of applications on just about any infrastructure.
--->
&lt;p>对于 Kubernetes 来说，2017 年是丰收的一年，GitHub的最新 &lt;a href="https://octoverse.github.com">Octoverse 报告&lt;/a> 说明了该项目获得了多少关注。&lt;/p>
&lt;p>Kubernetes 是 &lt;a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/">用于运行应用程序容器的开源平台&lt;/a>，它提供了一个统一的界面，使开发人员和操作团队能够自动执行部署、管理和扩展几乎任何基础架构上的各种应用程序。&lt;/p>
&lt;!--
Solving these shared challenges by leveraging a wide community of expertise and industrial experience, as Kubernetes does, helps engineers focus on building their own products at the top of the stack, rather than needlessly duplicating work that now exists as a standard part of the “cloud native” toolkit.
However, achieving these gains via ad-hoc collective organizing is its own unique challenge, one which makes it increasingly difficult to support open source, community-driven efforts through periods of rapid growth.
Read on to find out how the Kubernetes Community has addressed these scaling challenges to reach the top of the charts in GitHub’s 2017 Octoverse report.
--->
&lt;p>Kubernetes 所做的，是通过利用广泛的专业知识和行业经验来解决这些共同的挑战，可以帮助工程师专注于在堆栈的顶部构建自己的产品，而不是不必要地进行重复工作，比如现在已经存在的 “云原生” 工具包的标准部分。&lt;/p>
&lt;p>但是，通过临时的集体组织来实现这些收益是它独有的挑战，这使得支持开源，社区驱动的工作变得越来越困难。&lt;/p>
&lt;p>继续阅读以了解 Kubernetes 社区如何解决这些挑战，从而在 GitHub 的 2017 Octoverse 报告中位居榜首。&lt;/p>
&lt;!--
## Most-Discussed on GitHub
The top two most-discussed repos of 2017 are both based on Kubernetes:
![Most Discussed](/images/blog-logging/2018-04-24-open-source-charts-2017/most-discussed.png)
Of all the open source repositories on GitHub, none received more issue comments than [kubernetes/kubernetes](https://github.com/kubernetes/kubernetes/). [OpenShift](http://openshift.com/), a [CNCF certified distribution of Kubernetes](https://www.cncf.io/announcement/2017/11/13/cloud-native-computing-foundation-launches-certified-kubernetes-program-32-conformant-distributions-platforms/), took second place.
Open discussion with ample time for community feedback and review helps build shared infrastructure and establish new standards for cloud native computing.
--->
&lt;h2 id="github-上讨论最多的">GitHub 上讨论最多的&lt;/h2>
&lt;p>2017 年讨论最多的两个仓库都是基于 Kubernetes 的：&lt;/p>
&lt;p>！&lt;a href="https://kubernetes.io/images/blog-logging/2018-04-24-open-source-charts-2017/most-discussed.png">讨论最多&lt;/a>&lt;/p>
&lt;p>在 GitHub 的所有开源存储库中，没有比 &lt;a href="https://github.com/kubernetes/kubernetes/">kubernetes/kubernetes&lt;/a> 收到更多的评论。 &lt;a href="http://openshift.com/">OpenShift&lt;/a>， &lt;a href="https://www.cncf.io/announcement/2017/11/13/cloud-native-computing-foundation-launches-certified-kubernetes-program-32-conformant-distributions-platforms/">CNCF 认证的 Kubernetes 发行版&lt;/a> 排名第二。&lt;/p>
&lt;p>利用充足时间进行公开讨论来获取社区反馈和审查，有助于建立共享的基础架构并为云原生计算建立新标准。&lt;/p>
&lt;!--
## Most Reviewed on GitHub
Successfully scaling an open source effort’s communications often leads to better coordination and higher-quality feature delivery. The Kubernetes project’s [Special Interest Group (SIG)](https://github.com/kubernetes/community/blob/master/sig-list.md) structure has helped it become GitHub’s second most reviewed project:
![Most Reviewed](/images/blog-logging/2018-04-24-open-source-charts-2017/most-reviews.png)
Using SIGs to segment and standardize mechanisms for community participation helps channel more frequent reviews from better-qualified community members.
When managed effectively, active community discussions indicate more than just a highly contentious codebase, or a project with an extensive list of unmet needs.
--->
&lt;h2 id="github-上审阅最多的">GitHub 上审阅最多的&lt;/h2>
&lt;p>成功扩展开放源代码工作的通信通常会带来更好的协调和更高质量的功能交付。Kubernetes 项目的 &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Group（SIG）&lt;/a> 结构已使其成为 GitHub 审阅第二多的项目：&lt;/p>
&lt;p>！&lt;a href="https://kubernetes.io/images/blog-logging/2018-04-24-open-source-charts-2017/most-reviews.png">审阅最多&lt;/a>&lt;/p>
&lt;p>使用 SIG 对社区参与机制进行细分和标准化有助于从资格更高的社区成员中获得更频繁的审阅。&lt;/p>
&lt;p>如果得到有效管理，活跃的社区讨论不仅表明代码库存在很大的争议，也可能表明项目包含大量未满足的需求。&lt;/p>
&lt;!--
Scaling a project’s capacity to handle issues and community interactions helps to expand the conversation. Meanwhile, large communities come with more diverse use cases and a larger array of support problems to manage. The Kubernetes [SIG organization structure](https://github.com/kubernetes/community#sigs) helps to address the challenges of complex communication at scale.
SIG meetings provide focused opportunities for users, maintainers, and specialists from various disciplines to collaborate together in support of this community effort. These investments in organizing help create an environment where it’s easier to prioritize architecture discussion and planning over commit velocity; enabling the project to sustain this kind of scale.
--->
&lt;p>扩展项目处理问题和社区互动的能力有助于扩大交流。同时，大型社区具有更多不同的用例和更多的支持问题需要管理。Kubernetes &lt;a href="https://github.com/kubernetes/community#sigs">SIG 组织结构&lt;/a> 帮助应对大规模复杂通信的挑战。&lt;/p>
&lt;p>SIG 会议为不同学科的用户、维护者和专家提供了重点合作的机会，以共同协作来支持社区的工作。这些在组织上的投资有助于创建一个环境，在这样的环境中，可以更轻松地将架构讨论和规划的优先级排到提交速度前面，并使项目能够维持这种规模。&lt;/p>
&lt;!--
## Join the party!
You may already be using solutions that are successfully managed and scaled on Kubernetes. For example, GitHub.com, which hosts Kubernetes’ upstream source code, [now runs on Kubernetes](https://githubengineering.com/kubernetes-at-github/) as well!
Check out the [Kubernetes Contributors’ guide](https://github.com/kubernetes/community/blob/master/contributors/guide/README.md) for more information on how to get started as a contributor.
You can also join the [weekly Kubernetes Community meeting](https://github.com/kubernetes/community/tree/master/communication#weekly-meeting) and consider [joining a SIG or two](https://github.com/kubernetes/community/blob/master/sig-list.md#master-sig-list).
--->
&lt;h2 id="加入我们">加入我们！&lt;/h2>
&lt;p>您可能已经在 Kubernetes 上成功使用管理和扩展的解决方案。例如，托管 Kubernetes 上游源代码的 GitHub.com &lt;a href="https://githubengineering.com/kubernetes-at-github/">现在也可以在 Kubernetes 上运行&lt;/a> ！&lt;/p>
&lt;p>请查看 &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/guide/README.md">Kubernetes 贡献者指南&lt;/a> ，以获取有关如何开始作为贡献者的更多信息。&lt;/p>
&lt;p>您也可以参加 &lt;a href="https://github.com/kubernetes/community/tree/master/communication#weekly-meeting">每周 Kubernetes 的社区会议&lt;/a> 并考虑 &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md#master-sig-list">加入一个或两个 SIG&lt;/a>。&lt;/p></description></item><item><title>Blog: “基于容器的应用程序设计原理”</title><link>https://kubernetes.io/zh/blog/2018/03/Principles-Of-Container-App-Design/</link><pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/03/Principles-Of-Container-App-Design/</guid><description>
&lt;!--
---
title: "Principles of Container-based Application Design"
date: 2018-03-15
slug: principles-of-container-app-design
url: /blog/2018/03/Principles-Of-Container-App-Design
---
-->
&lt;!--
It's possible nowadays to put almost any application in a container and run it. Creating cloud-native applications, however—containerized applications that are automated and orchestrated effectively by a cloud-native platform such as Kubernetes—requires additional effort. Cloud-native applications anticipate failure; they run and scale reliably even when their infrastructure experiences outages. To offer such capabilities, cloud-native platforms like Kubernetes impose a set of contracts and constraints on applications. These contracts ensure that applications they run conform to certain constraints and allow the platform to automate application management.
-->
&lt;p>如今，可以将几乎所有应用程序放入容器中并运行它。
但是，创建云原生应用程序（由 Kubernetes 等云原生平台自动有效地编排的容器化应用程序）需要付出额外的努力。
云原生应用程序会预期失败；
它们可以可靠的运行和扩展，即使基础架构出现故障。
为了提供这样的功能，像 Kubernetes 这样的云原生平台对应用程序施加了一系列约定和约束。
这些合同确保运行的应用程序符合某些约束条件，并允许平台自动执行应用程序管理。&lt;/p>
&lt;!--
I've outlined [seven principles][1]for containerized applications to follow in order to be fully cloud-native.
-->
&lt;p>我总结了容器化应用要成为彻底的云原生应用所要遵从的&lt;a href="https://www.redhat.com/en/resources/cloud-native-container-design-whitepaper">七个原则&lt;/a>。&lt;/p>
&lt;p>| ----- |
| &lt;img src="https://lh5.googleusercontent.com/1XqojkVC0CET1yKCJqZ3-0VWxJ3W8Q74zPLlqnn6eHSJsjHOiBTB7EGUX5o_BOKumgfkxVdgBeLyoyMfMIXwVm9p2QXkq_RRy2mDJG1qEExJDculYL5PciYcWfPAKxF2-DGIdiLw" alt=""> |&lt;/p>
&lt;!--
| Container Design Principles |
-->
&lt;p>| 容器设计原则 |&lt;/p>
&lt;!--
These seven principles cover both build time and runtime concerns.
-->
&lt;p>这七个原则涵盖了构建时间和运行时问题。&lt;/p>
&lt;!--
#### Build time
-->
&lt;h4 id="建立时间">建立时间&lt;/h4>
&lt;!--
* **Single Concern:** Each container addresses a single concern and does it well.
-->
&lt;ul>
&lt;li>&lt;strong>单独关注点：&lt;/strong> 每个容器都解决了一个单独的关注点，并做到了。&lt;/li>
&lt;/ul>
&lt;!--
* **Self-Containment:** A container relies only on the presence of the Linux kernel. Additional libraries are added when the container is built.
-->
&lt;ul>
&lt;li>&lt;strong>自包含：&lt;/strong> 容器仅依赖于Linux内核的存在。 构建容器时会添加其他库。&lt;/li>
&lt;/ul>
&lt;!--
* **Image Immutability:** Containerized applications are meant to be immutable, and once built are not expected to change between different environments.
-->
&lt;ul>
&lt;li>&lt;strong>镜像不可变性：&lt;/strong> 容器化应用程序是不可变的，并且一旦构建，就不会在不同环境之间发生变化。&lt;/li>
&lt;/ul>
&lt;!--
#### Runtime
-->
&lt;h4 id="运行时">运行时&lt;/h4>
&lt;!--
* **High Observability:** Every container must implement all necessary APIs to help the platform observe and manage the application in the best way possible.
-->
&lt;ul>
&lt;li>&lt;strong>高度可观察性：&lt;/strong> 每个容器都必须实现所有必要的API，以帮助平台以最佳方式观察和管理应用程序。&lt;/li>
&lt;/ul>
&lt;!--
* **Lifecycle Conformance:** A container must have a way to read events coming from the platform and conform by reacting to those events.
-->
&lt;ul>
&lt;li>&lt;strong>生命周期一致性：&lt;/strong> 容器必须具有读取来自平台的事件并通过对这些事件做出反应来进行一致性的方式。&lt;/li>
&lt;/ul>
&lt;!--
* **Process Disposability:** Containerized applications must be as ephemeral as possible and ready to be replaced by another container instance at any point in time.
-->
&lt;ul>
&lt;li>&lt;strong>进程可丢弃：&lt;/strong> 容器化的应用程序必须尽可能短暂，并随时可以被另一个容器实例替换。&lt;/li>
&lt;/ul>
&lt;!--
* **Runtime Confinement:** Every container must declare its resource requirements and restrict resource use to the requirements indicated.
The build time principles ensure that containers have the right granularity, consistency, and structure in place. The runtime principles dictate what functionalities must be implemented in order for containerized applications to possess cloud-native function. Adhering to these principles helps ensure that your applications are suitable for automation in Kubernetes.
-->
&lt;ul>
&lt;li>&lt;strong>运行时可约束&lt;/strong> 每个容器必须声明其资源需求，并根据所标明的需求限制其资源使用。
构建时间原则可确保容器具有正确的颗粒度，一致性和适当的结构。
运行时原则规定了必须执行哪些功能才能使容器化的应用程序具有云原生功能。
遵循这些原则有助于确保您的应用程序适合Kubernetes中的自动化。&lt;/li>
&lt;/ul>
&lt;!--
The white paper is freely available for download:
-->
&lt;p>白皮书可以免费下载：&lt;/p>
&lt;!--
To read more about designing cloud-native applications for Kubernetes, check out my [Kubernetes Patterns][3] book.
-->
&lt;p>要了解有关为Kubernetes设计云原生应用程序的更多信息，请翻阅我的&lt;a href="http://leanpub.com/k8spatterns/">Kubernetes 模式&lt;/a>这本书。&lt;/p>
&lt;!--
— [Bilgin Ibryam][4], Principal Architect, Red Hat
-->
&lt;p>— &lt;a href="http://twitter.com/bibryam">Bilgin Ibryam&lt;/a>，首席架构师，Red Hat&lt;/p>
&lt;!--
Twitter:  
Blog: [http://www.ofbizian.com][5]
Linkedin:
-->
&lt;p>推特：  &lt;br>
博客： &lt;a href="http://www.ofbizian.com/">http://www.ofbizian.com&lt;/a>&lt;br>
领英：&lt;/p>
&lt;!--
Bilgin Ibryam (@bibryam) is a principal architect at Red Hat, open source committer at ASF, blogger, author, and speaker. He is the author of Camel Design Patterns and Kubernetes Patterns books. In his day-to-day job, Bilgin enjoys mentoring, training and leading teams to be successful with distributed systems, microservices, containers, and cloud-native applications in general.
-->
&lt;p>Bilgin Ibryam（@bibryam）是 Red Hat 的首席架构师，ASF 的开源提交者，博客，作者和发言人。
他是骆驼设计模式和 Kubernetes 模式书籍的作者。
在日常工作中，Bilgin 乐于指导，培训和领导团队，以使他们在分布式系统，微服务，容器和云原生应用程序方面取得成功。&lt;/p></description></item><item><title>Blog: Kubernetes 1.9 对 Windows Server 容器提供 Beta 版本支持</title><link>https://kubernetes.io/blog/2018/01/Kubernetes-V19-Beta-Windows-Support/</link><pubDate>Tue, 09 Jan 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2018/01/Kubernetes-V19-Beta-Windows-Support/</guid><description>
&lt;!--
---
title: Kubernetes v1.9 releases beta support for Windows Server Containers
date: 2018-01-09
slug: kubernetes-v19-beta-windows-support
url: /blog/2018/01/Kubernetes-V19-Beta-Windows-Support
---
--->
&lt;!--
With the release of Kubernetes v1.9, our mission of ensuring Kubernetes works well everywhere and for everyone takes a great step forward. We’ve advanced support for Windows Server to beta along with continued feature and functional advancements on both the Kubernetes and Windows platforms. SIG-Windows has been working since March of 2016 to open the door for many Windows-specific applications and workloads to run on Kubernetes, significantly expanding the implementation scenarios and the enterprise reach of Kubernetes.
--->
&lt;p>随着 Kubernetes v1.9 的发布，我们确保所有人在任何地方都能正常运行 Kubernetes 的使命前进了一大步。我们的 Beta 版本对 Windows Server 的支持进行了升级，并且在 Kubernetes 和 Windows 平台上都提供了持续的功能改进。为了在 Kubernetes 上运行许多特定于 Windows 的应用程序和工作负载，SIG-Windows 自2016年3月以来一直在努力，大大扩展了 Kubernetes 的实现场景和企业适用范围。&lt;/p>
&lt;!--
Enterprises of all sizes have made significant investments in .NET and Windows based applications. Many enterprise portfolios today contain .NET and Windows, with Gartner claiming that [80%](http://www.gartner.com/document/3446217) of enterprise apps run on Windows. According to StackOverflow Insights, 40% of professional developers use the .NET programming languages (including .NET Core).
--->
&lt;p>各种规模的企业都在 .NET 和基于 Windows 的应用程序上进行了大量投资。如今许多企业产品组合都包含 .NET 和 Windows，Gartner 声称 &lt;a href="http://www.gartner.com/document/3446217">80%&lt;/a> 的企业应用都在 Windows 上运行。根据 StackOverflow Insights，40% 的专业开发人员使用 .NET 编程语言（包括 .NET Core）。&lt;/p>
&lt;!--
But why is all this information important? It means that enterprises have both legacy and new born-in-the-cloud (microservice) applications that utilize a wide array of programming frameworks. There is a big push in the industry to modernize existing/legacy applications to containers, using an approach similar to “lift and shift”. Modernizing existing applications into containers also provides added flexibility for new functionality to be introduced in additional Windows or Linux containers. Containers are becoming the de facto standard for packaging, deploying, and managing both existing and microservice applications. IT organizations are looking for an easier and homogenous way to orchestrate and manage containers across their Linux and Windows environments. Kubernetes v1.9 now offers beta support for Windows Server containers, making it the clear choice for orchestrating containers of any kind.
--->
&lt;p>但为什么这些信息都很重要？这意味着企业既有传统的，也有新生的云（microservice）应用程序，利用了大量的编程框架。业界正在大力推动将现有/遗留应用程序现代化到容器中，使用类似于“提升和转移”的方法。同时，也能灵活地向其他 Windows 或 Linux 容器引入新功能。容器正在成为打包、部署和管理现有程序和微服务应用程序的业界标准。IT 组织正在寻找一种更简单且一致的方法来跨 Linux 和 Windows 环境进行协调和管理容器。Kubernetes v1.9 现在对 Windows Server 容器提供了 Beta 版本支持，使之成为策划任何类型容器的明确选择。&lt;/p>
&lt;!--
### Features
Alpha support for Windows Server containers in Kubernetes was great for proof-of-concept projects and visualizing the road map for support of Windows in Kubernetes. The alpha release had significant drawbacks, however, and lacked many features, especially in networking. SIG-Windows, Microsoft, Cloudbase Solutions, Apprenda, and other community members banded together to create a comprehensive beta release, enabling Kubernetes users to start evaluating and using Windows.
--->
&lt;h3 id="特点">特点&lt;/h3>
&lt;p>Kubernetes 中对 Windows Server 容器的 Alpha 支持是非常有用的，尤其是对于概念项目和可视化 Kubernetes 中 Windows 支持的路线图。然而，Alpha 版本有明显的缺点，并且缺少许多特性，特别是在网络方面。SIG Windows、Microsoft、Cloudbase Solutions、Apprenda 和其他社区成员联合创建了一个全面的 Beta 版本，使 Kubernetes 用户能够开始评估和使用 Windows。&lt;/p>
&lt;!--
Some key feature improvements for Windows Server containers on Kubernetes include:
- Improved support for pods! Multiple Windows Server containers in a pod can now share the network namespace using network compartments in Windows Server. This feature brings the concept of a pod to parity with Linux-based containers
- Reduced network complexity by using a single network endpoint per pod
- Kernel-Based load-balancing using the Virtual Filtering Platform (VFP) Hyper-v Switch Extension (analogous to Linux iptables)
- Container Runtime Interface (CRI) pod and node level statistics. Windows Server containers can now be profiled for Horizontal Pod Autoscaling using performance metrics gathered from the pod and the node
--->
&lt;p>Kubernetes 对 Windows 服务器容器的一些关键功能改进包括：&lt;/p>
&lt;ul>
&lt;li>改进了对 Pod 的支持！Pod 中多个 Windows Server 容器现在可以使用 Windows Server 中的网络隔离专区共享网络命名空间。此功能中 Pod 的概念相当于基于 Linux 的容器&lt;/li>
&lt;li>可通过每个 Pod 使用单个网络端点来降低网络复杂性&lt;/li>
&lt;li>可以使用 Virtual Filtering Platform（VFP）的 Hyper-v Switch Extension（类似于 Linux iptables）达到基于内核的负载平衡&lt;/li>
&lt;li>具备 Container Runtime Interface（CRI）的 Pod 和 Node 级别的统计信息。可以使用从 Pod 和节点收集的性能指标配置 Windows Server 容器的 Horizontal Pod Autoscaling&lt;/li>
&lt;/ul>
&lt;!--
- Support for kubeadm commands to add Windows Server nodes to a Kubernetes environment. Kubeadm simplifies the provisioning of a Kubernetes cluster, and with the support for Windows Server, you can use a single tool to deploy Kubernetes in your infrastructure
- Support for ConfigMaps, Secrets, and Volumes. These are key features that allow you to separate, and in some cases secure, the configuration of the containers from the implementation
The crown jewels of Kubernetes 1.9 Windows support, however, are the networking enhancements. With the release of Windows Server 1709, Microsoft has enabled key networking capabilities in the operating system and the Windows Host Networking Service (HNS) that paved the way to produce a number of CNI plugins that work with Windows Server containers in Kubernetes. The Layer-3 routed and network overlay plugins that are supported with Kubernetes 1.9 are listed below:
--->
&lt;ul>
&lt;li>支持 kubeadm 命令将 Windows Server 的 Node 添加到 Kubernetes 环境。Kubeadm 简化了 Kubernetes 集群的配置，通过对 Windows Server 的支持，您可以在您的基础配置中使用单一的工具部署 Kubernetes&lt;/li>
&lt;li>支持 ConfigMaps, Secrets, 和 Volumes。这些是非常关键的特性，您可以将容器的配置从实施体系中分离出来，并且在大部分情况下是安全的 &lt;br>
然而，kubernetes 1.9 windows 支持的最大亮点是网络增强。随着 Windows 服务器 1709 的发布，微软在操作系统和 Windows Host Networking Service（HNS）中启用了关键的网络功能，这为创造大量与 Kubernetes 中的 Windows 服务器容器一起工作的 CNI 插件铺平了道路。Kubernetes 1.9 支持的第三层路由和网络覆盖插件如下所示：&lt;/li>
&lt;/ul>
&lt;!--
1. Upstream L3 Routing - IP routes configured in upstream ToR
2. Host-Gateway - IP routes configured on each host
3. Open vSwitch (OVS) &amp; Open Virtual Network (OVN) with Overlay - Supports STT and Geneve tunneling types
You can read more about each of their [configuration, setup, and runtime capabilities](/docs/getting-started-guides/windows/) to make an informed selection for your networking stack in Kubernetes.
--->
&lt;ol>
&lt;li>上游 L3 路由 - 上游 ToR 中配置的 IP 路由&lt;/li>
&lt;li>Host-Gateway - 在每个主机上配置的 IP 路由&lt;/li>
&lt;li>具有 Overlay 的 Open vSwitch（OVS）和 Open Virtual Network（OVN） - 支持 STT 和 Geneve 的 tunneling 类型
您可以阅读更多有关 &lt;a href="https://kubernetes.io/docs/getting-started-guides/windows/">配置、设置和运行时功能&lt;/a> 的信息，以便在 Kubernetes 中为您的网络堆栈做出明智的选择。&lt;/li>
&lt;/ol>
&lt;!--
Even though you have to continue running the Kubernetes Control Plane and Master Components in Linux, you are now able to introduce Windows Server as a Node in Kubernetes. As a community, this is a huge milestone and achievement. We will now start seeing .NET, .NET Core, ASP.NET, IIS, Windows Services, Windows executables and many more windows-based applications in Kubernetes.
--->
&lt;p>如果您需要继续在 Linux 中运行 Kubernetes Control Plane 和 Master Components，现在也可以将 Windows Server 作为 Kubernetes 中的一个节点引入。对一个社区来说，这是一个巨大的里程碑和成就。现在，我们将会在 Kubernetes 中看到 .NET，.NET Core，ASP.NET，IIS，Windows 服务，Windows 可执行文件以及更多基于 Windows 的应用程序。&lt;/p>
&lt;!--
### What’s coming next
A lot of work went into this beta release, but the community realizes there are more areas of investment needed before we can release Windows support as GA (General Availability) for production workloads. Some keys areas of focus for the first two quarters of 2018 include:
--->
&lt;h3 id="接下来还会有什么">接下来还会有什么&lt;/h3>
&lt;p>这个 Beta 版本进行了大量工作，但是社区意识到在将 Windows 支持作为生产工作负载发布为 GA（General Availability）之前，我们需要更多领域的投资。2018年前两个季度的重点关注领域包括：&lt;/p>
&lt;!--
1. Continue to make progress in the area of networking. Additional CNI plugins are under development and nearing completion
- Overlay - win-overlay (vxlan or IP-in-IP encapsulation using Flannel)&amp;nbsp;
- Win-l2bridge (host-gateway)&amp;nbsp;
- OVN using cloud networking - without overlays
- Support for Kubernetes network policies in ovn-kubernetes
- Support for Hyper-V Isolation
- Support for StatefulSet functionality for stateful applications
- Produce installation artifacts and documentation that work on any infrastructure and across many public cloud providers like Microsoft Azure, Google Cloud, and Amazon AWS
- Continuous Integration/Continuous Delivery (CI/CD) infrastructure for SIG-Windows
- Scalability and Performance testing
Even though we have not committed to a timeline for GA, SIG-Windows estimates a GA release in the first half of 2018.
--->
&lt;ol>
&lt;li>继续在网络领域取得更多进展。其他 CNI 插件正在开发中，并且即将完成&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Overlay - win-Overlay（vxlan 或 IP-in-IP 使用 Flannel 封装）&lt;/li>
&lt;li>Win-l2bridge（host-gateway）&lt;/li>
&lt;li>使用云网络的 OVN - 不再依赖 Overlay&lt;/li>
&lt;li>在 ovn-Kubernetes 中支持 Kubernetes 网络策略&lt;/li>
&lt;li>支持 Hyper-V Isolation&lt;/li>
&lt;li>支持有状态应用程序的 StatefulSet 功能&lt;/li>
&lt;li>生成适用于任何基础架构以及跨多公共云提供商（例如 Microsoft Azure，Google Cloud 和 Amazon AWS）的安装工具和文档&lt;/li>
&lt;li>SIG-Windows 的 Continuous Integration/Continuous Delivery（CI/CD）基础结构&lt;/li>
&lt;li>可伸缩性和性能测试
尽管我们尚未承诺正式版的具体时间线，但估计 SIG-Windows 将于2018年上半年正式发布。&lt;/li>
&lt;/ul>
&lt;!--
### Get Involved
As we continue to make progress towards General Availability of this feature in Kubernetes, we welcome you to get involved, contribute code, provide feedback, deploy Windows Server containers to your Kubernetes cluster, or simply join our community.
--->
&lt;h3 id="加入我们">加入我们&lt;/h3>
&lt;p>随着我们在 Kubernetes 的普遍可用性方向不断取得进展，我们欢迎您参与进来，贡献代码、提供反馈，将 Windows 服务器容器部署到 Kubernetes 集群，或者干脆加入我们的社区。&lt;/p>
&lt;!--
- If you want to get started on deploying Windows Server containers in Kubernetes, read our getting started guide at [/docs/getting-started-guides/windows/](/docs/getting-started-guides/windows/)
- We meet every other Tuesday at 12:30 Eastern Standard Time (EST) at [https://zoom.us/my/sigwindows](https://zoom.us/my/sigwindows). All our meetings are recorded on youtube and referenced at [https://www.youtube.com/playlist?list=PL69nYSiGNLP2OH9InCcNkWNu2bl-gmIU4](https://www.youtube.com/playlist?list=PL69nYSiGNLP2OH9InCcNkWNu2bl-gmIU4)
- Chat with us on Slack at [https://kubernetes.slack.com/messages/sig-windows](https://kubernetes.slack.com/messages/sig-windows)
- Find us on GitHub at [https://github.com/kubernetes/community/tree/master/sig-windows](https://github.com/kubernetes/community/tree/master/sig-windows)
--->
&lt;ul>
&lt;li>如果你想要开始在 Kubernetes 中部署 Windows Server 容器，请阅读我们的开始导览 &lt;a href="https://kubernetes.io/docs/getting-started-guides/windows/">/docs/getting-started-guides/windows/&lt;/a>&lt;/li>
&lt;li>我们每隔一个星期二在美国东部标准时间（EST）的12:30在 &lt;a href="https://zoom.us/my/sigwindows">https://zoom.us/my/sigwindows&lt;/a> 开会。所有会议内容都记录在 Youtube 并附上了参考材料 &lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP2OH9InCcNkWNu2bl-gmIU4">https://www.youtube.com/playlist?list=PL69nYSiGNLP2OH9InCcNkWNu2bl-gmIU4&lt;/a>&lt;/li>
&lt;li>通过 Slack 联系我们 &lt;a href="https://kubernetes.slack.com/messages/sig-windows">https://kubernetes.slack.com/messages/sig-windows&lt;/a>&lt;/li>
&lt;li>在 Github 上找到我们 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows">https://github.com/kubernetes/community/tree/master/sig-windows&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Thank you,
Michael Michael (@michmike77)
SIG-Windows Lead
Senior Director of Product Management, Apprenda
--->
&lt;p>谢谢大家，&lt;/p>
&lt;p>Michael Michael (@michmike77)&lt;br>
SIG-Windows 领导人&lt;br>
Apprenda 产品管理高级总监&lt;/p></description></item><item><title>Blog: Kubernetes 中自动缩放</title><link>https://kubernetes.io/zh/blog/2017/11/17/autoscaling-in-kubernetes/</link><pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2017/11/17/autoscaling-in-kubernetes/</guid><description>
&lt;!--
---
title: " Autoscaling in Kubernetes "
date: 2017-11-17
slug: autoscaling-in-kubernetes
url: /blog/2017/11/Autoscaling-In-Kubernetes
---
-->
&lt;!--
Kubernetes allows developers to automatically adjust cluster sizes and the number of pod replicas based on current traffic and load. These adjustments reduce the amount of unused nodes, saving money and resources. In this talk, Marcin Wielgus of Google walks you through the current state of pod and node autoscaling in Kubernetes: .how it works, and how to use it, including best practices for deployments in production applications.
-->
&lt;p>Kubernetes 允许开发人员根据当前的流量和负载自动调整集群大小和 pod 副本的数量。这些调整减少了未使用节点的数量，节省了资金和资源。
在这次演讲中，谷歌的 Marcin Wielgus 将带领您了解 Kubernetes 中 pod 和 node 自动调焦的当前状态：它是如何工作的，以及如何使用它，包括在生产应用程序中部署的最佳实践。&lt;/p>
&lt;!--
Enjoyed this talk? Join us for more exciting sessions on scaling and automating your Kubernetes clusters at KubeCon in Austin on December 6-8. [Register Now](https://www.eventbrite.com/e/kubecon-cloudnativecon-north-america-registration-37824050754?_ga=2.9666039.317115486.1510003873-1623727562.1496428006)
-->
&lt;p>喜欢这个演讲吗？ 12 月 6 日至 8 日，在 Austin 参加 KubeCon 关于扩展和自动化您的 Kubernetes 集群的更令人兴奋的会议。&lt;a href="https://www.eventbrite.com/e/kubecon-cloudnativecon-north-america-registration-37824050754?_ga=2.9666039.317115486.1510003873-1623727562.1496428006">现在注册&lt;/a>。&lt;/p>
&lt;!--
Be sure to check out [Automating and Testing Production Ready Kubernetes Clusters in the Public Cloud](http://sched.co/CU64) by Ron Lipke, Senior Developer, Platform as a Service, Gannet/USA Today Network.
-->
&lt;p>一定要查看由 Ron Lipke， Gannet/USA Today Network, 平台即服务高级开发人员，在&lt;a href="http://sched.co/CU64">公共云中自动化和测试产品就绪的 Kubernetes 集群&lt;/a>。&lt;/p></description></item><item><title>Blog: Kubernetes 1.8 的五天</title><link>https://kubernetes.io/zh/blog/2017/10/24/five-days-of-kubernetes-18/</link><pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2017/10/24/five-days-of-kubernetes-18/</guid><description>
&lt;!--
---
title: " Five Days of Kubernetes 1.8 "
date: 2017-10-24
slug: five-days-of-kubernetes-18
url: /blog/2017/10/Five-Days-Of-Kubernetes-18
---
-->
&lt;!--
Kubernetes 1.8 is live, made possible by hundreds of contributors pushing thousands of commits in this latest releases.
-->
&lt;p>Kubernetes 1.8 已经推出，数百名贡献者在这个最新版本中推出了成千上万的提交。&lt;/p>
&lt;!--
The community has tallied more than 66,000 commits in the main repo and continues rapid growth outside of the main repo, which signals growing maturity and stability for the project. The community has logged more than 120,000 commits across all repos and 17,839 commits across all repos for v1.7.0 to v1.8.0 alone.
-->
&lt;p>社区已经有超过 66,000 个提交在主仓库，并在主仓库之外继续快速增长，这标志着该项目日益成熟和稳定。仅 v1.7.0 到 v1.8.0，社区就记录了所有仓库的超过 120,000 次提交和 17839 次提交。&lt;/p>
&lt;!--
With the help of our growing community of 1,400 plus contributors, we issued more than 3,000 PRs and pushed more than 5,000 commits to deliver Kubernetes 1.8 with significant security and workload support updates. This all points to increased stability, a result of our project-wide focus on maturing [process](https://github.com/kubernetes/sig-release), formalizing [architecture](https://github.com/kubernetes/community/tree/master/sig-architecture), and strengthening Kubernetes’ [governance model](https://github.com/kubernetes/community/tree/master/community/elections/2017).
-->
&lt;p>在拥有 1400 多名贡献者，并且不断发展壮大的社区的帮助下，我们合并了 3000 多个 PR，并发布了 5000 多个提交，最后的 Kubernetes 1.8 在安全和工作负载方面添加了很多的更新。
这一切都表明稳定性的提高，这是我们整个项目关注成熟&lt;a href="https://github.com/kubernetes/sig-release">流程&lt;/a>、形式化&lt;a href="https://github.com/kubernetes/community/tree/master/sig-architecture">架构&lt;/a>和加强 Kubernetes 的&lt;a href="https://github.com/kubernetes/community/tree/master/community/elections/2017">治理模型&lt;/a>的结果。&lt;/p>
&lt;!--
While many improvements have been contributed, we highlight key features in this series of in-depth&amp;nbsp;posts listed below. [Follow along](https://twitter.com/kubernetesio) and see what’s new and improved with storage, security and more.
-->
&lt;p>虽然有很多改进，但我们在下面列出的这一系列深度文章中突出了一些关键特性。&lt;a href="https://twitter.com/kubernetesio">跟随&lt;/a>并了解存储，安全等方面的新功能和改进功能。&lt;/p>
&lt;!--
**Day 1:** [5 Days of Kubernetes 1.8](https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18)
**Day 2:** [kubeadm v1.8 Introduces Easy Upgrades for Kubernetes Clusters](https://kubernetes.io/blog/2017/10/kubeadm-v18-released)
**Day 3:** [Kubernetes v1.8 Retrospective: It Takes a Village to Raise a Kubernetes](https://kubernetes.io/blog/2017/10/it-takes-village-to-raise-kubernetes)
**Day 4:** [Using RBAC, Generally Available in Kubernetes v1.8](https://kubernetes.io/blog/2017/10/using-rbac-generally-available-18)
**Day 5:** [Enforcing Network Policies in Kubernetes](https://kubernetes.io/blog/2017/10/enforcing-network-policies-in-kubernetes)
-->
&lt;p>&lt;strong>第一天：&lt;/strong> &lt;a href="https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18">Kubernetes 1.8 的五天&lt;/a>
&lt;strong>第二天：&lt;/strong> &lt;a href="https://kubernetes.io/blog/2017/10/kubeadm-v18-released">kubeadm v1.8 为 Kubernetes 集群引入了简单的升级&lt;/a>
&lt;strong>第三天：&lt;/strong> &lt;a href="https://kubernetes.io/blog/2017/10/it-takes-village-to-raise-kubernetes">Kubernetes v1.8 回顾：提升一个 Kubernetes 需要一个 Village&lt;/a>
&lt;strong>第四天：&lt;/strong> &lt;a href="https://kubernetes.io/blog/2017/10/using-rbac-generally-available-18">使用 RBAC，一般在 Kubernetes v1.8 中提供&lt;/a>
&lt;strong>第五天：&lt;/strong> &lt;a href="https://kubernetes.io/blog/2017/10/enforcing-network-policies-in-kubernetes">在 Kubernetes 执行网络策略&lt;/a>&lt;/p>
&lt;!--
**Connect**
-->
&lt;p>&lt;strong>链接&lt;/strong>&lt;/p>
&lt;!--
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Join the community portal for advocates on [K8sPort](http://k8sport.org/)
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates&amp;nbsp;
- Connect with the community on [Slack](http://slack.k8s.io/)
- Get involved with the Kubernetes project on [GitHub](https://github.com/kubernetes/kubernetes)
-->
&lt;ul>
&lt;li>在 &lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a> 上发布问题（或回答问题）&lt;/li>
&lt;li>加入 &lt;a href="http://k8sport.org/">K8sPort&lt;/a> 布道师的社区门户网站&lt;/li>
&lt;li>在 Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> 关注我们以获取最新更新&lt;/li>
&lt;li>与 &lt;a href="http://slack.k8s.io/">Slack&lt;/a> 上的社区联系&lt;/li>
&lt;li>参与 &lt;a href="https://github.com/kubernetes/kubernetes">GitHub&lt;/a> 上的 Kubernetes 项目&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 社区指导委员会选举结果</title><link>https://kubernetes.io/blog/2017/10/Kubernetes-Community-Steering-Committee-Election-Results/</link><pubDate>Thu, 05 Oct 2017 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2017/10/Kubernetes-Community-Steering-Committee-Election-Results/</guid><description>
&lt;!--
---
title: " Kubernetes Community Steering Committee Election Results "
date: 2017-10-05
slug: kubernetes-community-steering-committee-election-results
url: /blog/2017/10/Kubernetes-Community-Steering-Committee-Election-Results
---
-->
&lt;!--
Beginning with the announcement of Kubernetes 1.0 at OSCON in 2015, there has been a concerted effort to share the power and burden of leadership across the Kubernetes community.
-->
&lt;p>自 2015 年 OSCON 发布 Kubernetes 1.0 以来，大家一直在共同努力，在 Kubernetes 社区中共同分享领导力和责任。&lt;/p>
&lt;!--
With the work of the Bootstrap Governance Committee, consisting of Brandon Philips, Brendan Burns, Brian Grant, Clayton Coleman, Joe Beda, Sarah Novotny and Tim Hockin - a cross section of long-time leaders representing 5 different companies with major investments of talent and effort in the Kubernetes Ecosystem - we wrote an initial [Steering Committee Charter](https://github.com/kubernetes/steering/blob/master/charter.md) and launched a community wide election to seat a Kubernetes Steering Committee.
-->
&lt;p>在 Brandon Philips、Brendan Burns、Brian Grant、Clayton Coleman、Joe Beda、Sarah Novotny 和 Tim Hockin 组成的自举治理委员会的工作下 - 代表 5 家不同公司的长期领导者，他们对 Kubernetes 生态系统进行了大量的人才投资和努力 - 编写了初始的&lt;a href="https://github.com/kubernetes/steering/blob/master/charter.md">指导委员会章程&lt;/a>，并发起了一次社区选举，以选举 Kubernetes 指导委员会成员。&lt;/p>
&lt;!--
To quote from the Charter -
-->
&lt;p>引用章程 -&lt;/p>
&lt;!--
_The initial role of the steering committee is to **instantiate the formal process for Kubernetes governance**. In addition to defining the initial governance process, the bootstrap committee strongly believes that **it is important to provide a means for iterating** the processes defined by the steering committee. We do not believe that we will get it right the first time, or possibly ever, and won’t even complete the governance development in a single shot. The role of the steering committee is to be a live, responsive body that can refactor and reform as necessary to adapt to a changing project and community._
-->
&lt;p>_指导委员会的最初职责是&lt;strong>实例化 Kubernetes 治理的正式过程&lt;/strong>。除定义初始治理过程外，指导委员会还坚信&lt;strong>提供一种方法来迭代指导委员会定义的方法很重要&lt;/strong>。我们不相信我们会在第一次或以后把这些做好，也不会一口气完成治理开发工作。指导委员会的作用是成为一个积极响应的机构，可以根据需要进行重构和改造，以适应不断变化的项目和社区。&lt;/p>
&lt;!--
This is our largest step yet toward making an implicit governance structure explicit. Kubernetes vision has been one of an inclusive and broad community seeking to build software which empowers our users with the portability of containers. The Steering Committee will be a strong leadership voice guiding the project toward success.
-->
&lt;p>这是将我们隐式治理结构明确化的最大一步。Kubernetes 的愿景一直是成为一个包容而广泛的社区，用我们的软件带给用户容器的便利性。指导委员会将是一个强有力的引领声音，指导该项目取得成功。&lt;/p>
&lt;!--
The Kubernetes Community is pleased to announce the results of the 2017 Steering Committee Elections. **Please congratulate Aaron Crickenberger, Derek Carr, Michelle Noorali, Phillip Wittrock, Quinton Hoole and Timothy St. Clair** , who will be joining the members of the Bootstrap Governance committee on the newly formed Kubernetes Steering Committee. Derek, Michelle, and Phillip will serve for 2 years. Aaron, Quinton, and Timothy will serve for 1 year.
-->
&lt;p>Kubernetes 社区很高兴地宣布 2017 年指导委员会选举的结果。 &lt;strong>请祝贺 Aaron Crickenberger、Derek Carr、Michelle Noorali、Phillip Wittrock、Quinton Hoole 和 Timothy St. Clair&lt;/strong>，他们将成为新成立的 Kubernetes 指导委员会的自举治理委员会成员。Derek、Michelle 和 Phillip 将任职 2 年。Aaron、Quinton、和 Timothy 将任职 1 年。&lt;/p>
&lt;!--
This group will meet regularly in order to clarify and streamline the structure and operation of the project. Early work will include electing a representative to the CNCF Governing Board, evolving project processes, refining and documenting the vision and scope of the project, and chartering and delegating to more topical community groups.
-->
&lt;p>该小组将定期开会，以阐明和简化项目的结构和运行。早期的工作将包括选举 CNCF 理事会的代表，发展项目流程，完善和记录项目的愿景和范围，以及授权和委派更多主题社区团体。&lt;/p>
&lt;!--
Please see [the full Steering Committee backlog](https://github.com/kubernetes/steering/blob/master/backlog.md) for more details.
-->
&lt;p>请参阅&lt;a href="https://github.com/kubernetes/steering/blob/master/backlog.md">完整的指导委员会待办事项列表&lt;/a>以获取更多详细信息。&lt;/p></description></item><item><title>Blog: 使用 Kubernetes Pet Sets 和 Datera Elastic Data Fabric 的 FlexVolume 扩展有状态的应用程序</title><link>https://kubernetes.io/zh/blog/2016/08/Stateful-Applications-Using-Kubernetes-Datera/</link><pubDate>Mon, 29 Aug 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2016/08/Stateful-Applications-Using-Kubernetes-Datera/</guid><description>
&lt;!--
---
title: " Scaling Stateful Applications using Kubernetes Pet Sets and FlexVolumes with Datera Elastic Data Fabric "
date: 2016-08-29
slug: stateful-applications-using-kubernetes-datera
url: /blog/2016/08/Stateful-Applications-Using-Kubernetes-Datera
---
--->
&lt;!--
_Editor’s note: today’s guest post is by Shailesh Mittal, Software Architect and Ashok Rajagopalan, Sr Director Product at Datera Inc, talking about Stateful Application provisioning with Kubernetes on Datera Elastic Data Fabric._
--->
&lt;p>&lt;em>编者注：今天的邀请帖子来自 Datera 公司的软件架构师 Shailesh Mittal 和高级产品总监 Ashok Rajagopalan，介绍在 Datera Elastic Data Fabric 上用 Kubernetes 配置状态应用程序。&lt;/em>&lt;/p>
&lt;!--
**Introduction**
Persistent volumes in Kubernetes are foundational as customers move beyond stateless workloads to run stateful applications. While Kubernetes has supported stateful applications such as MySQL, Kafka, Cassandra, and Couchbase for a while, the introduction of Pet Sets has significantly improved this support. In particular, the procedure to sequence the provisioning and startup, the ability to scale and associate durably by [Pet Sets](/docs/user-guide/petset/) has provided the ability to automate to scale the “Pets” (applications that require consistent handling and durable placement).
--->
&lt;p>&lt;strong>简介&lt;/strong>&lt;/p>
&lt;p>用户从无状态工作负载转移到运行有状态应用程序，Kubernetes 中的持久卷是基础。虽然 Kubernetes 早已支持有状态的应用程序，比如 MySQL、Kafka、Cassandra 和 Couchbase，但是 Pet Sets 的引入明显改善了情况。特别是，&lt;a href="https://kubernetes.io/docs/user-guide/petset/">Pet Sets&lt;/a> 具有持续扩展和关联的能力，在配置和启动的顺序过程中，可以自动缩放“Pets”（需要连续处理和持久放置的应用程序）。&lt;/p>
&lt;!--
Datera, elastic block storage for cloud deployments, has [seamlessly integrated with Kubernetes](http://datera.io/blog-library/8/19/datera-simplifies-stateful-containers-on-kubernetes-13) through the [FlexVolume](/docs/user-guide/volumes/#flexvolume) framework. Based on the first principles of containers, Datera allows application resource provisioning to be decoupled from the underlying physical infrastructure. This brings clean contracts (aka, no dependency or direct knowledge of the underlying physical infrastructure), declarative formats, and eventually portability to stateful applications.
--->
&lt;p>Datera 是用于云部署的弹性块存储，可以通过 &lt;a href="https://kubernetes.io/docs/user-guide/volumes/#flexvolume">FlexVolume&lt;/a> 框架与 &lt;a href="http://datera.io/blog-library/8/19/datera-simplifies-stateful-containers-on-kubernetes-13">Kubernetes 无缝集成&lt;/a>。基于容器的基本原则，Datera 允许应用程序的资源配置与底层物理基础架构分离，为有状态的应用程序提供简洁的协议（也就是说，不依赖底层物理基础结构及其相关内容）、声明式格式和最后移植的能力。&lt;/p>
&lt;!--
While Kubernetes allows for great flexibility to define the underlying application infrastructure through yaml configurations, Datera allows for that configuration to be passed to the storage infrastructure to provide persistence. Through the notion of Datera AppTemplates, in a Kubernetes environment, stateful applications can be automated to scale.
--->
&lt;p>Kubernetes 可以通过 yaml 配置来灵活定义底层应用程序基础架构，而 Datera 可以将该配置传递给存储基础结构以提供持久性。通过 Datera AppTemplates 声明，在 Kubernetes 环境中，有状态的应用程序可以自动扩展。&lt;/p>
&lt;!--
**Deploying Persistent Storage**
Persistent storage is defined using the Kubernetes [PersistentVolume](/docs/user-guide/persistent-volumes/#persistent-volumes) subsystem. PersistentVolumes are volume plugins and define volumes that live independently of the lifecycle of the pod that is using it. They are implemented as NFS, iSCSI, or by cloud provider specific storage system. Datera has developed a volume plugin for PersistentVolumes that can provision iSCSI block storage on the Datera Data Fabric for Kubernetes pods.
--->
&lt;p>&lt;strong>部署永久性存储&lt;/strong>&lt;/p>
&lt;p>永久性存储是通过 Kubernetes 的子系统 &lt;a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#persistent-volumes">PersistentVolume&lt;/a> 定义的。PersistentVolumes 是卷插件，它定义的卷的生命周期和使用它的 Pod 相互独立。PersistentVolumes 由 NFS、iSCSI 或云提供商的特定存储系统实现。Datera 开发了用于 PersistentVolumes 的卷插件，可以在 Datera Data Fabric 上为 Kubernetes 的 Pod 配置 iSCSI 块存储。&lt;/p>
&lt;!--
The Datera volume plugin gets invoked by kubelets on minion nodes and relays the calls to the Datera Data Fabric over its REST API. Below is a sample deployment of a PersistentVolume with the Datera plugin:
--->
&lt;p>Datera 卷插件从 minion nodes 上的 kubelet 调用，并通过 REST API 回传到 Datera Data Fabric。以下是带有 Datera 插件的 PersistentVolume 的部署示例：&lt;/p>
&lt;pre>&lt;code> apiVersion: v1
kind: PersistentVolume
metadata:
name: pv-datera-0
spec:
capacity:
storage: 100Gi
accessModes:
- ReadWriteOnce
persistentVolumeReclaimPolicy: Retain
flexVolume:
driver: &amp;quot;datera/iscsi&amp;quot;
fsType: &amp;quot;xfs&amp;quot;
options:
volumeID: &amp;quot;kube-pv-datera-0&amp;quot;
size: “100&amp;quot;
replica: &amp;quot;3&amp;quot;
backstoreServer: &amp;quot;[tlx170.tlx.daterainc.com](http://tlx170.tlx.daterainc.com/):7717”
&lt;/code>&lt;/pre>&lt;!--
This manifest defines a PersistentVolume of 100 GB to be provisioned in the Datera Data Fabric, should a pod request the persistent storage.
--->
&lt;p>为 Pod 申请 PersistentVolume，要按照以下清单在 Datera Data Fabric 中配置 100 GB 的 PersistentVolume。&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl get pv
NAME CAPACITY ACCESSMODES STATUS CLAIM REASON AGE
pv-datera-0 100Gi RWO Available 8s
pv-datera-1 100Gi RWO Available 2s
pv-datera-2 100Gi RWO Available 7s
pv-datera-3 100Gi RWO Available 4s
&lt;/code>&lt;/pre>&lt;!--
**Configuration**
The Datera PersistenceVolume plugin is installed on all minion nodes. When a pod lands on a minion node with a valid claim bound to the persistent storage provisioned earlier, the Datera plugin forwards the request to create the volume on the Datera Data Fabric. All the options that are specified in the PersistentVolume manifest are sent to the plugin upon the provisioning request.
--->
&lt;p>&lt;strong>配置&lt;/strong>&lt;/p>
&lt;p>Datera PersistenceVolume 插件安装在所有 minion node 上。minion node 的声明是绑定到之前设置的永久性存储上的，当 Pod 进入具备有效声明的 minion node 上时，Datera 插件会转发请求，从而在 Datera Data Fabric 上创建卷。根据配置请求，PersistentVolume 清单中所有指定的选项都将发送到插件。&lt;/p>
&lt;!--
Once a volume is provisioned in the Datera Data Fabric, volumes are presented as an iSCSI block device to the minion node, and kubelet mounts this device for the containers (in the pod) to access it.
--->
&lt;p>在 Datera Data Fabric 中配置的卷会作为 iSCSI 块设备呈现给 minion node，并且 kubelet 将该设备安装到容器（在 Pod 中）进行访问。&lt;/p>
&lt;p>&lt;img src="https://lh4.googleusercontent.com/ILlUm1HrWhGa8uTt97dQ786Gn20FHFZkavfucz05NHv6moZWiGDG7GlELM6o4CSzANWvZckoAVug5o4jMg17a-PbrfD1FRbDPeUCIc8fKVmVBNUsUPshWanXYkBa3gIJy5BnhLmZ" alt="">&lt;/p>
&lt;!--
**Using Persistent Storage**
Kubernetes PersistentVolumes are used along with a pod using PersistentVolume Claims. Once a claim is defined, it is bound to a PersistentVolume matching the claim’s specification. A typical claim for the PersistentVolume defined above would look like below:
--->
&lt;p>&lt;strong>使用永久性存储&lt;/strong>&lt;/p>
&lt;p>Kubernetes PersistentVolumes 与具备 PersistentVolume Claims 的 Pod 一起使用。定义声明后，会被绑定到与声明规范匹配的 PersistentVolume 上。上面提到的定义 PersistentVolume 的典型声明如下所示：&lt;/p>
&lt;pre>&lt;code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
name: pv-claim-test-petset-0
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 100Gi
&lt;/code>&lt;/pre>&lt;!--
When this claim is defined and it is bound to a PersistentVolume, resources can be used with the pod specification:
--->
&lt;p>定义这个声明并将其绑定到 PersistentVolume 时，资源与 Pod 规范可以一起使用：&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl get pv
NAME CAPACITY ACCESSMODES STATUS CLAIM REASON AGE
pv-datera-0 100Gi RWO Bound default/pv-claim-test-petset-0 6m
pv-datera-1 100Gi RWO Bound default/pv-claim-test-petset-1 6m
pv-datera-2 100Gi RWO Available 7s
pv-datera-3 100Gi RWO Available 4s
[root@tlx241 /]# kubectl get pvc
NAME STATUS VOLUME CAPACITY ACCESSMODES AGE
pv-claim-test-petset-0 Bound pv-datera-0 0 3m
pv-claim-test-petset-1 Bound pv-datera-1 0 3m
&lt;/code>&lt;/pre>&lt;!--
A pod can use a PersistentVolume Claim like below:
--->
&lt;p>Pod 可以使用 PersistentVolume 声明，如下所示：&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: kube-pv-demo
spec:
containers:
- name: data-pv-demo
image: nginx
volumeMounts:
- name: test-kube-pv1
mountPath: /data
ports:
- containerPort: 80
volumes:
- name: test-kube-pv1
persistentVolumeClaim:
claimName: pv-claim-test-petset-0
&lt;/code>&lt;/pre>&lt;!--
The result is a pod using a PersistentVolume Claim as a volume. It in-turn sends the request to the Datera volume plugin to provision storage in the Datera Data Fabric.
--->
&lt;p>程序的结果是 Pod 将 PersistentVolume Claim 作为卷。依次将请求发送到 Datera 卷插件，然后在 Datera Data Fabric 中配置存储。&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl describe pods kube-pv-demo
Name: kube-pv-demo
Namespace: default
Node: tlx243/172.19.1.243
Start Time: Sun, 14 Aug 2016 19:17:31 -0700
Labels: \&amp;lt;none\&amp;gt;
Status: Running
IP: 10.40.0.3
Controllers: \&amp;lt;none\&amp;gt;
Containers:
data-pv-demo:
Container ID: [docker://ae2a50c25e03143d0dd721cafdcc6543fac85a301531110e938a8e0433f74447](about:blank)
Image: nginx
Image ID: [docker://sha256:0d409d33b27e47423b049f7f863faa08655a8c901749c2b25b93ca67d01a470d](about:blank)
Port: 80/TCP
State: Running
Started: Sun, 14 Aug 2016 19:17:34 -0700
Ready: True
Restart Count: 0
Environment Variables: \&amp;lt;none\&amp;gt;
Conditions:
Type Status
Initialized True
Ready True
PodScheduled True
Volumes:
test-kube-pv1:
Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
ClaimName: pv-claim-test-petset-0
ReadOnly: false
default-token-q3eva:
Type: Secret (a volume populated by a Secret)
SecretName: default-token-q3eva
QoS Tier: BestEffort
Events:
FirstSeen LastSeen Count From SubobjectPath Type Reason Message
--------- -------- ----- ---- ------------- -------- ------ -------
43s 43s 1 {default-scheduler } Normal Scheduled Successfully assigned kube-pv-demo to tlx243
42s 42s 1 {kubelet tlx243} spec.containers{data-pv-demo} Normal Pulling pulling image &amp;quot;nginx&amp;quot;
40s 40s 1 {kubelet tlx243} spec.containers{data-pv-demo} Normal Pulled Successfully pulled image &amp;quot;nginx&amp;quot;
40s 40s 1 {kubelet tlx243} spec.containers{data-pv-demo} Normal Created Created container with docker id ae2a50c25e03
40s 40s 1 {kubelet tlx243} spec.containers{data-pv-demo} Normal Started Started container with docker id ae2a50c25e03
&lt;/code>&lt;/pre>&lt;!--
The persistent volume is presented as iSCSI device at minion node (tlx243 in this case):
--->
&lt;p>永久卷在 minion node（在本例中为 tlx243）中显示为 iSCSI 设备：&lt;/p>
&lt;pre>&lt;code>[root@tlx243 ~]# lsscsi
[0:2:0:0] disk SMC SMC2208 3.24 /dev/sda
[11:0:0:0] disk DATERA IBLOCK 4.0 /dev/sdb
[root@tlx243 datera~iscsi]# mount ``` grep sdb
/dev/sdb on /var/lib/kubelet/pods/6b99bd2a-628e-11e6-8463-0cc47ab41442/volumes/datera~iscsi/pv-datera-0 type xfs (rw,relatime,attr2,inode64,noquota)
&lt;/code>&lt;/pre>&lt;!--
Containers running in the pod see this device mounted at /data as specified in the manifest:
--->
&lt;p>在 Pod 中运行的容器按照清单中将设备安装在 /data 上：&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl exec kube-pv-demo -c data-pv-demo -it bash
root@kube-pv-demo:/# mount ``` grep data
/dev/sdb on /data type xfs (rw,relatime,attr2,inode64,noquota)
&lt;/code>&lt;/pre>&lt;!--
**Using Pet Sets**
Typically, pods are treated as stateless units, so if one of them is unhealthy or gets superseded, Kubernetes just disposes it. In contrast, a PetSet is a group of stateful pods that has a stronger notion of identity. The goal of a PetSet is to decouple this dependency by assigning identities to individual instances of an application that are not anchored to the underlying physical infrastructure.
--->
&lt;p>&lt;strong>使用 Pet Sets&lt;/strong>&lt;/p>
&lt;p>通常，Pod 被视为无状态单元，因此，如果其中之一状态异常或被取代，Kubernetes 会将其丢弃。相反，PetSet 是一组有状态的 Pod，具有更强的身份概念。PetSet 可以将标识分配给应用程序的各个实例，这些应用程序没有与底层物理结构连接，PetSet 可以消除这种依赖性。&lt;/p>
&lt;!--
A PetSet requires {0..n-1} Pets. Each Pet has a deterministic name, PetSetName-Ordinal, and a unique identity. Each Pet has at most one pod, and each PetSet has at most one Pet with a given identity. A PetSet ensures that a specified number of “pets” with unique identities are running at any given time. The identity of a Pet is comprised of:
- a stable hostname, available in DNS
- an ordinal index
- stable storage: linked to the ordinal &amp; hostname
A typical PetSet definition using a PersistentVolume Claim looks like below:
--->
&lt;p>每个 PetSet 需要{0..n-1}个 Pet。每个 Pet 都有一个确定的名字、PetSetName-Ordinal 和唯一的身份。每个 Pet 最多有一个 Pod，每个 PetSet 最多包含一个给定身份的 Pet。要确保每个 PetSet 在任何特定时间运行时，具有唯一标识的“pet”的数量都是确定的。Pet 的身份标识包括以下几点：&lt;/p>
&lt;ul>
&lt;li>一个稳定的主机名，可以在 DNS 中使用&lt;/li>
&lt;li>一个序号索引&lt;/li>
&lt;li>稳定的存储：链接到序号和主机名&lt;/li>
&lt;/ul>
&lt;p>使用 PersistentVolume Claim 定义 PetSet 的典型例子如下所示：&lt;/p>
&lt;pre>&lt;code># A headless service to create DNS records
apiVersion: v1
kind: Service
metadata:
name: test-service
labels:
app: nginx
spec:
ports:
- port: 80
name: web
clusterIP: None
selector:
app: nginx
---
apiVersion: apps/v1alpha1
kind: PetSet
metadata:
name: test-petset
spec:
serviceName: &amp;quot;test-service&amp;quot;
replicas: 2
template:
metadata:
labels:
app: nginx
annotations:
[pod.alpha.kubernetes.io/initialized:](http://pod.alpha.kubernetes.io/initialized:) &amp;quot;true&amp;quot;
spec:
terminationGracePeriodSeconds: 0
containers:
- name: nginx
image: [gcr.io/google\_containers/nginx-slim:0.8](http://gcr.io/google_containers/nginx-slim:0.8)
ports:
- containerPort: 80
name: web
volumeMounts:
- name: pv-claim
mountPath: /data
volumeClaimTemplates:
- metadata:
name: pv-claim
annotations:
[volume.alpha.kubernetes.io/storage-class:](http://volume.alpha.kubernetes.io/storage-class:) anything
spec:
accessModes: [&amp;quot;ReadWriteOnce&amp;quot;]
resources:
requests:
storage: 100Gi
&lt;/code>&lt;/pre>&lt;!--
We have the following PersistentVolume Claims available:
--->
&lt;p>我们提供以下 PersistentVolume Claim：&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl get pvc
NAME STATUS VOLUME CAPACITY ACCESSMODES AGE
pv-claim-test-petset-0 Bound pv-datera-0 0 41m
pv-claim-test-petset-1 Bound pv-datera-1 0 41m
pv-claim-test-petset-2 Bound pv-datera-2 0 5s
pv-claim-test-petset-3 Bound pv-datera-3 0 2s
&lt;/code>&lt;/pre>&lt;!--
When this PetSet is provisioned, two pods get instantiated:
--->
&lt;p>配置 PetSet 时，将实例化两个 Pod：&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl get pods
NAMESPACE NAME READY STATUS RESTARTS AGE
default test-petset-0 1/1 Running 0 7s
default test-petset-1 1/1 Running 0 3s
&lt;/code>&lt;/pre>&lt;!--
Here is how the PetSet test-petset instantiated earlier looks like:
--->
&lt;p>以下是一个 PetSet：test-petset 实例化之前的样子：&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl describe petset test-petset
Name: test-petset
Namespace: default
Image(s): [gcr.io/google\_containers/nginx-slim:0.8](http://gcr.io/google_containers/nginx-slim:0.8)
Selector: app=nginx
Labels: app=nginx
Replicas: 2 current / 2 desired
Annotations: \&amp;lt;none\&amp;gt;
CreationTimestamp: Sun, 14 Aug 2016 19:46:30 -0700
Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed
No volumes.
No events.
&lt;/code>&lt;/pre>&lt;!--
Once a PetSet is instantiated, such as test-petset below, upon increasing the number of replicas (i.e. the number of pods started with that PetSet), more pods get instantiated and more PersistentVolume Claims get bound to new pods:
--->
&lt;p>一旦实例化 PetSet（例如下面的 test-petset），随着副本数（从 PetSet 的初始 Pod 数量算起）的增加，实例化的 Pod 将变得更多，并且更多的 PersistentVolume Claim 会绑定到新的 Pod 上：&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl patch petset test-petset -p'{&amp;quot;spec&amp;quot;:{&amp;quot;replicas&amp;quot;:&amp;quot;3&amp;quot;}}'
&amp;quot;test-petset” patched
[root@tlx241 /]# kubectl describe petset test-petset
Name: test-petset
Namespace: default
Image(s): [gcr.io/google\_containers/nginx-slim:0.8](http://gcr.io/google_containers/nginx-slim:0.8)
Selector: app=nginx
Labels: app=nginx
Replicas: 3 current / 3 desired
Annotations: \&amp;lt;none\&amp;gt;
CreationTimestamp: Sun, 14 Aug 2016 19:46:30 -0700
Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed
No volumes.
No events.
[root@tlx241 /]# kubectl get pods
NAME READY STATUS RESTARTS AGE
test-petset-0 1/1 Running 0 29m
test-petset-1 1/1 Running 0 28m
test-petset-2 1/1 Running 0 9s
&lt;/code>&lt;/pre>&lt;!--
Now the PetSet is running 3 pods after patch application.
--->
&lt;p>现在，应用修补程序后，PetSet 正在运行3个 Pod。&lt;/p>
&lt;!--
When the above PetSet definition is patched to have one more replica, it introduces one more pod in the system. This in turn results in one more volume getting provisioned on the Datera Data Fabric. So volumes get dynamically provisioned and attached to a pod upon the PetSet scaling up.
--->
&lt;p>当上述 PetSet 定义修补完成，会产生另一个副本，PetSet 将在系统中引入另一个 pod。反之，这会导致在 Datera Data Fabric 上配置更多的卷。因此，在 PetSet 进行扩展时，要配置动态卷并将其附加到 Pod 上。&lt;/p>
&lt;!--
To support the notion of durability and consistency, if a pod moves from one minion to another, volumes do get attached (mounted) to the new minion node and detached (unmounted) from the old minion to maintain persistent access to the data.
--->
&lt;p>为了平衡持久性和一致性的概念，如果 Pod 从一个 Minion 转移到另一个，卷确实会附加（安装）到新的 minion node 上，并与旧的 Minion 分离（卸载），从而实现对数据的持久访问。&lt;/p>
&lt;!--
**Conclusion**
This demonstrates Kubernetes with Pet Sets orchestrating stateful and stateless workloads. While the Kubernetes community is working on expanding the FlexVolume framework’s capabilities, we are excited that this solution makes it possible for Kubernetes to be run more widely in the datacenters.
--->
&lt;p>&lt;strong>结论&lt;/strong>&lt;/p>
&lt;p>本文展示了具备 Pet Sets 的 Kubernetes 协调有状态和无状态工作负载。当 Kubernetes 社区致力于扩展 FlexVolume 框架的功能时，我们很高兴这个解决方案使 Kubernetes 能够在数据中心广泛运行。&lt;/p>
&lt;!--
Join and contribute: Kubernetes [Storage SIG](https://groups.google.com/forum/#!forum/kubernetes-sig-storage).
--->
&lt;p>加入我们并作出贡献：Kubernetes &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-storage">Storage SIG&lt;/a>.&lt;/p>
&lt;!--
- [Download Kubernetes](http://get.k8s.io/)
- Get involved with the Kubernetes project on [GitHub](https://github.com/kubernetes/kubernetes)
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Connect with the community on the [k8s Slack](http://slack.k8s.io/)
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
--->
&lt;ul>
&lt;li>&lt;a href="http://get.k8s.io/">下载 Kubernetes&lt;/a>&lt;/li>
&lt;li>参与 Kubernetes 项目 &lt;a href="https://github.com/kubernetes/kubernetes">GitHub&lt;/a>&lt;/li>
&lt;li>发布问题（或者回答问题） &lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>&lt;/li>
&lt;li>联系社区 &lt;a href="http://slack.k8s.io/">k8s Slack&lt;/a>&lt;/li>
&lt;li>在 Twitter 上关注我们 &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 生日快乐。哦，这是你要去的地方！</title><link>https://kubernetes.io/zh/blog/2016/07/21/oh-the-places-you-will-go/</link><pubDate>Thu, 21 Jul 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2016/07/21/oh-the-places-you-will-go/</guid><description>
&lt;!--
---
title: " Happy Birthday Kubernetes. Oh, the places you’ll go! "
date: 2016-07-21
slug: oh-the-places-you-will-go
url: /blog/2016/07/Oh-The-Places-You-Will-Go
---
-->
&lt;!--
_Editor’s note, Today’s guest post is from an independent Kubernetes contributor, Justin Santa Barbara, sharing his reflection on growth of the project from inception to its future._
**Dear K8s,**
_It’s hard to believe you’re only one - you’ve grown up so fast. On the occasion of your first birthday, I thought I would write a little note about why I was so excited when you were born, why I feel fortunate to be part of the group that is raising you, and why I’m eager to watch you continue to grow up!_
-->
&lt;p>&lt;em>编者按，今天的嘉宾帖子来自一位独立的 kubernetes 撰稿人 Justin Santa Barbara，分享了他对项目从一开始到未来发展的思考。&lt;/em>&lt;/p>
&lt;p>&lt;strong>亲爱的 K8s,&lt;/strong>&lt;/p>
&lt;p>&lt;em>很难相信你是唯一的一个 - 成长这么快的。在你一岁生日的时候，我想我可以写一个小纸条，告诉你为什么我在你出生的时候那么兴奋，为什么我觉得很幸运能成为抚养你长大的一员，为什么我渴望看到你继续成长！&lt;/em>&lt;/p>
&lt;!--
_--Justin_
You started with an excellent foundation - good declarative functionality, built around a solid API with a well defined schema and the machinery so that we could evolve going forwards. And sure enough, over your first year you grew so fast: autoscaling, HTTP load-balancing support (Ingress), support for persistent workloads including clustered databases (PetSets). You’ve made friends with more clouds (welcome Azure &amp; OpenStack to the family), and even started to span zones and clusters (Federation). And these are just some of the most visible changes - there’s so much happening inside that brain of yours!
I think it’s wonderful you’ve remained so open in all that you do - you seem to write down everything on GitHub - for better or worse. I think we’ve all learned a lot about that on the way, like the perils of having engineers make scaling statements that are then weighed against claims made without quite the same framework of precision and rigor. But I’m proud that you chose not to lower your standards, but rose to the challenge and just ran faster instead - it might not be the most realistic approach, but it is the only way to move mountains!
-->
&lt;p>&lt;em>--Justin&lt;/em>&lt;/p>
&lt;p>你从一个优秀的基础 - 良好的声明性功能开始，它是围绕一个具有良好定义的模式和机制的坚实的 API 构建的，这样我们就可以向前发展了。果然，在你的第一年里，你增长得如此之快：autoscaling、HTTP load-balancing support (Ingress)、support for persistent workloads including clustered databases (PetSets)。你已经和更多的云交了朋友(欢迎 azure 和 openstack 加入家庭)，甚至开始跨越区域和集群(Federation)。这些只是一些最明显的变化 - 在你的大脑里发生了太多的变化！&lt;/p>
&lt;p>我觉得你一直保持开放的态度真是太好了 - 你好像把所有的东西都写在 github 上 - 不管是好是坏。我想我们在这方面都学到了很多，比如让工程师做缩放声明的风险，然后在没有完全相同的精确性和严谨性框架的情况下，将这些声明与索赔进行权衡。但我很自豪你选择了不降低你的标准，而是上升到挑战，只是跑得更快 - 这可能不是最现实的办法，但这是唯一的方式能移动山！&lt;/p>
&lt;!--
And yet, somehow, you’ve managed to avoid a lot of the common dead-ends that other open source software has fallen into, particularly as those projects got bigger and the developers end up working on it more than they use it directly. How did you do that? There’s a probably-apocryphal story of an employee at IBM that makes a huge mistake, and is summoned to meet with the big boss, expecting to be fired, only to be told “We just spent several million dollars training you. Why would we want to fire you?”. Despite all the investment google is pouring into you (along with Redhat and others), I sometimes wonder if the mistakes we are avoiding could be worth even more. There is a very open development process, yet there’s also an “oracle” that will sometimes course-correct by telling us what happens two years down the road if we make a particular design decision. This is a parent you should probably listen to!
-->
&lt;p>然而，不知何故，你已经设法避免了许多其他开源软件陷入的共同死胡同，特别是当那些项目越来越大，开发人员最终要做的比直接使用它更多的时候。你是怎么做到的？有一个很可能是虚构的故事，讲的是 IBM 的一名员工犯了一个巨大的错误，被传唤去见大老板，希望被解雇，却被告知“我们刚刚花了几百万美元培训你。我们为什么要解雇你？“。尽管谷歌对你进行了大量的投资(包括 redhat 和其他公司)，但我有时想知道，我们正在避免的错误是否更有价值。有一个非常开放的开发过程，但也有一个“oracle”，它有时会通过告诉我们两年后如果我们做一个特定的设计决策会发生什么来纠正错误。这是你应该听的父母！&lt;/p>
&lt;!--
And so although you’re only a year old, you really have an [old soul](http://queue.acm.org/detail.cfm?id=2898444). I’m just one of the [many people raising you](https://kubernetes.io/blog/2016/07/happy-k8sbday-1), but it’s a wonderful learning experience for me to be able to work with the people that have built these incredible systems and have all this domain knowledge. Yet because we started from scratch (rather than taking the existing Borg code) we’re at the same level and can still have genuine discussions about how to raise you. Well, at least as close to the same level as we could ever be, but it’s to their credit that they are all far too nice ever to mention it!
If I would pick just two of the wise decisions those brilliant people made:
-->
&lt;p>所以，尽管你只有一岁，你真的有一个&lt;a href="http://queue.acm.org/detail.cfm?ID=2898444">旧灵魂&lt;/a>。我只是&lt;a href="https://kubernetes.io/blog/2016/07/happy-k8sbday-1">很多人抚养你&lt;/a>中的一员，但对我来说，能够与那些建立了这些令人难以置信的系统并拥有所有这些领域知识的人一起工作是一次极好的学习经历。然而，因为我们是白手起家(而不是采用现有的 Borg 代码)，我们处于同一水平，仍然可以就如何培养你进行真正的讨论。好吧，至少和我们的水平一样接近，但值得称赞的是，他们都太好了，从来没提过！&lt;/p>
&lt;p>如果我选择两个聪明人做出的明智决定：&lt;/p>
&lt;!--
- Labels &amp; selectors give us declarative “pointers”, so we can say “why” we want things, rather than listing the things directly. It’s the secret to how you can scale to [great heights](https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set); not by naming each step, but saying “a thousand more steps just like that first one”.
- Controllers are state-synchronizers: we specify the goals, and your controllers will indefatigably work to bring the system to that state. They work through that strongly-typed API foundation, and are used throughout the code, so Kubernetes is more of a set of a hundred small programs than one big one. It’s not enough to scale to thousands of nodes technically; the project also has to scale to thousands of developers and features; and controllers help us get there.
-->
&lt;ul>
&lt;li>标签和选择器给我们声明性的“pointers”，所以我们可以说“为什么”我们想要东西，而不是直接列出东西。这是如何扩展到[伟大高度]的秘密(&lt;a href="https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set">https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set&lt;/a>)；不是命名每一步，而是说“像第一步一样多走一千步”。&lt;/li>
&lt;li>控制器是状态同步器：我们指定目标，您的控制器将不遗余力地工作，使系统达到该状态。它们工作在强类型 API 基础上，并且贯穿整个代码，因此 Kubernetes 比一个大的程序多一百个小程序。仅仅从技术上扩展到数千个节点是不够的；这个项目还必须扩展到数千个开发人员和特性；控制器帮助我们达到目的。&lt;/li>
&lt;/ul>
&lt;!--
And so on we will go! We’ll be replacing those controllers and building on more, and the API-foundation lets us build anything we can express in that way - with most things just a label or annotation away! But your thoughts will not be defined by language: with third party resources you can express anything you choose. Now we can build Kubernetes without building in Kubernetes, creating things that feel as much a part of Kubernetes as anything else. Many of the recent additions, like ingress, DNS integration, autoscaling and network policies were done or could be done in this way. Eventually it will be hard to imagine you before these things, but tomorrow’s standard functionality can start today, with no obstacles or gatekeeper, maybe even for an audience of one.
So I’m looking forward to seeing more and more growth happen further and further from the core of Kubernetes. We had to work our way through those phases; starting with things that needed to happen in the kernel of Kubernetes - like replacing replication controllers with deployments. Now we’re starting to build things that don’t require core changes. But we’re still still talking about infrastructure separately from applications. It’s what comes next that gets really interesting: when we start building applications that rely on the Kubernetes APIs. We’ve always had the Cassandra example that uses the Kubernetes API to self-assemble, but we haven’t really even started to explore this more widely yet. In the same way that the S3 APIs changed how we build things that remember, I think the k8s APIs are going to change how we build things that think.
-->
&lt;p>等等我们就走！我们将取代那些控制器，建立更多，API 基金会让我们构建任何我们可以用这种方式表达的东西 - 大多数东西只是标签或注释远离！但你的思想不会由语言来定义：有了第三方资源，你可以表达任何你选择的东西。现在我们可以不用在 Kubernetes 建造Kubernetes 了，创造出与其他任何东西一样感觉是 Kubernetes 的一部分的东西。最近添加的许多功能，如ingress、DNS integration、autoscaling and network policies ，都已经完成或可以通过这种方式完成。最终，在这些事情发生之前很难想象你会是怎样的一个人，但是明天的标准功能可以从今天开始，没有任何障碍或看门人，甚至对一个听众来说也是这样。&lt;/p>
&lt;p>所以我期待着看到越来越多的增长发生在离 Kubernetes 核心越来越远的地方。我们必须通过这些阶段来工作；从需要在 kubernetes 内核中发生的事情开始——比如用部署替换复制控制器。现在我们开始构建不需要核心更改的东西。但我们仍然在讨论基础设施和应用程序。接下来真正有趣的是：当我们开始构建依赖于 kubernetes api 的应用程序时。我们一直有使用 kubernetes api 进行自组装的 cassandra 示例，但我们还没有真正开始更广泛地探讨这个问题。正如 S3 APIs 改变了我们构建记忆事物的方式一样，我认为 k8s APIs 也将改变我们构建思考事物的方式。&lt;/p>
&lt;!--
So I’m looking forward to your second birthday: I can try to predict what you’ll look like then, but I know you’ll surpass even the most audacious things I can imagine. Oh, the places you’ll go!
_-- Justin Santa Barbara, Independent Kubernetes Contributor_
-->
&lt;p>所以我很期待你的二岁生日：我可以试着预测你那时的样子，但我知道你会超越我所能想象的最大胆的东西。哦，这是你要去的地方！&lt;/p>
&lt;p>&lt;em>-- Justin Santa Barbara, 独立的 Kubernetes 贡献者&lt;/em>&lt;/p></description></item><item><title>Blog: Dashboard - Kubernetes 的全功能 Web 界面</title><link>https://kubernetes.io/zh/blog/2016/07/15/dashboard-web-interface-for-kubernetes/</link><pubDate>Fri, 15 Jul 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2016/07/15/dashboard-web-interface-for-kubernetes/</guid><description>
&lt;!--
---
title: " Dashboard - Full Featured Web Interface for Kubernetes "
date: 2016-07-15
slug: dashboard-web-interface-for-kubernetes
url: /blog/2016/07/Dashboard-Web-Interface-For-Kubernetes
---
-->
&lt;!--
_Editor’s note: this post is part of a [series of in-depth articles](https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1-3) on what's new in Kubernetes 1.3_
[Kubernetes Dashboard](http://github.com/kubernetes/dashboard) is a project that aims to bring a general purpose monitoring and operational web interface to the Kubernetes world.&amp;nbsp;Three months ago we [released](https://kubernetes.io/blog/2016/04/building-awesome-user-interfaces-for-kubernetes) the first production ready version, and since then the dashboard has made massive improvements. In a single UI, you’re able to perform majority of possible interactions with your Kubernetes clusters without ever leaving your browser. This blog post breaks down new features introduced in the latest release and outlines the roadmap for the future.&amp;nbsp;
-->
&lt;p>&lt;em>编者按：这篇文章是&lt;a href="https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1-3">一系列深入的文章&lt;/a> 中关于Kubernetes 1.3的新内容的一部分&lt;/em>
&lt;a href="http://github.com/kubernetes/dashboard">Kubernetes Dashboard&lt;/a>是一个旨在为 Kubernetes 世界带来通用监控和操作 Web 界面的项目。三个月前，我们&lt;a href="https://kubernetes.io/blog/2016/04/building-awesome-user-interfaces-for-kubernetes">发布&lt;/a>第一个面向生产的版本，从那时起 dashboard 已经做了大量的改进。在一个 UI 中，您可以在不离开浏览器的情况下，与 Kubernetes 集群执行大多数可能的交互。这篇博客文章分解了最新版本中引入的新功能，并概述了未来的路线图。&lt;/p>
&lt;!--
**Full-Featured Dashboard**
Thanks to a large number of contributions from the community and project members, we were able to deliver many new features for [Kubernetes 1.3 release](https://kubernetes.io/blog/2016/07/kubernetes-1.3-bridging-cloud-native-and-enterprise-workloads). We have been carefully listening to all the great feedback we have received from our users (see the [summary infographics](http://static.lwy.io/img/kubernetes_dashboard_infographic.png)) and addressed the highest priority requests and pain points.
-->
&lt;p>&lt;strong>全功能的 Dashboard&lt;/strong>&lt;/p>
&lt;p>由于社区和项目成员的大量贡献，我们能够为&lt;a href="https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/">Kubernetes 1.3发行版&lt;/a>提供许多新功能。我们一直在认真听取用户的反馈(参见&lt;a href="http://static.lwy.io/img/kubernetes_dashboard_infographic.png">摘要信息图表&lt;/a>)，并解决了最高优先级的请求和难点。
--&amp;gt;&lt;/p>
&lt;!--
The Dashboard UI now handles all workload resources. This means that no matter what workload type you run, it is visible in the web interface and you can do operational changes on it. For example, you can modify your stateful MySQL installation with [Pet Sets](/docs/user-guide/petset/), do a rolling update of your web server with Deployments or install cluster monitoring with DaemonSets.&amp;nbsp;
[![](https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz) ](https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz)
-->
&lt;p>Dashboard UI 现在处理所有工作负载资源。这意味着无论您运行什么工作负载类型，它都在 web 界面中可见，并且您可以对其进行操作更改。例如，可以使用&lt;a href="https://kubernetes.io/docs/user-guide/petset/">Pet Sets&lt;/a>修改有状态的 mysql 安装，使用部署对 web 服务器进行滚动更新，或使用守护程序安装群集监视。&lt;/p>
&lt;p>&lt;a href="https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz">&lt;img src="https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz" alt=""> &lt;/a>&lt;/p>
&lt;!--
In addition to viewing resources, you can create, edit, update, and delete them. This feature enables many use cases. For example, you can kill a failed Pod, do a rolling update on a Deployment, or just organize your resources. You can also export and import YAML configuration files of your cloud apps and store them in a version control system.
![](https://lh6.googleusercontent.com/zz-qjNcGgvWXrK1LIipUdIdPyeWJ1EyPVJxRnSvI6pMcLBkxDxpQt-ObsIiZsS_X0RjVBWtXYO5TCvhsymb__CGXFzKuPUnUrB4HKnAMsxtYdWLwMmHEb8c9P9Chzlo5ePHRKf5O)
-->
&lt;p>除了查看资源外，还可以创建、编辑、更新和删除资源。这个特性支持许多用例。例如，您可以杀死一个失败的 pod，对部署进行滚动更新，或者只组织资源。您还可以导出和导入云应用程序的 yaml 配置文件，并将它们存储在版本控制系统中。&lt;/p>
&lt;p>&lt;img src="https://lh6.googleusercontent.com/zz-qjNcGgvWXrK1LIipUdIdPyeWJ1EyPVJxRnSvI6pMcLBkxDxpQt-ObsIiZsS_X0RjVBWtXYO5TCvhsymb__CGXFzKuPUnUrB4HKnAMsxtYdWLwMmHEb8c9P9Chzlo5ePHRKf5O" alt="">&lt;/p>
&lt;!--
The release includes a beta view of cluster nodes for administration and operational use cases. The UI lists all nodes in the cluster to allow for overview analysis and quick screening for problematic nodes. The details view shows all information about the node and links to pods running on it.
![](https://lh6.googleusercontent.com/3CSTUy-8Tz-yAL9tCqxNUqMcWJYKK0dwk7kidE9zy-L-sXFiD4A4Y2LKEqbJKgI6Fl6xbzYxsziI8dULVXPJbu6eU0ci7hNtqi3tTuhdbVD6CG3EXw151fvt2MQuqumHRbab6g-_)
-->
&lt;p>这个版本包括一个用于管理和操作用例的集群节点的 beta 视图。UI 列出集群中的所有节点，以便进行总体分析和快速筛选有问题的节点。details 视图显示有关该节点的所有信息以及指向在其上运行的 pod 的链接。&lt;/p>
&lt;p>&lt;img src="https://lh6.googleusercontent.com/3CSTUy-8Tz-yAL9tCqxNUqMcWJYKK0dwk7kidE9zy-L-sXFiD4A4Y2LKEqbJKgI6Fl6xbzYxsziI8dULVXPJbu6eU0ci7hNtqi3tTuhdbVD6CG3EXw151fvt2MQuqumHRbab6g-_" alt="">&lt;/p>
&lt;!--
There are also many smaller scope new features that the we shipped with the release, namely: support for namespaced resources, internationalization, performance improvements, and many bug fixes (find out more in the [release notes](https://github.com/kubernetes/dashboard/releases/tag/v1.1.0)). All these improvements result in a better and simpler user experience of the product.
-->
&lt;p>我们随发行版提供的还有许多小范围的新功能，即：支持命名空间资源、国际化、性能改进和许多错误修复(请参阅&lt;a href="https://github.com/kubernetes/dashboard/releases/tag/v1.1.0">发行说明&lt;/a>中的更多内容)。所有这些改进都会带来更好、更简单的产品用户体验。&lt;/p>
&lt;!--
**Future Work**
The team has ambitious plans for the future spanning across multiple use cases. We are also open to all feature requests, which you can post on our [issue tracker](https://github.com/kubernetes/dashboard/issues).
-->
&lt;p>&lt;strong>Future Work&lt;/strong>&lt;/p>
&lt;p>该团队对跨越多个用例的未来有着雄心勃勃的计划。我们还对所有功能请求开放，您可以在我们的&lt;a href="https://github.com/kubernetes/dashboard/issues">问题跟踪程序&lt;/a>上发布这些请求。&lt;/p>
&lt;!--
Here is a list of our focus areas for the following months:
- [Handle more Kubernetes resources](https://github.com/kubernetes/dashboard/issues/961) - To show all resources that a cluster user may potentially interact with. Once done, Dashboard can act as a complete replacement for CLI.&amp;nbsp;
- [Monitoring and troubleshooting](https://github.com/kubernetes/dashboard/issues/962) - To add resource usage statistics/graphs to the objects shown in Dashboard. This focus area will allow for actionable debugging and troubleshooting of cloud applications.
- [Security, auth and logging in](https://github.com/kubernetes/dashboard/issues/964) - Make Dashboard accessible from networks external to a Cluster and work with custom authentication systems.
-->
&lt;p>以下是我们接下来几个月的重点领域：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/dashboard/issues/961">Handle more Kubernetes resources&lt;/a> - 显示群集用户可能与之交互的所有资源。一旦完成，dashboard 就可以完全替代cli。&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/dashboard/issues/962">Monitoring and troubleshooting&lt;/a> - 将资源使用统计信息/图表添加到 Dashboard 中显示的对象。这个重点领域将允许对云应用程序进行可操作的调试和故障排除。&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/dashboard/issues/964">Security, auth and logging in&lt;/a> - 使仪表板可从群集外部的网络访问，并使用自定义身份验证系统。&lt;/li>
&lt;/ul>
&lt;!--
**Connect With Us**
We would love to talk with you and hear your feedback!
- Email us at the [SIG-UI mailing list](https://groups.google.com/forum/#!forum/kubernetes-sig-ui)
- Chat with us on the Kubernetes Slack&amp;nbsp;[#SIG-UI channel](https://kubernetes.slack.com/messages/sig-ui/)
- Join our meetings: 4PM CEST. See the [SIG-UI calendar](https://calendar.google.com/calendar/embed?src=google.com_52lm43hc2kur57dgkibltqc6kc%40group.calendar.google.com&amp;ctz=Europe/Warsaw) for details.
_-- Piotr Bryk, Software Engineer, Google_
-->
&lt;p>&lt;strong>联系我们&lt;/strong>&lt;/p>
&lt;p>我们很乐意与您交谈并听取您的反馈！&lt;/p>
&lt;ul>
&lt;li>请在[SIG-UI邮件列表](&lt;a href="https://groups.google.com/forum/">https://groups.google.com/forum/&lt;/a>向我们发送电子邮件！论坛/kubernetes sig ui)&lt;/li>
&lt;li>在 kubernetes slack 上与我们聊天。&lt;a href="https://kubernetes.slack.com/messages/sig-ui/">#SIG-UI channel&lt;/a>&lt;/li>
&lt;li>参加我们的会议：东部时间下午4点。请参阅&lt;a href="https://calendar.google.com/calendar/embed?src=google.com_52lm43hc2kur57dgkibltqc6kc%40group.calendar.google.com&amp;amp;ctz=Europe/Warsaw">SIG-UI日历&lt;/a>了解详细信息。&lt;/li>
&lt;/ul></description></item><item><title>Blog: Citrix + Kubernetes = 全垒打</title><link>https://kubernetes.io/zh/blog/2016/07/14/citrix-netscaler-and-kubernetes/</link><pubDate>Thu, 14 Jul 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2016/07/14/citrix-netscaler-and-kubernetes/</guid><description>
&lt;!--
---
title: " Citrix + Kubernetes = A Home Run "
date: 2016-07-14
slug: citrix-netscaler-and-kubernetes
url: /blog/2016/07/Citrix-Netscaler-And-Kubernetes
---
-->
&lt;!--
_Editor’s note: today’s guest post is by Mikko Disini, a Director of Product Management at Citrix Systems, sharing their collaboration experience on a Kubernetes integration.&amp;nbsp;_
-->
&lt;p>编者按：今天的客座文章来自 Citrix Systems 的产品管理总监 Mikko Disini，他分享了他们在 Kubernetes 集成上的合作经验。 _&lt;/p>
&lt;!--
Technical collaboration is like sports. If you work together as a team, you can go down the homestretch and pull through for a win. That’s our experience with the Google Cloud Platform team.
-->
&lt;p>技术合作就像体育运动。如果你能像一个团队一样合作，你就能在最后关头取得胜利。这就是我们对谷歌云平台团队的经验。&lt;/p>
&lt;!--
Recently, we approached Google Cloud Platform (GCP) to collaborate on behalf of Citrix customers and the broader enterprise market looking to migrate workloads.&amp;nbsp;This migration required including the [NetScaler Docker load balancer](https://www.citrix.com/blogs/2016/06/20/the-best-docker-load-balancer-at-dockercon-in-seattle-this-week/), CPX, into Kubernetes nodes and resolving any issues with getting traffic into the CPX proxies. &amp;nbsp;
-->
&lt;p>最近，我们与 Google 云平台（GCP）联系，代表 Citrix 客户以及更广泛的企业市场，希望就工作负载的迁移进行协作。此迁移需要将 [NetScaler Docker 负载均衡器]https://www.citrix.com/blogs/2016/06/20/the-best-docker-load-balancer-at-dockercon-in-seattle-this-week/) CPX 包含到 Kubernetes 节点中，并解决将流量引入 CPX 代理的任何问题。&lt;/p>
&lt;!--
**Why NetScaler and Kubernetes?**
-->
&lt;p>&lt;strong>为什么是 NetScaler 和 Kubernetes&lt;/strong>&lt;/p>
&lt;!--
1. Citrix customers want the same Layer 4 to Layer 7 capabilities from NetScaler that they have on-prem as they move to the cloud as they begin deploying their container and microservices architecture with Kubernetes&amp;nbsp;
2. Kubernetes provides a proven infrastructure for running containers and VMs with automated workload delivery
3. NetScaler CPX provides Layer 4 to Layer 7 services and highly efficient telemetry data to a logging and analytics platform, [NetScaler Management and Analytics System](https://www.citrix.com/blogs/2016/05/24/introducing-the-next-generation-netscaler-management-and-analytics-system/)
-->
&lt;ol>
&lt;li>Citrix 的客户希望他们开始使用 Kubernetes 部署他们的容器和微服务体系结构时，能够像当初迁移到云计算时一样，享有 NetScaler 所提供的第 4 层到第 7 层能力 &lt;/li>
&lt;li>Kubernetes 提供了一套经过验证的基础设施，可用来运行容器和虚拟机，并自动交付工作负载；&lt;/li>
&lt;li>NetScaler CPX 提供第 4 层到第 7 层的服务，并为日志和分析平台 &lt;a href="https://www.citrix.com/blogs/2016/05/24/introducing-the-next-generation-netscaler-management-and-analytics-system/">NetScaler 管理和分析系统&lt;/a> 提供高效的度量数据。&lt;/li>
&lt;/ol>
&lt;!--
I wish all our experiences working together with a technical partner were as good as working with GCP. We had a list of issues to enable our use cases and were able to collaborate swiftly on a solution. To resolve these, GCP team offered in depth technical assistance, working with Citrix such that NetScaler CPX can spin up and take over as a client-side proxy running on each host.&amp;nbsp;
-->
&lt;p>我希望我们所有与技术合作伙伴一起工作的经验都能像与 GCP 一起工作一样好。我们有一个列表，包含支持我们的用例所需要解决的问题。我们能够快速协作形成解决方案。为了解决这些问题，GCP 团队提供了深入的技术支持，与 Citrix 合作，从而使得 NetScaler CPX 能够在每台主机上作为客户端代理启动运行。&lt;/p>
&lt;!--
Next, NetScaler CPX needed to be inserted in the data path of GCP ingress load balancer so that NetScaler CPX can spread traffic to front end web servers. The NetScaler team made modifications so that NetScaler CPX listens to API server events and configures itself to create a VIP, IP table rules and server rules to take ingress traffic and load balance across front end applications. Google Cloud Platform team provided feedback and assistance to verify modifications made to overcome the technical hurdles. Done!
-->
&lt;p>接下来，需要在 GCP 入口负载均衡器的数据路径中插入 NetScaler CPX，使 NetScaler CPX 能够将流量分散到前端 web 服务器。NetScaler 团队进行了修改，以便 NetScaler CPX 监听 API 服务器事件，并配置自己来创建 VIP、IP 表规则和服务器规则，以便跨前端应用程序接收流量和负载均衡。谷歌云平台团队提供反馈和帮助，验证为克服技术障碍所做的修改。完成了!&lt;/p>
&lt;!--
NetScaler CPX use case is supported in [Kubernetes 1.3](https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/). Citrix customers and the broader enterprise market will have the opportunity to leverage NetScaler with Kubernetes, thereby lowering the friction to move workloads to the cloud.&amp;nbsp;
-->
&lt;p>NetScaler CPX 用例在 &lt;a href="https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/">Kubernetes 1.3&lt;/a> 中提供支持。Citrix 的客户和更广泛的企业市场将有机会基于 Kubernetes 享用 NetScaler 服务，从而降低将工作负载转移到云平台的阻力。 &lt;/p>
&lt;!--
You can learn more about&amp;nbsp;NetScaler CPX [here](https://www.citrix.com/networking/microservices.html).
-->
&lt;p>您可以在&lt;a href="https://www.citrix.com/networking/microservices.html">此处&lt;/a>了解有关 NetScaler CPX 的更多信息。&lt;/p>
&lt;!--
_&amp;nbsp;-- Mikko Disini, Director of Product Management - NetScaler, Citrix Systems_
-->
&lt;p>_ -- Mikko Disini，Citrix Systems NetScaler 产品管理总监&lt;/p></description></item><item><title>Blog: 容器中运行有状态的应用！？ Kubernetes 1.3 说 “是！”</title><link>https://kubernetes.io/blog/2016/07/stateful-applications-in-containers-kubernetes/</link><pubDate>Wed, 13 Jul 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2016/07/stateful-applications-in-containers-kubernetes/</guid><description>
&lt;!--
---
title: " Stateful Applications in Containers!? Kubernetes 1.3 Says “Yes!” "
date: 2016-07-13
slug: stateful-applications-in-containers-kubernetes
url: /blog/2016/07/stateful-applications-in-containers-kubernetes
---
-->
&lt;!--
_Editor's note: today’s guest post is from Mark Balch, VP of Products at Diamanti, who’ll share more about the contributions they’ve made to Kubernetes._
-->
&lt;p>&lt;em>编者注： 今天的来宾帖子来自 Diamanti 产品副总裁 Mark Balch，他将分享有关他们对 Kubernetes 所做的贡献的更多信息。&lt;/em>&lt;/p>
&lt;!--
Congratulations to the Kubernetes community on another [value-packed release](https://kubernetes.io/blog/2016/07/kubernetes-1.3-bridging-cloud-native-and-enterprise-workloads). A focus on stateful applications and federated clusters are two reasons why I’m so excited about 1.3. Kubernetes support for stateful apps such as Cassandra, Kafka, and MongoDB is critical. Important services rely on databases, key value stores, message queues, and more. Additionally, relying on one data center or container cluster simply won’t work as apps grow to serve millions of users around the world. Cluster federation allows users to deploy apps across multiple clusters and data centers for scale and resiliency.
-->
&lt;p>祝贺 Kubernetes 社区发布了另一个&lt;a href="https://kubernetes.io/blog/2016/07/kubernetes-1.3-bridging-cloud-native-and-enterprise-workloads">有价值的版本&lt;/a>。
专注于有状态应用程序和联邦集群是我对 1.3 如此兴奋的两个原因。
Kubernetes 对有状态应用程序（例如 Cassandra、Kafka 和 MongoDB）的支持至关重要。
重要服务依赖于数据库、键值存储、消息队列等。
此外，随着应用程序的发展为全球数百万用户提供服务，仅依靠一个数据中心或容器集群将无法正常工作。
联邦集群允许用户跨多个集群和数据中心部署应用程序，以实现规模和弹性。&lt;/p>
&lt;!--
You may have [heard me say before](https://www.diamanti.com/blog/the-next-great-application-platform/) that containers are the next great application platform. Diamanti is accelerating container adoption for stateful apps in production - where performance and ease of deployment really matter.&amp;nbsp;
-->
&lt;p>您可能&lt;a href="https://www.diamanti.com/blog/the-next-great-application-platform/">之前听过我说过&lt;/a>，容器是下一个出色的应用程序平台。
Diamanti 正在加速在生产中使用有状态应用程序的容器-在这方面，性能和易于部署非常重要。&lt;/p>
&lt;!--
**Apps Need More Than Cattle**
-->
&lt;p>&lt;strong>应用程序不仅仅需要牛&lt;/strong>&lt;/p>
&lt;!--
Beyond stateless containers like web servers (so-called “cattle” because they are interchangeable), users are increasingly deploying stateful workloads with containers to benefit from “build once, run anywhere” and to improve bare metal efficiency/utilization. These “pets” (so-called because each requires special handling) bring new requirements including longer life cycle, configuration dependencies, stateful failover, and performance sensitivity. Container orchestration must address these needs to successfully deploy and scale apps.
-->
&lt;p>除了诸如Web服务器之类的无状态容器（因为它们是可互换的，因此被称为“牛”）之外，用户越来越多地使用容器来部署有状态工作负载，以从“一次构建，随处运行”中受益并提高裸机效率/利用率。
这些“宠物”（之所以称为“宠物”，是因为每个宠物都需要特殊的处理）带来了新的要求，包括更长的生命周期，配置依赖项，有状态故障转移以及性能敏感性。
容器编排必须满足这些需求，才能成功部署和扩展应用程序。&lt;/p>
&lt;!--
Enter [Pet Set](/docs/user-guide/petset/), a new object in Kubernetes 1.3 for improved stateful application support. Pet Set sequences through the startup phase of each database replica (for example), ensuring orderly master/slave configuration. Pet Set also simplifies service discovery by leveraging ubiquitous DNS SRV records, a well-recognized and long-understood mechanism.
-->
&lt;p>输入 &lt;a href="https://kubernetes.io/docs/user-guide/petset/">Pet Set&lt;/a>，这是 Kubernetes 1.3 中的新对象，用于改进对状态应用程序的支持。
Pet Set 在每个数据库副本的启动阶段进行排序（例如），以确保有序的主/从配置。
Pet Set 还利用普遍存在的 DNS SRV 记录简化了服务发现，DNS SRV 记录是一种广为人知且长期了解的机制。&lt;/p>
&lt;!--
Diamanti’s [FlexVolume contribution](https://github.com/kubernetes/kubernetes/pull/13840) to Kubernetes enables stateful workloads by providing persistent volumes with low-latency storage and guaranteed performance, including enforced quality-of-service from container to media.
-->
&lt;p>Diamanti 对 Kubernete s的 &lt;a href="https://github.com/kubernetes/kubernetes/pull/13840">FlexVolume 贡献&lt;/a> 通过为持久卷提供低延迟存储并保证性能来实现有状态工作负载，包括从容器到媒体的强制服务质量。&lt;/p>
&lt;!--
**A Federalist**
-->
&lt;p>&lt;strong>联邦主义者&lt;/strong>&lt;/p>
&lt;!--
Users who are planning for application availability must contend with issues of failover and scale across geography. Cross-cluster federated services allows containerized apps to easily deploy across multiple clusters. Federated services tackles challenges such as managing multiple container clusters and coordinating service deployment and discovery across federated clusters.
-->
&lt;p>为应用可用性作规划的用户必须应对故障迁移问题并在整个地理区域内扩展。
跨集群联邦服务允许容器化的应用程序轻松跨多个集群进行部署。
联邦服务解决了诸如管理多个容器集群以及协调跨联邦集群的服务部署和发现之类的挑战。&lt;/p>
&lt;!--
Like a strictly centralized model, federation provides a common app deployment interface. With each cluster retaining autonomy, however, federation adds flexibility to manage clusters locally during network outages and other events. Cross-cluster federated services also applies consistent service naming and adoption across container clusters, simplifying DNS resolution.
-->
&lt;p>像严格的集中式模型一样，联邦身份验证提供了通用的应用程序部署界面。
但是，由于每个集群都具有自治权，因此联邦会增加了在网络中断和其他事件期间在本地管理集群的灵活性。
跨集群联邦服务还可以提供跨容器集群应用一致的服务命名和采用，简化 DNS 解析。&lt;/p>
&lt;!--
It’s easy to imagine powerful multi-cluster use cases with cross-cluster federated services in future releases. An example is scheduling containers based on governance, security, and performance requirements. Diamanti’s scheduler extension was developed with this concept in mind. Our [first implementation](https://github.com/kubernetes/kubernetes/pull/13580) makes the Kubernetes scheduler aware of network and storage resources local to each cluster node. Similar concepts can be applied in the future to broader placement controls with cross-cluster federated services.&amp;nbsp;
-->
&lt;p>很容易想象在将来的版本中具有跨集群联邦服务的强大多集群用例。
一个示例是根据治理，安全性和性能要求调度容器。
Diamanti 的调度程序扩展是在考虑了这一概念的基础上开发的。
我们的&lt;a href="https://github.com/kubernetes/kubernetes/pull/13580">第一个实现&lt;/a>使 Kubernetes 调度程序意识到每个群集节点本地的网络和存储资源。
将来，类似的概念可以应用于跨集群联邦服务的更广泛的放置控件。&lt;/p>
&lt;!--
**Get Involved**
-->
&lt;p>&lt;strong>参与其中&lt;/strong>&lt;/p>
&lt;!--
With interest growing in stateful apps, work has already started to further enhance Kubernetes storage. The Storage Special Interest Group is discussing proposals to support local storage resources. Diamanti is looking forward to extend FlexVolume to include richer APIs that enable local storage and storage services including data protection, replication, and reduction. We’re also working on proposals for improved app placement, migration, and failover across container clusters through Kubernetes cross-cluster federated services.
-->
&lt;p>随着对有状态应用的兴趣日益浓厚，人们已经开始进一步增强 Kubernetes 存储的工作。
存储特别兴趣小组正在讨论支持本地存储资源的提案。
Diamanti 期待将 FlexVolume 扩展到包括更丰富的 API，这些 API 可以启用本地存储和存储服务，包括数据保护，复制和缩减。
我们还正在研究有关通过 Kubernetes 跨集群联邦服务改善应用程序放置，迁移和跨容器集群故障转移的建议。&lt;/p>
&lt;!--
Join the conversation and contribute! Here are some places to get started:
-->
&lt;p>加入对话并做出贡献！
这里是一些入门的地方：&lt;/p>
&lt;!--
- Product Management [group](https://groups.google.com/forum/#!forum/kubernetes-sig-pm)
- Kubernetes [Storage SIG](https://groups.google.com/forum/#!forum/kubernetes-sig-storage)&amp;nbsp;
- Kubernetes [Cluster Federation SIG](https://groups.google.com/forum/#!forum/kubernetes-sig-federation)
-->
&lt;ul>
&lt;li>产品管理 &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-pm">组&lt;/a>&lt;/li>
&lt;li>Kubernetes &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-storage">存储 SIG&lt;/a> &lt;/li>
&lt;li>Kubernetes &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-federation">集群联邦 SIG&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
_-- Mark Balch, VP Products, [Diamanti](https://diamanti.com/). Twitter [@markbalch](https://twitter.com/markbalch)_
-->
&lt;p>&lt;em>-- &lt;a href="https://diamanti.com/">Diamanti&lt;/a> 产品副总裁 Mark Balch。 Twitter &lt;a href="https://twitter.com/markbalch">@markbalch&lt;/a>&lt;/em>&lt;/p></description></item></channel></rss>